{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minnji88/NLP-study/blob/main/crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3FHKhODog5H"
      },
      "source": [
        "from bs4 import BeautifulSoup as bs\r\n",
        "import requests\r\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1SLdw3v3OZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf690a7-e08d-485a-e5f5-ddfd77cc33ec"
      },
      "source": [
        "def Getdata(keyword):\r\n",
        "    data = list()\r\n",
        "    n = 5\r\n",
        "    for num in range(10):\r\n",
        "        num = num + 1\r\n",
        "        req = requests.get('https://github.com/search?p='+ str(num) +'&q='+ keyword +'&type=Repositories')\r\n",
        "        req.encoding = 'utf-8'\r\n",
        "        html = req.text\r\n",
        "\r\n",
        "        soup = bs(html,'html.parser')\r\n",
        "\r\n",
        "        session_list = soup.select('ul.repo-list > li > div.mt-n1 > div.f4.text-normal > a')\r\n",
        "\r\n",
        "        for session in session_list:\r\n",
        "            i = session.get('href')\r\n",
        "            req = requests.get(\"https://github.com/\" + i)\r\n",
        "            req.encoding = 'utf-8'\r\n",
        "            html = req.text\r\n",
        "            soup = bs(html,'html.parser')\r\n",
        "            session_list2 = soup.find('article')\r\n",
        "            print(session_list2)\r\n",
        "\r\n",
        "            if session_list2 != None:\r\n",
        "                n = n + 1\r\n",
        "                data.append(session_list2)\r\n",
        "\r\n",
        "                \r\n",
        "            data.append(session_list2)\r\n",
        "    return data\r\n",
        "\r\n",
        "gd = Getdata('tensorflow')\r\n",
        "\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "<hr/>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#generative-dialog\" id=\"user-content-generative-dialog\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Generative Dialog</h2>\n",
            "<pre><code>└── finch/tensorflow1/free_chat/chinese_lccc\n",
            "\t│\n",
            "\t├── data\n",
            "\t│   └── LCCC-base.json           \t# raw data downloaded from external\n",
            "\t│   └── LCCC-base_test.json         # raw data downloaded from external\n",
            "\t│   └── make_data.ipynb           \t# step 1. run this to generate vocab {char.txt} and data {train.txt &amp; test.txt}\n",
            "\t│   └── train.txt           \t\t# processed text file generated by {make_data.ipynb}\n",
            "\t│   └── test.txt           \t\t\t# processed text file generated by {make_data.ipynb}\n",
            "\t│\n",
            "\t├── vocab\n",
            "\t│   └── char.txt                \t# list of chars in vocabulary for chinese\n",
            "\t│   └── cc.zh.300.vec\t\t\t# fastText pretrained embedding downloaded from external\n",
            "\t│   └── char.npy\t\t\t# chinese characters and their embedding values (300 dim)\t\n",
            "\t│\t\n",
            "\t└── main\n",
            "\t\t└── lstm_seq2seq_train.ipynb    # step 2. train and evaluate model\n",
            "\t\t└── lstm_seq2seq_infer.ipynb    # step 4. model inference\n",
            "\t\t└── ...\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: <a href=\"https://github.com/thu-coai/CDial-GPT\">Large-scale Chinese Conversation Dataset</a></p>\n",
            "<pre><code>  Training Data: 5000000 (sampled due to small memory), Testing Data: 19008\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Data</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/data/train.txt\">&lt;Text File&gt;: Data Example</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/free_chat/chinese_lccc/data/make_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Make Data &amp; Vocabulary</a></p>\n",
            "<ul>\n",
            "<li><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/free_chat/chinese_lccc/vocab/char.txt\" rel=\"nofollow\">&lt;Text File&gt;: Vocabulary Example</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p>Model</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Model</th>\n",
            "<th>Env</th>\n",
            "<th>Test Case</th>\n",
            "<th>Perplexity</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/transformer_train.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Transformer Encoder + LSTM Generator</td>\n",
            "<td>TF1</td>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/transformer_infer.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>42.465</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/lstm_seq2seq_train.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>LSTM Encoder + LSTM Generator</td>\n",
            "<td>TF1</td>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/lstm_seq2seq_infer.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>41.250</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/lstm_pointer_train.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>LSTM Encoder + LSTM <a href=\"https://arxiv.org/abs/1704.04368\" rel=\"nofollow\">Pointer-Generator</a></td>\n",
            "<td>TF1</td>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/lstm_pointer_infer.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>36.525</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "<li>\n",
            "<p>If you want to deploy model in Java production</p>\n",
            "<pre><code> └── FreeChatInference\n",
            " \t│\n",
            " \t├── data\n",
            " \t│   └── transformer_export/\n",
            " \t│   └── char.txt\n",
            " \t│   └── libtensorflow-1.14.0.jar\n",
            " \t│   └── tensorflow_jni.dll\n",
            " \t│\n",
            " \t└── src              \n",
            " \t\t└── ModelInference.java\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/tensorflow-nlp/blob/master/finch/java/FreeChatInference/src/ModelInference.java\">&lt;Notebook&gt; Java Inference</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p>If you don't know the input and output node names in Java, you can display the node names:</p>\n",
            "<pre><code> !saved_model_cli show --dir ../model/xxx/1587959473/ --tag_set serve --signature_def serving_default\n",
            "</code></pre>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p>Large Pre-trained <a href=\"https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe\" rel=\"nofollow\">GPT</a></p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Test Case</th>\n",
            "<th>Model</th>\n",
            "<th>Env</th>\n",
            "<th>Author</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/gpt_lccc_base.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>GPT LCCC base</td>\n",
            "<td>TF1 + <a href=\"https://github.com/bojone/bert4keras\">bert4keras</a></td>\n",
            "<td><a href=\"https://github.com/thu-coai/CDial-GPT\">Tsinghua University</a></td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/gpt_lccc_large.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>GPT LCCC large</td>\n",
            "<td>TF1 + <a href=\"https://github.com/bojone/bert4keras\">bert4keras</a></td>\n",
            "<td><a href=\"https://github.com/thu-coai/CDial-GPT\">Tsinghua University</a></td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/nezha_gpt.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>NEZHA LM</td>\n",
            "<td>TF1 + <a href=\"https://github.com/bojone/bert4keras\">bert4keras</a></td>\n",
            "<td><a href=\"https://github.com/bojone/nezha_gpt_dialog\">Jianlin Su</a></td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "<li>\n",
            "<p>Diverse Response Comparsion</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>问题 / 模型回复</th>\n",
            "<th>Our RNN</th>\n",
            "<th>GPT base</th>\n",
            "<th>GPT large</th>\n",
            "<th>Nezha</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td>Q: 你好</td>\n",
            "<td>A: 你好</td>\n",
            "<td>A: 你好</td>\n",
            "<td>A: 你好，你好，我是大好人。</td>\n",
            "<td>A: 您好</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 早上好</td>\n",
            "<td>A: 早上好</td>\n",
            "<td>A: 早上好，好久不见</td>\n",
            "<td>A: 早上好！</td>\n",
            "<td>A: 早啊早啊</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 晚上好</td>\n",
            "<td>A: 晚上好</td>\n",
            "<td>A: 你好</td>\n",
            "<td>A: 好久不见</td>\n",
            "<td>A: 晚上好！</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 再见</td>\n",
            "<td>A: 再见</td>\n",
            "<td>A: 不见</td>\n",
            "<td>A: 你是不是不爱我了</td>\n",
            "<td>A: 我要走了</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 好久不见</td>\n",
            "<td>A: 好久不见</td>\n",
            "<td>A: 是呀，好久不见。</td>\n",
            "<td>A: 是啊，好久不见。</td>\n",
            "<td>A: 是啊，最近怎么样啊</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 想死你了</td>\n",
            "<td>A: 我也是</td>\n",
            "<td>A: 我也想你，</td>\n",
            "<td>A: 我不信</td>\n",
            "<td>A: 哈哈哈哈哈</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 谢谢你</td>\n",
            "<td>A: 不客气</td>\n",
            "<td>A: 不用谢</td>\n",
            "<td>A: 不用</td>\n",
            "<td>A: 不谢不谢，</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 爱你</td>\n",
            "<td>A: 爱你</td>\n",
            "<td>A: 你是爱我</td>\n",
            "<td>A: 我也爱你，</td>\n",
            "<td>A: 我也爱你，你最美</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你叫什么名字</td>\n",
            "<td>A: 你猜</td>\n",
            "<td>A: 我也想知道</td>\n",
            "<td>A: 叫你</td>\n",
            "<td>A: 你是不是想说我的名字太俗了</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你几岁了</td>\n",
            "<td>A: 问女孩子年龄可不是绅士所为哦 ~</td>\n",
            "<td>A: 你猜</td>\n",
            "<td>A: 你猜</td>\n",
            "<td>A: 我不知道</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 现在几点了</td>\n",
            "<td>A: 11点</td>\n",
            "<td>A: 你说呢？</td>\n",
            "<td>A: 十二点</td>\n",
            "<td>A: 我也不知道啊</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 今天天气怎么样</td>\n",
            "<td>A: 下雨了</td>\n",
            "<td>A: 今天很冷，但是我还是觉得很冷</td>\n",
            "<td>A: 很好</td>\n",
            "<td>A: 不错</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我们现在在哪里</td>\n",
            "<td>A: 在这里</td>\n",
            "<td>A: 我在这里等你呢</td>\n",
            "<td>A: 我不知道你在说什么</td>\n",
            "<td>A: 在我们宿舍楼下。。。</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你能给我讲个笑话吗</td>\n",
            "<td>A: 不能</td>\n",
            "<td>A: 讲个笑话</td>\n",
            "<td>A: 讲给谁听</td>\n",
            "<td>A: 不能</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你是男孩还是女孩呀</td>\n",
            "<td>A: 我是萌妹纸</td>\n",
            "<td>A: 你猜</td>\n",
            "<td>A: 你猜呀</td>\n",
            "<td>A: 我是萌妹纸</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你会几种语言呀</td>\n",
            "<td>A: 你猜</td>\n",
            "<td>A: 不会</td>\n",
            "<td>A: 你猜呀~</td>\n",
            "<td>A: 只会说一句</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你能陪我玩吗</td>\n",
            "<td>A: 可以啊</td>\n",
            "<td>A: 可以</td>\n",
            "<td>A: 我可以啊</td>\n",
            "<td>A: 你猜</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 说话可以大声一点吗</td>\n",
            "<td>A: 可以</td>\n",
            "<td>A: 可以的</td>\n",
            "<td>A: 不要</td>\n",
            "<td>A: 我不</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 天气真好</td>\n",
            "<td>A: 是的</td>\n",
            "<td>A: 是啊！</td>\n",
            "<td>A: 天气好是因为有你</td>\n",
            "<td>A: 是啊。</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 天气太糟糕了</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 是的</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 下雨了</td>\n",
            "<td>A: 是的</td>\n",
            "<td>A: 你们那边也下雨了</td>\n",
            "<td>A: 是呀</td>\n",
            "<td>A: 我也是</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 雨好大</td>\n",
            "<td>A: 今天下雨了</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 是的，好大雨</td>\n",
            "<td>A: 对呀，我们还穿短袖</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我讨厌艳阳天</td>\n",
            "<td>A: 我也讨厌艳阳天</td>\n",
            "<td>A: 我也是。</td>\n",
            "<td>A: 我喜欢阴天</td>\n",
            "<td>A: 我喜欢</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 好晒啊</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 是啊！！</td>\n",
            "<td>A: 是啊，不是很热的</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 今天好冷</td>\n",
            "<td>A: 多穿点衣服</td>\n",
            "<td>A: 你们都不来</td>\n",
            "<td>A: 你那儿也冷了</td>\n",
            "<td>A: 是啊。</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 今天好热</td>\n",
            "<td>A: 今天热死了</td>\n",
            "<td>A: 热死我了</td>\n",
            "<td>A: 对啊</td>\n",
            "<td>A: 对啊，我都热疯</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 风好大</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 风太大了。。。</td>\n",
            "<td>A: 你不是在上课嘛</td>\n",
            "<td>A: 嗯嗯嗯，风好大，冷</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 雾太大了看不清路</td>\n",
            "<td>A: 雾太大了</td>\n",
            "<td>A: 哈哈，你这是在逗我笑</td>\n",
            "<td>A: 我也看不清</td>\n",
            "<td>A: 你是不是傻啊</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 打雷了好可怕</td>\n",
            "<td>A: 可怕</td>\n",
            "<td>A: 我们这边也是，我们这边下雨了，我们这边也没有下雨。</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 我们这边下雨了</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 下雪了诶</td>\n",
            "<td>A: 下雪了？</td>\n",
            "<td>A: 是滴！</td>\n",
            "<td>A: 嗯嗯嗯！！好大的雪！！</td>\n",
            "<td>A: 下了一天了</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 好烦啊</td>\n",
            "<td>A: 我也好烦</td>\n",
            "<td>A: 我也是</td>\n",
            "<td>A: 我好烦</td>\n",
            "<td>A: 我也是</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 好开心</td>\n",
            "<td>A: 开心就好</td>\n",
            "<td>A: 你是不是想我了</td>\n",
            "<td>A: 是呀</td>\n",
            "<td>A: 哈哈哈，我也是！</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 太激动了</td>\n",
            "<td>A: 激动什么</td>\n",
            "<td>A: 我也激动</td>\n",
            "<td>A: 哈哈，还是不要了</td>\n",
            "<td>A: 哈哈</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我好难过</td>\n",
            "<td>A: 怎么了？</td>\n",
            "<td>A: 我也是，我们都是</td>\n",
            "<td>A: 不要难过，我陪你</td>\n",
            "<td>A: 你怎么难过</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我想哭</td>\n",
            "<td>A: 抱抱</td>\n",
            "<td>A: 抱紧你</td>\n",
            "<td>A: 我不想哭！你不要想太多！</td>\n",
            "<td>A: 哭吧哭吧，我陪你哭</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 太好笑了</td>\n",
            "<td>A: 笑死我了</td>\n",
            "<td>A: 你也笑了？</td>\n",
            "<td>A: 我也觉得</td>\n",
            "<td>A: 哈哈</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我好伤心</td>\n",
            "<td>A: 怎么了</td>\n",
            "<td>A: 你伤心什么</td>\n",
            "<td>A: 我也是</td>\n",
            "<td>A: 你是不是想笑死我！！下次再给你拍</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 心好痛</td>\n",
            "<td>A: 抱抱</td>\n",
            "<td>A: 你心痛什么</td>\n",
            "<td>A: 心碎一地了。</td>\n",
            "<td>A: 怎么了</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 好累啊</td>\n",
            "<td>A: 加油</td>\n",
            "<td>A: 你不会还是一个人在寝室吧</td>\n",
            "<td>A: 好好休息</td>\n",
            "<td>A: 累了就睡觉</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我好疲惫</td>\n",
            "<td>A: 我也是</td>\n",
            "<td>A: 我也差不多。。</td>\n",
            "<td>A: 我好困</td>\n",
            "<td>A: 你不是一直都很疲惫么</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我爱你</td>\n",
            "<td>A: 我也爱你</td>\n",
            "<td>A: 我也是</td>\n",
            "<td>A: 我们都爱你</td>\n",
            "<td>A: 爱我就来见面</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我讨厌你</td>\n",
            "<td>A: 我也讨厌你</td>\n",
            "<td>A: 你不是一直很喜欢我</td>\n",
            "<td>A: 不是我的</td>\n",
            "<td>A: 我也讨厌你</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你真是太棒啦</td>\n",
            "<td>A: 谢谢</td>\n",
            "<td>A: 你最棒</td>\n",
            "<td>A: 你也棒棒哒</td>\n",
            "<td>A: 谢谢！</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你好厉害啊</td>\n",
            "<td>A: 哈哈</td>\n",
            "<td>A: 是不是</td>\n",
            "<td>A: 你也不差呀</td>\n",
            "<td>A: 你也可以的</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 吓死我了</td>\n",
            "<td>A: 哈哈</td>\n",
            "<td>A: 我都不敢出门了</td>\n",
            "<td>A: 哈哈哈哈哈！！！</td>\n",
            "<td>A: 哈哈哈哈哈笑死我了</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我想回家</td>\n",
            "<td>A: 我也想回家</td>\n",
            "<td>A: 我也想！！</td>\n",
            "<td>A: 想家的时候回来，想家的时候离开。</td>\n",
            "<td>A: 回来吧，家真的好啊！</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我想爸妈了</td>\n",
            "<td>A: 我也想爸妈</td>\n",
            "<td>A: 哈哈</td>\n",
            "<td>A: 我也想</td>\n",
            "<td>A: 想我吗</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 不知道小孩在家有没有听话</td>\n",
            "<td>A: 我也不知道</td>\n",
            "<td>A: 没有</td>\n",
            "<td>A: 听话的话肯定是会听话的。</td>\n",
            "<td>A: 我也是听不懂啊</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 想回家撸猫</td>\n",
            "<td>A: 我也想回家</td>\n",
            "<td>A: 你也想啊？</td>\n",
            "<td>A: 我们这也有一个</td>\n",
            "<td>A: 回呀回呀</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "</article>\n",
            "<article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><h1><a aria-hidden=\"true\" class=\"anchor\" href=\"#tensorflow-speech-recognition\" id=\"user-content-tensorflow-speech-recognition\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Tensorflow Speech Recognition</h1>\n",
            "<p>Speech recognition using google's <a href=\"https://github.com/tensorflow/tensorflow/\">tensorflow</a> deep learning framework, <a href=\"https://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html\" rel=\"nofollow\">sequence-to-sequence</a> neural networks.</p>\n",
            "<p>Replaces <a href=\"https://github.com/pannous/caffe-speech-recognition\">caffe-speech-recognition</a>, see there for some background.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#update-mozilla-released-deepspeech\" id=\"user-content-update-mozilla-released-deepspeech\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Update <strong>Mozilla</strong> released <a href=\"https://github.com/mozilla/DeepSpeech\">DeepSpeech</a></h2>\n",
            "<p>They achieve good <a href=\"http://doyouunderstand.me\" rel=\"nofollow\">error rates</a>. Free Speech is in good hands, go <em>there</em> if you are an end user.\n",
            "For now <em>this</em> project is only maintained for educational purposes.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#ultimate-goal\" id=\"user-content-ultimate-goal\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Ultimate goal</h2>\n",
            "<p>Create a decent standalone speech recognition for Linux etc.\n",
            "Some people say we have the models but not enough training data.\n",
            "We disagree: There is plenty of training data (100GB <a href=\"http://www.openslr.org/12\" rel=\"nofollow\">here</a> and 21GB <a href=\"http://www.openslr.org/7/\" rel=\"nofollow\">here on openslr.org</a> , synthetic Text to Speech snippets, Movies with transcripts, Gutenberg, YouTube with captions etc etc) we just need a simple yet powerful model. It's only a question of time...</p>\n",
            "<p><a href=\"/pannous/tensorflow-speech-recognition/blob/master/images/0_Karen_160.png\" rel=\"noopener noreferrer\" target=\"_blank\"><img alt=\"Sample spectrogram, That's what she said, too laid?\" src=\"/pannous/tensorflow-speech-recognition/raw/master/images/0_Karen_160.png\" style=\"max-width:100%;\"/></a></p>\n",
            "<p>Sample spectrogram, Karen uttering 'zero' with 160 words per minute.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#installation\" id=\"user-content-installation\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Installation</h2>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#clone-code\" id=\"user-content-clone-code\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>clone code</h3>\n",
            "<pre><code>git clone https://github.com/pannous/tensorflow-speech-recognition\n",
            "cd tensorflow-speech-recognition\n",
            "git clone https://github.com/pannous/layer.git\n",
            "git clone https://github.com/pannous/tensorpeers.git\n",
            "</code></pre>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#pyaudio\" id=\"user-content-pyaudio\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>pyaudio</h3>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#requirements-portaudio-from-httpwwwportaudiocom\" id=\"user-content-requirements-portaudio-from-httpwwwportaudiocom\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>requirements portaudio from <a href=\"http://www.portaudio.com/\" rel=\"nofollow\">http://www.portaudio.com/</a></h4>\n",
            "<pre><code>git clone  https://git.assembla.com/portaudio.git\n",
            "./configure --prefix=/path/to/your/local\n",
            "make\n",
            "make install\n",
            "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/your/local/lib\n",
            "export LIDRARY_PATH=$LIBRARY_PATH:/path/to/your/local/lib\n",
            "export CPATH=$CPATH:/path/to/your/local/include\n",
            "source ~/.bashrc\n",
            "</code></pre>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#install-pyaudio\" id=\"user-content-install-pyaudio\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>install pyaudio</h4>\n",
            "<pre><code>pip install pyaudio\n",
            "</code></pre>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#getting-started\" id=\"user-content-getting-started\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Getting started</h2>\n",
            "<p>Toy examples:\n",
            "<code>./number_classifier_tflearn.py</code>\n",
            "<code>./speaker_classifier_tflearn.py</code></p>\n",
            "<p>Some less trivial architectures:\n",
            "<code>./densenet_layer.py</code></p>\n",
            "<p>Later:\n",
            "<code>./train.sh</code>\n",
            "<code>./record.py</code></p>\n",
            "<p><a href=\"/pannous/tensorflow-speech-recognition/blob/master/images/spectrogram.demo.png\" rel=\"noopener noreferrer\" target=\"_blank\"><img alt=\"Sample spectrogram or record.py\" src=\"/pannous/tensorflow-speech-recognition/raw/master/images/spectrogram.demo.png\" style=\"max-width:100%;\"/></a></p>\n",
            "<p>Update: Nervana <a href=\"https://www.youtube.com/watch?v=NaqZkV_fBIM\" rel=\"nofollow\">demonstrated</a> that it is possible for 'independents' to build speech recognizers that are state of the art.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#fun-tasks-for-newcomers\" id=\"user-content-fun-tasks-for-newcomers\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Fun tasks for newcomers</h3>\n",
            "<ul>\n",
            "<li>Watch video : <a href=\"https://www.youtube.com/watch?v=u9FPqkuoEJ8\" rel=\"nofollow\">https://www.youtube.com/watch?v=u9FPqkuoEJ8</a></li>\n",
            "<li>Understand and correct the corresponding code: <a href=\"/pannous/tensorflow-speech-recognition/blob/master/lstm-tflearn.py\">lstm-tflearn.py</a></li>\n",
            "<li>Data Augmentation :  create on-the-fly modulation of the data: increase the speech frequency, add background noise, alter the pitch etc,...</li>\n",
            "</ul>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#extensions\" id=\"user-content-extensions\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Extensions</h3>\n",
            "<p><strong>Extensions</strong> to current tensorflow which are probably needed:</p>\n",
            "<ul>\n",
            "<li><a href=\"https://github.com/baidu-research/warp-ctc/tree/master/tensorflow_binding\">WarpCTC on the GPU</a> see <a href=\"https://github.com/tensorflow/tensorflow/issues/2146\">issue</a></li>\n",
            "<li>Incremental collaborative snapshots ('<a href=\"https://github.com/pannous/tensorpeers\">P2P learning</a>') !</li>\n",
            "<li>Modular graphs/models + persistance</li>\n",
            "</ul>\n",
            "<p>Even though this project is far from finished we hope it gives you some starting points.</p>\n",
            "<p>Looking for a tensorflow collaboration / consultant / deep learning contractor? Reach out to <a href=\"mailto:info@pannous.com?subject=contractor\">info@pannous.com</a></p>\n",
            "</article>\n",
            "<article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><pre><code>\tCode has been run on Google Colab, thanks Google for providing computational resources\n",
            "</code></pre>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#contents\" id=\"user-content-contents\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Contents</h4>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Natural Language Processing（自然语言处理）</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/finch#text-classification\">Text Classification（文本分类）</a></p>\n",
            "<ul>\n",
            "<li>\n",
            "<p>IMDB（ENG）</p>\n",
            "</li>\n",
            "<li>\n",
            "<p>CLUE Emotion Analysis Dataset (CHN)</p>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/finch#text-matching\">Text Matching（文本匹配）</a></p>\n",
            "<ul>\n",
            "<li>\n",
            "<p>SNLI（ENG）</p>\n",
            "</li>\n",
            "<li>\n",
            "<p>微众银行智能客服（CHN）</p>\n",
            "</li>\n",
            "<li>\n",
            "<p>蚂蚁金融语义相似度 (CHN)</p>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/finch#intent-detection-and-slot-filling\">Intent Detection and Slot Filling（意图检测与槽位填充）</a></p>\n",
            "<ul>\n",
            "<li>ATIS（ENG）</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/finch#retrieval-dialog\">Retrieval Dialog（检索式对话）</a></p>\n",
            "<ul>\n",
            "<li>\n",
            "<p>ElasticSearch</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Sparse Retrieval</p>\n",
            "</li>\n",
            "<li>\n",
            "<p>Dense Retrieval</p>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/finch#generative-dialog\">Generative Dialog（生成式对话）</a></p>\n",
            "<ul>\n",
            "<li>Large-scale Chinese Conversation Dataset (CHN)</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/finch#multi-turn-dialogue-rewriting\">Multi-turn Dialogue Rewriting（多轮对话改写）</a></p>\n",
            "<ul>\n",
            "<li>20k 腾讯 AI 研发数据（CHN）</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/finch#semantic-parsing\">Semantic Parsing（语义解析）</a></p>\n",
            "<ul>\n",
            "<li>Facebook's Hierarchical Task Oriented Dialog（ENG）</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/finch#multi-hop-question-answering\">Multi-hop Question Answering（多跳问题回答）</a></p>\n",
            "<ul>\n",
            "<li>bAbI（ENG）</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/finch#text-visualization\">Text Visualization（文本可视化）</a></p>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Topic Modelling</p>\n",
            "</li>\n",
            "<li>\n",
            "<p>Explain Prediction</p>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p>Knowledge Graph（知识图谱）</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/finch#knowledge-graph-completion\">Knowledge Graph Completion（知识图谱补全）</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/finch#knowledge-base-question-answering\">Knowledge Base Question Answering（知识图谱问答）</a></p>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/finch#recommender-system\">Recommender System（推荐系统）</a></p>\n",
            "<ul>\n",
            "<li>Movielens 1M（English Data）</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "<hr/>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#text-classification\" id=\"user-content-text-classification\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Text Classification</h2>\n",
            "<pre><code>└── finch/tensorflow2/text_classification/imdb\n",
            "\t│\n",
            "\t├── data\n",
            "\t│   └── glove.840B.300d.txt          # pretrained embedding, download and put here\n",
            "\t│   └── make_data.ipynb              # step 1. make data and vocab: train.txt, test.txt, word.txt\n",
            "\t│   └── train.txt  \t\t     # incomplete sample, format &lt;label, text&gt; separated by \\t \n",
            "\t│   └── test.txt   \t\t     # incomplete sample, format &lt;label, text&gt; separated by \\t\n",
            "\t│   └── train_bt_part1.txt  \t     # (back-translated) incomplete sample, format &lt;label, text&gt; separated by \\t\n",
            "\t│\n",
            "\t├── vocab\n",
            "\t│   └── word.txt                     # incomplete sample, list of words in vocabulary\n",
            "\t│\t\n",
            "\t└── main\n",
            "\t\t└── sliced_rnn.ipynb         # step 2: train and evaluate model\n",
            "\t\t└── ...\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\" rel=\"nofollow\">IMDB</a>（English Data）</p>\n",
            "<pre><code>  Training Data: 25000, Testing Data: 25000, Labels: 2\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_classification/imdb/data/make_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Make Data &amp; Vocabulary</a></p>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_classification/imdb/data/train.txt\" rel=\"nofollow\">&lt;Text File&gt;: Data Example</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_classification/imdb/data/train_bt_part1.txt\" rel=\"nofollow\">&lt;Text File&gt;: Data Example (Back-Translated)</a></p>\n",
            "</li>\n",
            "</ul>\n",
            "<pre><code> Back-Translation increases training data from 25000 to 50000\n",
            " \n",
            " which is done by \"english -&gt; french -&gt; english\" translation\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_classification/imdb/vocab/word.txt\" rel=\"nofollow\">&lt;Text File&gt;: Vocabulary Example</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p>Model: TF-IDF + Logistic Regression (<a href=\"https://scikit-learn.org/stable/\" rel=\"nofollow\">Sklearn</a>)</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Logistic Regression</th>\n",
            "<th>Binary TF</th>\n",
            "<th>NGram Range</th>\n",
            "<th>Knowledge Dist</th>\n",
            "<th>Testing Accuracy</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/sklearn/text_classification/imdb/tfidf_lr_binary_false.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>False</td>\n",
            "<td>(1, 1)</td>\n",
            "<td>False</td>\n",
            "<td>88.3%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/sklearn/text_classification/imdb/tfidf_lr_binary_true.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>True</td>\n",
            "<td>(1, 1)</td>\n",
            "<td>False</td>\n",
            "<td>88.8%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/sklearn/text_classification/imdb/tfidf_lr_binary_true_bigram.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>True</td>\n",
            "<td>(1, 2)</td>\n",
            "<td>False</td>\n",
            "<td>89.6%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_classification/imdb/main/lr_know_dist.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>True</td>\n",
            "<td>(1, 2)</td>\n",
            "<td>True</td>\n",
            "<td>90.7%</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<p>-&gt; <a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/spark/text_classification/imdb/tfidf_lr.ipynb\" rel=\"nofollow\">PySpark</a> Equivalent</p>\n",
            "</li>\n",
            "<li>\n",
            "<p>Model: <a href=\"https://arxiv.org/abs/1607.01759\" rel=\"nofollow\">FastText</a>, CNN and RNN</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Model</th>\n",
            "<th>Testing Accuracy</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/framework/official_fasttext/text_classification/imdb/unigram.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>FastText (Unigram)</td>\n",
            "<td>87.3%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/framework/official_fasttext/text_classification/imdb/bigram.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>FastText (Bigram)</td>\n",
            "<td>89.8%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/framework/official_fasttext/text_classification/imdb/autotune.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>FastText (AutoTune)</td>\n",
            "<td>90.1%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_classification/imdb/main/cnn_attention_bt_char_label_smooth_cyclical.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1408.5882\" rel=\"nofollow\">TextCNN</a></td>\n",
            "<td>91.8%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/imdb/main/sliced_rnn_bt_char_label_smooth_clr.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1807.02291\" rel=\"nofollow\">Sliced RNN</a></td>\n",
            "<td>92.6%</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "<li>\n",
            "<p>Model: Large-scale Transformer</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p>TensorFlow 2 + <a href=\"https://github.com/huggingface/transformers\">transformers</a></p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Model</th>\n",
            "<th>Batch Size</th>\n",
            "<th>Max Length</th>\n",
            "<th>Testing Accuracy</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/imdb/main/bert_finetune_32_128.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">BERT</a></td>\n",
            "<td>32</td>\n",
            "<td>128</td>\n",
            "<td>92.6%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/imdb/main/bert_finetune_16_200.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>BERT</td>\n",
            "<td>16</td>\n",
            "<td>200</td>\n",
            "<td>93.3%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/imdb/main/bert_finetune_12_256.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>BERT</td>\n",
            "<td>12</td>\n",
            "<td>256</td>\n",
            "<td>93.8%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/imdb/main/bert_finetune_8_300.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>BERT</td>\n",
            "<td>8</td>\n",
            "<td>300</td>\n",
            "<td>94%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/imdb/main/roberta_finetune_8_300.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1907.11692\" rel=\"nofollow\">RoBERTa</a></td>\n",
            "<td>8</td>\n",
            "<td>300</td>\n",
            "<td>94.7%</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "<pre><code>└── finch/tensorflow2/text_classification/clue\n",
            "\t│\n",
            "\t├── data\n",
            "\t│   └── make_data.ipynb              # step 1. make data and vocab\n",
            "\t│   └── train.txt  \t\t     # download from clue benchmark\n",
            "\t│   └── test.txt   \t\t     # download from clue benchmark\n",
            "\t│\n",
            "\t├── vocab\n",
            "\t│   └── label.txt                    # list of emotion labels\n",
            "\t│\t\n",
            "\t└── main\n",
            "\t\t└── bert_finetune.ipynb      # step 2: train and evaluate model\n",
            "\t\t└── ...\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: <a href=\"https://github.com/CLUEbenchmark/CLUEmotionAnalysis2020\">CLUE Emotion Analysis Dataset</a>（Chinese Data）</p>\n",
            "<pre><code>  Training Data: 31728, Testing Data: 3967, Labels: 7\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Model: TF-IDF + Linear Model</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Logistic Regression</th>\n",
            "<th>Binary TF</th>\n",
            "<th>NGram Range</th>\n",
            "<th>Split By</th>\n",
            "<th>Testing Accuracy</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/clue/main/tfidf_lr_binary_f_char_unigram.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>False</td>\n",
            "<td>(1, 1)</td>\n",
            "<td>Char</td>\n",
            "<td>57.4%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/clue/main/tfidf_lr_binary_t_word_unigram.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>True</td>\n",
            "<td>(1, 1)</td>\n",
            "<td>Word</td>\n",
            "<td>57.7%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/clue/main/tfidf_lr_binary_f_word_bigram.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>False</td>\n",
            "<td>(1, 2)</td>\n",
            "<td>Word</td>\n",
            "<td>57.8%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/clue/main/tfidf_lr_binary_f_word_unigram.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>False</td>\n",
            "<td>(1, 1)</td>\n",
            "<td>Word</td>\n",
            "<td>58.3%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/clue/main/tfidf_lr_binary_t_char_bigram.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>True</td>\n",
            "<td>(1, 2)</td>\n",
            "<td>Char</td>\n",
            "<td>59.1%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/clue/main/tfidf_lr_binary_f_char_bigram.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>False</td>\n",
            "<td>(1, 2)</td>\n",
            "<td>Char</td>\n",
            "<td>59.4%</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "<li>\n",
            "<p>Model: Deep Model</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Model</th>\n",
            "<th>Env</th>\n",
            "<th>Testing Accuracy</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/clue/main/bert_finetune.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">BERT</a></td>\n",
            "<td>TF2</td>\n",
            "<td>61.7%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/clue/main/bert_further_pretrain_finetune.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>BERT + <a href=\"https://arxiv.org/abs/2004.10964\" rel=\"nofollow\">TAPT</a> (<a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_classification/clue/main/bert_further_pretrain.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a>)</td>\n",
            "<td>TF2</td>\n",
            "<td>62.3%</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "<hr/>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#text-matching\" id=\"user-content-text-matching\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Text Matching</h2>\n",
            "<pre><code>└── finch/tensorflow2/text_matching/snli\n",
            "\t│\n",
            "\t├── data\n",
            "\t│   └── glove.840B.300d.txt       # pretrained embedding, download and put here\n",
            "\t│   └── download_data.ipynb       # step 1. run this to download snli dataset\n",
            "\t│   └── make_data.ipynb           # step 2. run this to generate train.txt, test.txt, word.txt \n",
            "\t│   └── train.txt  \t\t  # incomplete sample, format &lt;label, text1, text2&gt; separated by \\t \n",
            "\t│   └── test.txt   \t\t  # incomplete sample, format &lt;label, text1, text2&gt; separated by \\t\n",
            "\t│\n",
            "\t├── vocab\n",
            "\t│   └── word.txt                  # incomplete sample, list of words in vocabulary\n",
            "\t│\t\n",
            "\t└── main              \n",
            "\t\t└── dam.ipynb      \t  # step 3. train and evaluate model\n",
            "\t\t└── esim.ipynb      \t  # step 3. train and evaluate model\n",
            "\t\t└── ......\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: <a href=\"https://nlp.stanford.edu/projects/snli/\" rel=\"nofollow\">SNLI</a>（English Data）</p>\n",
            "<pre><code>  Training Data: 550152, Testing Data: 10000, Labels: 3\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/snli/data/download_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Download Data</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/snli/data/make_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Make Data &amp; Vocabulary</a></p>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/snli/data/train.txt\" rel=\"nofollow\">&lt;Text File&gt;: Data Example</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/snli/vocab/word.txt\" rel=\"nofollow\">&lt;Text File&gt;: Vocabulary Example</a></p>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Reference</th>\n",
            "<th>Env</th>\n",
            "<th>Testing Accuracy</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/snli/main/dam.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1606.01933\" rel=\"nofollow\">DAM</a></td>\n",
            "<td>TF2</td>\n",
            "<td>85.3%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/snli/main/pyramid_multi_attn.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1602.06359\" rel=\"nofollow\">Match Pyramid</a></td>\n",
            "<td>TF2</td>\n",
            "<td>87.1%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/snli/main/esim.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1609.06038\" rel=\"nofollow\">ESIM</a></td>\n",
            "<td>TF2</td>\n",
            "<td>87.4%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/snli/main/re2_birnn.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1908.00300\" rel=\"nofollow\">RE2</a></td>\n",
            "<td>TF2</td>\n",
            "<td>87.7%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/snli/main/re2_3_birnn_label_smooth.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>RE3</td>\n",
            "<td>TF2</td>\n",
            "<td>88.3%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/snli/main/bert_finetune.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">BERT</a></td>\n",
            "<td>TF2</td>\n",
            "<td>90.4%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/snli/main/roberta_finetune.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1907.11692\" rel=\"nofollow\">RoBERTa</a></td>\n",
            "<td>TF2</td>\n",
            "<td>91.1%</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "</ul>\n",
            "<pre><code>└── finch/tensorflow2/text_matching/chinese\n",
            "\t│\n",
            "\t├── data\n",
            "\t│   └── make_data.ipynb           # step 1. run this to generate char.txt and char.npy\n",
            "\t│   └── train.csv  \t\t  # incomplete sample, format &lt;text1, text2, label&gt; separated by comma \n",
            "\t│   └── test.csv   \t\t  # incomplete sample, format &lt;text1, text2, label&gt; separated by comma\n",
            "\t│\n",
            "\t├── vocab\n",
            "\t│   └── cc.zh.300.vec             # pretrained embedding, download and put here\n",
            "\t│   └── char.txt                  # incomplete sample, list of chinese characters\n",
            "\t│   └── char.npy                  # saved pretrained embedding matrix for this task\n",
            "\t│\t\n",
            "\t└── main              \n",
            "\t\t└── pyramid.ipynb      \t  # step 2. train and evaluate model\n",
            "\t\t└── esim.ipynb      \t  # step 2. train and evaluate model\n",
            "\t\t└── ......\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: <a href=\"https://github.com/terrifyzhao/text_matching/tree/master/input\">微众银行智能客服</a>（Chinese Data）</p>\n",
            "<pre><code>  Training Data: 100000, Testing Data: 10000, Labels: 2, Balanced\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/chinese/data/make_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Make Data &amp; Vocabulary</a></p>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/chinese/data/train.csv\">&lt;Text File&gt;: Data Example</a> (数据示例)</p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/chinese/vocab/char.txt\">&lt;Text File&gt;: Vocabulary</a></p>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p>Model\t(can be compared to <a href=\"https://github.com/wangle1218/deep_text_matching\">this benchmark</a> since the dataset is the same)</p>\n",
            "</li>\n",
            "</ul>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Reference</th>\n",
            "<th>Env</th>\n",
            "<th>Split by</th>\n",
            "<th>Testing Accuracy</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/chinese/main/word_re2_cyclical_label_smooth.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1908.00300\" rel=\"nofollow\">RE2</a></td>\n",
            "<td>TF2</td>\n",
            "<td>Word</td>\n",
            "<td>82.5%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/chinese/main/esim.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1609.06038\" rel=\"nofollow\">ESIM</a></td>\n",
            "<td>TF2</td>\n",
            "<td>Char</td>\n",
            "<td>82.5%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/text_matching/chinese/main/pyramid.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1602.06359\" rel=\"nofollow\">Match Pyramid</a></td>\n",
            "<td>TF2</td>\n",
            "<td>Char</td>\n",
            "<td>82.7%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/chinese/main/re2_cyclical_label_smooth.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1908.00300\" rel=\"nofollow\">RE2</a></td>\n",
            "<td>TF2</td>\n",
            "<td>Char</td>\n",
            "<td>83.8%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/chinese/main/bert_finetune.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">BERT</a></td>\n",
            "<td>TF2</td>\n",
            "<td>Char</td>\n",
            "<td>83.8%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/text_matching/chinese/main/bert_chinese_wwm.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://github.com/ymcui/Chinese-BERT-wwm\">BERT-wwm</a></td>\n",
            "<td>TF1 + <a href=\"https://github.com/bojone/bert4keras\">bert4keras</a></td>\n",
            "<td>Char</td>\n",
            "<td>84.75%</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "</ul>\n",
            "<pre><code>└── finch/tensorflow2/text_matching/ant\n",
            "\t│\n",
            "\t├── data\n",
            "\t│   └── make_data.ipynb           # step 1. run this to generate char.txt and char.npy\n",
            "\t│   └── train.json           \t  # incomplete sample, format &lt;text1, text2, label&gt; separated by comma \n",
            "\t│   └── dev.json   \t\t  # incomplete sample, format &lt;text1, text2, label&gt; separated by comma\n",
            "\t│\n",
            "\t├── vocab\n",
            "\t│   └── cc.zh.300.vec             # pretrained embedding, download and put here\n",
            "\t│   └── char.txt                  # incomplete sample, list of chinese characters\n",
            "\t│   └── char.npy                  # saved pretrained embedding matrix for this task\n",
            "\t│\t\n",
            "\t└── main              \n",
            "\t\t└── pyramid.ipynb      \t  # step 2. train and evaluate model\n",
            "\t\t└── bert.ipynb      \t  # step 2. train and evaluate model\n",
            "\t\t└── ......\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: <a href=\"https://cluebenchmarks.com/introduce.html\" rel=\"nofollow\">蚂蚁金融语义相似度</a>（Chinese Data）</p>\n",
            "<pre><code>  Training Data: 34334, Testing Data: 4316, Labels: 2, Imbalanced\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/ant/data/make_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Make Data &amp; Vocabulary</a></p>\n",
            "<ul>\n",
            "<li><a href=\"https://github.com/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/ant/vocab/char.txt\">&lt;Text File&gt;: Vocabulary</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p>Model</p>\n",
            "</li>\n",
            "</ul>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Reference</th>\n",
            "<th>Env</th>\n",
            "<th>Split by</th>\n",
            "<th>Testing Accuracy</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/ant/main/re2.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1908.00300\" rel=\"nofollow\">RE2</a></td>\n",
            "<td>TF2</td>\n",
            "<td>Char</td>\n",
            "<td>66.5%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/ant/main/pyramid.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1602.06359\" rel=\"nofollow\">Match Pyramid</a></td>\n",
            "<td>TF2</td>\n",
            "<td>Char</td>\n",
            "<td>69.0%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/joint/main/pyramid.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Match Pyramid + Joint Training</td>\n",
            "<td>TF2</td>\n",
            "<td>Char</td>\n",
            "<td>70.3%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/ant/main/bert.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">BERT</a></td>\n",
            "<td>TF2</td>\n",
            "<td>Char</td>\n",
            "<td>73.8%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/ant/main/bert_further_pretrain_finetune.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>BERT + <a href=\"https://arxiv.org/abs/2004.10964\" rel=\"nofollow\">TAPT</a> (<a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/ant/main/bert_further_pretrain.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a>)</td>\n",
            "<td>TF2</td>\n",
            "<td>Char</td>\n",
            "<td>74.3%</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "<li>\n",
            "<p>Joint training</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p>set data_1 = 微众银行智能客服 (size: 100000)</p>\n",
            "</li>\n",
            "<li>\n",
            "<p>set data_2 = 蚂蚁金融语义相似度 (size: 34334)</p>\n",
            "</li>\n",
            "<li>\n",
            "<p>joint training (size: 100000 + 34334 = 134334)</p>\n",
            "</li>\n",
            "</ul>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>BERT</th>\n",
            "<th>train by data_1</th>\n",
            "<th>train by data_2</th>\n",
            "<th>joint train</th>\n",
            "<th>joint train + <a href=\"https://arxiv.org/abs/2004.10964\" rel=\"nofollow\">TAPT</a></th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td>Code</td>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/chinese/main/bert_finetune.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/ant/main/bert.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/joint/main/bert.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/text_matching/joint/main/bert_tapt.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>data_1 accuracy</td>\n",
            "<td>83.8%</td>\n",
            "<td>-</td>\n",
            "<td>84.4%</td>\n",
            "<td>85.0%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>data_2 accuracy</td>\n",
            "<td>-</td>\n",
            "<td>73.8%</td>\n",
            "<td>74.0%</td>\n",
            "<td>74.9%</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "</ul>\n",
            "<hr/>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#intent-detection-and-slot-filling\" id=\"user-content-intent-detection-and-slot-filling\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Intent Detection and Slot Filling</h2>\n",
            "<p><a href=\"https://camo.githubusercontent.com/1769c70609e04ed4ed93bb116addbd152d6b81601a6b871e82cdc077f5190ec8/68747470733a2f2f7975616e7869616f73632e6769746875622e696f2f323031392f30332f31382f2545362541372542442545352541312541422545352538352538352545352539322538432545362538342538462545352539422542452545382541462538362545352538382541422545342542422542422545352538412541312545372539412538342545352539462542412545362539432541432545362541362538322545352542462542352f312e706e67\" rel=\"noopener noreferrer\" target=\"_blank\"><img data-canonical-src=\"https://yuanxiaosc.github.io/2019/03/18/%E6%A7%BD%E5%A1%AB%E5%85%85%E5%92%8C%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/1.png\" height=\"300\" src=\"https://camo.githubusercontent.com/1769c70609e04ed4ed93bb116addbd152d6b81601a6b871e82cdc077f5190ec8/68747470733a2f2f7975616e7869616f73632e6769746875622e696f2f323031392f30332f31382f2545362541372542442545352541312541422545352538352538352545352539322538432545362538342538462545352539422542452545382541462538362545352538382541422545342542422542422545352538412541312545372539412538342545352539462542412545362539432541432545362541362538322545352542462542352f312e706e67\" style=\"max-width:100%;\"/></a></p>\n",
            "<pre><code>└── finch/tensorflow2/spoken_language_understanding/atis\n",
            "\t│\n",
            "\t├── data\n",
            "\t│   └── glove.840B.300d.txt           # pretrained embedding, download and put here\n",
            "\t│   └── make_data.ipynb               # step 1. run this to generate vocab: word.txt, intent.txt, slot.txt \n",
            "\t│   └── atis.train.w-intent.iob       # incomplete sample, format &lt;text, slot, intent&gt;\n",
            "\t│   └── atis.test.w-intent.iob        # incomplete sample, format &lt;text, slot, intent&gt;\n",
            "\t│\n",
            "\t├── vocab\n",
            "\t│   └── word.txt                      # list of words in vocabulary\n",
            "\t│   └── intent.txt                    # list of intents in vocabulary\n",
            "\t│   └── slot.txt                      # list of slots in vocabulary\n",
            "\t│\t\n",
            "\t└── main              \n",
            "\t\t└── bigru_clr.ipynb               # step 2. train and evaluate model\n",
            "\t\t└── ...\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: <a href=\"https://github.com/yvchen/JointSLU/tree/master/data\">ATIS</a>（English Data）</p>\n",
            "<pre><code>  Training Data: 4978, Testing Data: 893\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/spoken_language_understanding/atis/data/atis.train.w-intent.iob\" rel=\"nofollow\">&lt;Text File&gt;: Data Example</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/spoken_language_understanding/atis/data/make_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Make Vocabulary</a></p>\n",
            "<ul>\n",
            "<li><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/spoken_language_understanding/atis/vocab/word.txt\" rel=\"nofollow\">&lt;Text File&gt;: Vocabulary Example</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Model</th>\n",
            "<th>Helper</th>\n",
            "<th>Env</th>\n",
            "<th>Intent Accuracy</th>\n",
            "<th>Slot Micro-F1</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/python/atis/main/crfsuite.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf\" rel=\"nofollow\">CRF</a></td>\n",
            "<td>-</td>\n",
            "<td><a href=\"https://github.com/scrapinghub/python-crfsuite\">crfsuite</a></td>\n",
            "<td>-</td>\n",
            "<td>92.6%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/spoken_language_understanding/atis/main/bigru_clr.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://www.ijcai.org/Proceedings/16/Papers/425.pdf\" rel=\"nofollow\">Bi-GRU</a></td>\n",
            "<td>-</td>\n",
            "<td>TF2</td>\n",
            "<td>97.4%</td>\n",
            "<td>95.4%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/spoken_language_understanding/atis/main/bigru_clr_crf.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://www.ijcai.org/Proceedings/16/Papers/425.pdf\" rel=\"nofollow\">Bi-GRU</a></td>\n",
            "<td>+ CRF</td>\n",
            "<td>TF2</td>\n",
            "<td>97.2%</td>\n",
            "<td>95.8%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/spoken_language_understanding/atis/main/transformer.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1706.03762\" rel=\"nofollow\">Transformer</a></td>\n",
            "<td>-</td>\n",
            "<td>TF2</td>\n",
            "<td>96.5%</td>\n",
            "<td>95.5%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/spoken_language_understanding/atis/main/transformer_time_weight.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Transformer</td>\n",
            "<td>+ <a href=\"https://github.com/BlinkDL/minGPT-tuned\">Time Weighting</a></td>\n",
            "<td>TF2</td>\n",
            "<td>97.2%</td>\n",
            "<td>95.6%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/spoken_language_understanding/atis/main/transformer_time_mixing.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Transformer</td>\n",
            "<td>+ <a href=\"https://github.com/BlinkDL/minGPT-tuned\">Time Mixing</a></td>\n",
            "<td>TF2</td>\n",
            "<td>97.5%</td>\n",
            "<td>95.8%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/spoken_language_understanding/atis/main/elmo_o1_bigru.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Bi-GRU</td>\n",
            "<td>+ <a href=\"https://arxiv.org/abs/1802.05365\" rel=\"nofollow\">ELMO</a></td>\n",
            "<td>TF1</td>\n",
            "<td>97.5%</td>\n",
            "<td>96.1%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/spoken_language_understanding/atis/main/elmo_o1_bigru_crf.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Bi-GRU</td>\n",
            "<td>+ ELMO + CRF</td>\n",
            "<td>TF1</td>\n",
            "<td>97.3%</td>\n",
            "<td>96.3%</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "</ul>\n",
            "<hr/>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#retrieval-dialog\" id=\"user-content-retrieval-dialog\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Retrieval Dialog</h2>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: Build a chatbot answering fundamental questions</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Engine</th>\n",
            "<th>Encoder</th>\n",
            "<th>Vector Type</th>\n",
            "<th>Unit Test Accuracy</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/es/free_chat/main/default_retrieve.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Elastic Search</td>\n",
            "<td>Default (TF-IDF)</td>\n",
            "<td>Sparse</td>\n",
            "<td>80%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/es/free_chat/main/default_retrieve_seg.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Elastic Search</td>\n",
            "<td>Default (TF-IDF) + <a href=\"https://github.com/medcl/elasticsearch-analysis-ik\">Segmentation</a></td>\n",
            "<td>Sparse</td>\n",
            "<td>90%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/es/free_chat/main/dense_retrieve_bert_hub.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Elastic Search</td>\n",
            "<td><a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">Bert</a></td>\n",
            "<td>Dense</td>\n",
            "<td>80%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/es/free_chat/main/dense_retrieve.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Elastic Search</td>\n",
            "<td><a href=\"https://arxiv.org/abs/1907.04307\" rel=\"nofollow\">Universal Sentence Encoder</a></td>\n",
            "<td>Dense</td>\n",
            "<td>100%</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "</ul>\n",
            "<hr/>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#semantic-parsing\" id=\"user-content-semantic-parsing\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Semantic Parsing</h2>\n",
            "<p><a href=\"https://camo.githubusercontent.com/b834db00dcd5ca4e567157b5de8e708c00298bb8700999de336bc6331e1aa5b2/68747470733a2f2f706963332e7a68696d672e636f6d2f76322d66613263646363656538633732356166343235363462333737343162613437615f622e6a7067\" rel=\"noopener noreferrer\" target=\"_blank\"><img data-canonical-src=\"https://pic3.zhimg.com/v2-fa2cdccee8c725af42564b37741ba47a_b.jpg\" src=\"https://camo.githubusercontent.com/b834db00dcd5ca4e567157b5de8e708c00298bb8700999de336bc6331e1aa5b2/68747470733a2f2f706963332e7a68696d672e636f6d2f76322d66613263646363656538633732356166343235363462333737343162613437615f622e6a7067\" style=\"max-width:100%;\"/></a></p>\n",
            "<pre><code>└── finch/tensorflow2/semantic_parsing/tree_slu\n",
            "\t│\n",
            "\t├── data\n",
            "\t│   └── glove.840B.300d.txt     \t# pretrained embedding, download and put here\n",
            "\t│   └── make_data.ipynb           \t# step 1. run this to generate vocab: word.txt, intent.txt, slot.txt \n",
            "\t│   └── train.tsv   \t\t  \t# incomplete sample, format &lt;text, tokenized_text, tree&gt;\n",
            "\t│   └── test.tsv    \t\t  \t# incomplete sample, format &lt;text, tokenized_text, tree&gt;\n",
            "\t│\n",
            "\t├── vocab\n",
            "\t│   └── source.txt                \t# list of words in vocabulary for source (of seq2seq)\n",
            "\t│   └── target.txt                \t# list of words in vocabulary for target (of seq2seq)\n",
            "\t│\t\n",
            "\t└── main\n",
            "\t\t└── lstm_seq2seq_tf_addons.ipynb           # step 2. train and evaluate model\n",
            "\t\t└── ......\n",
            "\t\t\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: <a href=\"https://arxiv.org/abs/1810.07942\" rel=\"nofollow\">Semantic Parsing for Task Oriented Dialog</a>（English Data）</p>\n",
            "<pre><code>  Training Data: 31279, Testing Data: 9042\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/finch/blob/master/finch/tensorflow2/semantic_parsing/tree_slu/data/train.tsv\">&lt;Text File&gt;: Data Example</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/semantic_parsing/tree_slu/data/make_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Make Vocabulary</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/semantic_parsing/tree_slu/vocab/target.txt\" rel=\"nofollow\">&lt;Text File&gt;: Vocabulary Example</a></p>\n",
            "</li>\n",
            "</ul>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Reference</th>\n",
            "<th>Env</th>\n",
            "<th>Testing Exact Match</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/semantic_parsing/tree_slu/main/gru_seq2seq_tf_addons_clr.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1606.01933\" rel=\"nofollow\">GRU Seq2Seq</a></td>\n",
            "<td>TF2</td>\n",
            "<td>74.1%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/semantic_parsing/tree_slu/main/lstm_seq2seq_tf_addons_clr.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>LSTM Seq2Seq</td>\n",
            "<td>TF2</td>\n",
            "<td>74.1%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/semantic_parsing/tree_slu/main/gru_pointer_tf_addons_clr.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1704.04368\" rel=\"nofollow\">GRU Pointer-Generator</a></td>\n",
            "<td>TF2</td>\n",
            "<td>80.4%</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow2/semantic_parsing/tree_slu/main/gru_pointer_tf_addons_clr_char.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>GRU Pointer-Generator + Char Embedding</td>\n",
            "<td>TF2</td>\n",
            "<td>80.7%</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<p>The Exact Match result is higher than <a href=\"https://arxiv.org/abs/1810.07942\" rel=\"nofollow\">original paper</a></p>\n",
            "</li>\n",
            "</ul>\n",
            "<hr/>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#knowledge-graph-completion\" id=\"user-content-knowledge-graph-completion\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Knowledge Graph Completion</h2>\n",
            "<pre><code>└── finch/tensorflow2/knowledge_graph_completion/wn18\n",
            "\t│\n",
            "\t├── data\n",
            "\t│   └── download_data.ipynb       \t# step 1. run this to download wn18 dataset\n",
            "\t│   └── make_data.ipynb           \t# step 2. run this to generate vocabulary: entity.txt, relation.txt\n",
            "\t│   └── wn18  \t\t          \t# wn18 folder (will be auto created by download_data.ipynb)\n",
            "\t│   \t└── train.txt  \t\t  \t# incomplete sample, format &lt;entity1, relation, entity2&gt; separated by \\t\n",
            "\t│   \t└── valid.txt  \t\t  \t# incomplete sample, format &lt;entity1, relation, entity2&gt; separated by \\t \n",
            "\t│   \t└── test.txt   \t\t  \t# incomplete sample, format &lt;entity1, relation, entity2&gt; separated by \\t\n",
            "\t│\n",
            "\t├── vocab\n",
            "\t│   └── entity.txt                  \t# incomplete sample, list of entities in vocabulary\n",
            "\t│   └── relation.txt                \t# incomplete sample, list of relations in vocabulary\n",
            "\t│\t\n",
            "\t└── main              \n",
            "\t\t└── distmult_1-N.ipynb    \t# step 3. train and evaluate model\n",
            "\t\t└── ...\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: WN18</p>\n",
            "<pre><code>  Training Data: 141442, Testing Data: 5000\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/knowledge_graph_completion/wn18/data/download_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Download Data</a></p>\n",
            "<ul>\n",
            "<li><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/knowledge_graph_completion/wn18/data/wn18/train.txt\" rel=\"nofollow\">&lt;Text File&gt;: Data Example</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/knowledge_graph_completion/wn18/data/make_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Make Vocabulary</a></p>\n",
            "<ul>\n",
            "<li><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/knowledge_graph_completion/wn18/vocab/relation.txt\" rel=\"nofollow\">&lt;Text File&gt;: Vocabulary Example</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p>We use the idea of <a href=\"https://arxiv.org/abs/1707.01476\" rel=\"nofollow\">multi-label classification</a> to accelerate evaluation</p>\n",
            "<a href=\"https://camo.githubusercontent.com/393ef94be089e4f95201090f0d91f3d09afee95402a206df32df239ae46d9b6a/68747470733a2f2f706963342e7a68696d672e636f6d2f38302f76322d38636438343831383536663130316166343535303130373862303434353662625f373230772e6a7067\" rel=\"noopener noreferrer\" target=\"_blank\"><img data-canonical-src=\"https://pic4.zhimg.com/80/v2-8cd8481856f101af45501078b04456bb_720w.jpg\" src=\"https://camo.githubusercontent.com/393ef94be089e4f95201090f0d91f3d09afee95402a206df32df239ae46d9b6a/68747470733a2f2f706963342e7a68696d672e636f6d2f38302f76322d38636438343831383536663130316166343535303130373862303434353662625f373230772e6a7067\" style=\"max-width:100%;\"/></a>\n",
            "</li>\n",
            "</ul>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Reference</th>\n",
            "<th>Env</th>\n",
            "<th>MRR</th>\n",
            "<th>Hits@10</th>\n",
            "<th>Hits@3</th>\n",
            "<th>Hits@1</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/knowledge_graph_completion/wn18/main/distmult_1-N_clr.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1412.6575\" rel=\"nofollow\">DistMult</a></td>\n",
            "<td>TF2</td>\n",
            "<td>0.797</td>\n",
            "<td>0.938</td>\n",
            "<td>0.902</td>\n",
            "<td>0.688</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/knowledge_graph_completion/wn18/main/tucker_1-N_clr.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1901.09590\" rel=\"nofollow\">TuckER</a></td>\n",
            "<td>TF2</td>\n",
            "<td>0.885</td>\n",
            "<td>0.939</td>\n",
            "<td>0.909</td>\n",
            "<td>0.853</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow2/knowledge_graph_completion/wn18/main/complex_1-N_clr.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td><a href=\"https://arxiv.org/abs/1606.06357\" rel=\"nofollow\">ComplEx</a></td>\n",
            "<td>TF2</td>\n",
            "<td>0.938</td>\n",
            "<td>0.958</td>\n",
            "<td>0.948</td>\n",
            "<td>0.925</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "</ul>\n",
            "<hr/>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#knowledge-base-question-answering\" id=\"user-content-knowledge-base-question-answering\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Knowledge Base Question Answering</h2>\n",
            "<p><a href=\"https://camo.githubusercontent.com/65d7dbc8027f45113cfd2f213693bb50f75c93dc42ac5d22fc0ce03b69592d31/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31373734373839322d653939346564633335313862326435382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f73747269707c696d61676556696577322f322f772f383830\" rel=\"noopener noreferrer\" target=\"_blank\"><img data-canonical-src=\"https://upload-images.jianshu.io/upload_images/17747892-e994edc3518b2d58.png?imageMogr2/auto-orient/strip|imageView2/2/w/880\" height=\"350\" src=\"https://camo.githubusercontent.com/65d7dbc8027f45113cfd2f213693bb50f75c93dc42ac5d22fc0ce03b69592d31/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f31373734373839322d653939346564633335313862326435382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f73747269707c696d61676556696577322f322f772f383830\" style=\"max-width:100%;\"/></a></p>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Rule-based System（基于规则的系统）</p>\n",
            "<p>For example, we want to answer the following questions with car knowledge:</p>\n",
            "<pre><code> \tWhat is BMW?\n",
            "     \tI want to know about the BMW\n",
            "     \tPlease introduce the BMW to me\n",
            "     \tHow is the BMW?\n",
            "     \tHow is the BMW compared to the Benz?\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/python/kbqa/regex.ipynb\" rel=\"nofollow\">&lt;Notebook&gt; Regular Expression</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/python/kbqa/rule_based_qa.ipynb\" rel=\"nofollow\">&lt;Notebook&gt; Regular Expression + POS Feature</a></p>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "<hr/>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#multi-hop-question-answering\" id=\"user-content-multi-hop-question-answering\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Multi-hop Question Answering</h2>\n",
            "<p><a href=\"https://github.com/DSKSD/DeepNLP-models-Pytorch/blob/master/images/10.dmn-architecture.png\" rel=\"noopener noreferrer\" target=\"_blank\"><img src=\"https://github.com/DSKSD/DeepNLP-models-Pytorch/raw/master/images/10.dmn-architecture.png\" style=\"max-width:100%;\" width=\"500\"/></a></p>\n",
            "<pre><code>└── finch/tensorflow1/question_answering/babi\n",
            "\t│\n",
            "\t├── data\n",
            "\t│   └── make_data.ipynb           \t\t# step 1. run this to generate vocabulary: word.txt \n",
            "\t│   └── qa5_three-arg-relations_train.txt       # one complete example of babi dataset\n",
            "\t│   └── qa5_three-arg-relations_test.txt\t# one complete example of babi dataset\n",
            "\t│\n",
            "\t├── vocab\n",
            "\t│   └── word.txt                  \t\t# complete list of words in vocabulary\n",
            "\t│\t\n",
            "\t└── main              \n",
            "\t\t└── dmn_train.ipynb\n",
            "\t\t└── dmn_serve.ipynb\n",
            "\t\t└── attn_gru_cell.py\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: <a href=\"https://research.fb.com/downloads/babi/\" rel=\"nofollow\">bAbI</a>（English Data）</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/question_answering/babi/data/qa5_three-arg-relations_test.txt\" rel=\"nofollow\">&lt;Text File&gt;: Data Example</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/question_answering/babi/data/make_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Make Vocabulary</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p>Model: <a href=\"https://arxiv.org/abs/1603.01417\" rel=\"nofollow\">Dynamic Memory Network</a></p>\n",
            "<ul>\n",
            "<li>\n",
            "<p>TensorFlow 1</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/question_answering/babi/main/dmn_train.ipynb\" rel=\"nofollow\">&lt;Notebook&gt; DMN -&gt; 99.4% Testing Accuracy</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/question_answering/babi/main/dmn_serve.ipynb\" rel=\"nofollow\">Inference</a></p>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "<hr/>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#text-visualization\" id=\"user-content-text-visualization\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Text Visualization</h2>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Topic Modelling</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Model: TF-IDF + LDA</p>\n",
            "</li>\n",
            "<li>\n",
            "<p>Data: IMDB Movie Reviews</p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/sklearn/topic_modelling/imdb/lda.ipynb\" rel=\"nofollow\">&lt;Notebook&gt; Code</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/sklearn/topic_modelling/imdb/lda.html#topic=1&amp;lambda=1&amp;term=\" rel=\"nofollow\">&lt;Notebook&gt; Visualization</a></p>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p>Explain Prediction</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Model: <a href=\"https://github.com/slundberg/shap\">SHAP</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p>Data: IMDB Movie Reviews</p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/python/shap/imdb_linear.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></p>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "<hr/>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#recommender-system\" id=\"user-content-recommender-system\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Recommender System</h2>\n",
            "<p><a href=\"https://github.com/PaddlePaddle/book/blob/develop/05.recommender_system/image/rec_regression_network.png\" rel=\"noopener noreferrer\" target=\"_blank\"><img src=\"https://github.com/PaddlePaddle/book/raw/develop/05.recommender_system/image/rec_regression_network.png\" style=\"max-width:100%;\" width=\"500\"/></a></p>\n",
            "<pre><code>└── finch/tensorflow1/recommender/movielens\n",
            "\t│\n",
            "\t├── data\n",
            "\t│   └── make_data.ipynb           \t\t# run this to generate vocabulary\n",
            "\t│\n",
            "\t├── vocab\n",
            "\t│   └── user_job.txt\n",
            "\t│   └── user_id.txt\n",
            "\t│   └── user_gender.txt\n",
            "\t│   └── user_age.txt\n",
            "\t│   └── movie_types.txt\n",
            "\t│   └── movie_title.txt\n",
            "\t│   └── movie_id.txt\n",
            "\t│\t\n",
            "\t└── main              \n",
            "\t\t└── dnn_softmax.ipynb\n",
            "\t\t└── ......\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: <a href=\"https://grouplens.org/datasets/movielens/1m/\" rel=\"nofollow\">Movielens 1M</a>（English Data）</p>\n",
            "<pre><code>  Training Data: 900228, Testing Data: 99981, Users: 6000, Movies: 4000, Rating: 1-5\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/recommender/movielens/data/make_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Make Vocabulary</a></p>\n",
            "<ul>\n",
            "<li><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/recommender/movielens/data/train.txt\" rel=\"nofollow\">&lt;Text File&gt;: Data Example</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p>Model: <a href=\"https://www.paddlepaddle.org.cn/documentation/docs/en/1.5/beginners_guide/basics/recommender_system/index_en.html\" rel=\"nofollow\">Fusion</a></p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Scoring</th>\n",
            "<th>LR Decay</th>\n",
            "<th>Env</th>\n",
            "<th>Testing MAE</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/recommender/movielens/main/dnn_sigmoid.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Sigmoid (Continuous)</td>\n",
            "<td>Exponential</td>\n",
            "<td>TF1</td>\n",
            "<td>0.663</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/recommender/movielens/main/dnn_sigmoid_clr.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Sigmoid (Continuous)</td>\n",
            "<td>Cyclical</td>\n",
            "<td>TF1</td>\n",
            "<td>0.661</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/recommender/movielens/main/dnn_softmax.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Softmax (Discrete)</td>\n",
            "<td>Exponential</td>\n",
            "<td>TF1</td>\n",
            "<td>0.633</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/recommender/movielens/main/dnn_softmax_clr.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Softmax (Discrete)</td>\n",
            "<td>Cyclical</td>\n",
            "<td>TF1</td>\n",
            "<td>0.628</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<p>The MAE results seem better than the <a href=\"http://mymedialite.net/examples/datasets.html\" rel=\"nofollow\">all the results here</a> and <a href=\"https://test.pypi.org/project/scikit-surprise/\" rel=\"nofollow\">all the results here</a></p>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "<hr/>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#multi-turn-dialogue-rewriting\" id=\"user-content-multi-turn-dialogue-rewriting\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Multi-turn Dialogue Rewriting</h2>\n",
            "<p><a href=\"https://camo.githubusercontent.com/94621964932409a0639c1f9a031a0811e7671f32381e255ded762cc753731ef1/68747470733a2f2f706963312e7a68696d672e636f6d2f38302f76322d64383065666435376238316336656365393535613234376361373234376462345f31343430772e6a7067\" rel=\"noopener noreferrer\" target=\"_blank\"><img data-canonical-src=\"https://pic1.zhimg.com/80/v2-d80efd57b81c6ece955a247ca7247db4_1440w.jpg\" src=\"https://camo.githubusercontent.com/94621964932409a0639c1f9a031a0811e7671f32381e255ded762cc753731ef1/68747470733a2f2f706963312e7a68696d672e636f6d2f38302f76322d64383065666435376238316336656365393535613234376361373234376462345f31343430772e6a7067\" style=\"max-width:100%;\" width=\"600\"/></a></p>\n",
            "<pre><code>└── finch/tensorflow1/multi_turn_rewrite/chinese/\n",
            "\t│\n",
            "\t├── data\n",
            "\t│   └── make_data.ipynb         # run this to generate vocab, split train &amp; test data, make pretrained embedding\n",
            "\t│   └── corpus.txt\t\t# original data downloaded from external\n",
            "\t│   └── train_pos.txt\t\t# processed positive training data after {make_data.ipynb}\n",
            "\t│   └── train_neg.txt\t\t# processed negative training data after {make_data.ipynb}\n",
            "\t│   └── test_pos.txt\t\t# processed positive testing data after {make_data.ipynb}\n",
            "\t│   └── test_neg.txt\t\t# processed negative testing data after {make_data.ipynb}\n",
            "\t│\n",
            "\t├── vocab\n",
            "\t│   └── cc.zh.300.vec\t\t# fastText pretrained embedding downloaded from external\n",
            "\t│   └── char.npy\t\t# chinese characters and their embedding values (300 dim)\t\n",
            "\t│   └── char.txt\t\t# list of chinese characters used in this project \n",
            "\t│\t\n",
            "\t└── main              \n",
            "\t\t└── baseline_lstm_train.ipynb\n",
            "\t\t└── baseline_lstm_predict.ipynb\n",
            "\t\t└── ...\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: 20k 腾讯 AI 研发数据（Chinese Data）</p>\n",
            "<pre><code> data split as: training data (positive): 18986, testing data (positive): 1008\n",
            "\n",
            " Training data = 2 * 18986 because of 1:1 Negative Sampling\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/chin-gyou/dialogue-utterance-rewriter/blob/master/corpus.txt\">&lt;Text File&gt;: Full Data</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/multi_turn_rewrite/chinese/data/make_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Make Data &amp; Vocabulary &amp; Pretrained Embedding</a></p>\n",
            "<pre><code>  There are six incorrect data and we have deleted them\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/multi_turn_rewrite/chinese/data/train_pos.txt\">&lt;Text File&gt;: Positive Data Example</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/multi_turn_rewrite/chinese/data/train_neg.txt\">&lt;Text File&gt;: Negative Data Example</a></p>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p>Model (results can be compared to <a href=\"https://github.com/liu-nlper/dialogue-utterance-rewriter\">here</a> with the same dataset)</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Model</th>\n",
            "<th>Env</th>\n",
            "<th>Exact Match</th>\n",
            "<th>BLEU-1</th>\n",
            "<th>BLEU-2</th>\n",
            "<th>BLEU-4</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/multi_turn_rewrite/chinese/main/baseline_lstm_train_clr.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>LSTM Seq2Seq + <a href=\"https://arxiv.org/abs/1603.01417\" rel=\"nofollow\">Dynamic Memory</a></td>\n",
            "<td>TF1</td>\n",
            "<td>56.2%</td>\n",
            "<td>94.6</td>\n",
            "<td>89.1</td>\n",
            "<td>78.5</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/multi_turn_rewrite/chinese/main/baseline_gru_train_clr_multi_attn.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>GRU Seq2Seq + Dynamic Memory</td>\n",
            "<td>TF1</td>\n",
            "<td>56.2%</td>\n",
            "<td>95.0</td>\n",
            "<td>89.5</td>\n",
            "<td>78.9</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/multi_turn_rewrite/chinese/main/pointer_gru_train_clr.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>GRU <a href=\"https://arxiv.org/abs/1506.03134\" rel=\"nofollow\">Pointer</a></td>\n",
            "<td>TF1</td>\n",
            "<td>59.2%</td>\n",
            "<td>93.2</td>\n",
            "<td>87.7</td>\n",
            "<td>77.2</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/multi_turn_rewrite/chinese/main/pointer_gru_train_clr_multi_attn_.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>GRU <a href=\"https://arxiv.org/abs/1506.03134\" rel=\"nofollow\">Pointer</a> + Multi-Attention</td>\n",
            "<td>TF1</td>\n",
            "<td>60.2%</td>\n",
            "<td>94.2</td>\n",
            "<td>88.7</td>\n",
            "<td>78.3</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "<li>\n",
            "<p>Deployment: <a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/multi_turn_rewrite/chinese/main/baseline_lstm_export.ipynb\" rel=\"nofollow\">first export the model</a></p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Inference Code</th>\n",
            "<th>Environment</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/multi_turn_rewrite/chinese/main/baseline_lstm_predict.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Python</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://github.com/zhedongzheng/tensorflow-nlp/blob/master/finch/java/MultiDialogInference/src/ModelInference.java\">&lt;Notebook&gt;</a></td>\n",
            "<td>Java</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "<hr/>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#generative-dialog\" id=\"user-content-generative-dialog\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Generative Dialog</h2>\n",
            "<pre><code>└── finch/tensorflow1/free_chat/chinese_lccc\n",
            "\t│\n",
            "\t├── data\n",
            "\t│   └── LCCC-base.json           \t# raw data downloaded from external\n",
            "\t│   └── LCCC-base_test.json         # raw data downloaded from external\n",
            "\t│   └── make_data.ipynb           \t# step 1. run this to generate vocab {char.txt} and data {train.txt &amp; test.txt}\n",
            "\t│   └── train.txt           \t\t# processed text file generated by {make_data.ipynb}\n",
            "\t│   └── test.txt           \t\t\t# processed text file generated by {make_data.ipynb}\n",
            "\t│\n",
            "\t├── vocab\n",
            "\t│   └── char.txt                \t# list of chars in vocabulary for chinese\n",
            "\t│   └── cc.zh.300.vec\t\t\t# fastText pretrained embedding downloaded from external\n",
            "\t│   └── char.npy\t\t\t# chinese characters and their embedding values (300 dim)\t\n",
            "\t│\t\n",
            "\t└── main\n",
            "\t\t└── lstm_seq2seq_train.ipynb    # step 2. train and evaluate model\n",
            "\t\t└── lstm_seq2seq_infer.ipynb    # step 4. model inference\n",
            "\t\t└── ...\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Task: <a href=\"https://github.com/thu-coai/CDial-GPT\">Large-scale Chinese Conversation Dataset</a></p>\n",
            "<pre><code>  Training Data: 5000000 (sampled due to small memory), Testing Data: 19008\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p>Data</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/data/train.txt\">&lt;Text File&gt;: Data Example</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/free_chat/chinese_lccc/data/make_data.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;: Make Data &amp; Vocabulary</a></p>\n",
            "<ul>\n",
            "<li><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/finch/blob/master/finch/tensorflow1/free_chat/chinese_lccc/vocab/char.txt\" rel=\"nofollow\">&lt;Text File&gt;: Vocabulary Example</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p>Model</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Code</th>\n",
            "<th>Model</th>\n",
            "<th>Env</th>\n",
            "<th>Test Case</th>\n",
            "<th>Perplexity</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/transformer_train.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>Transformer Encoder + LSTM Generator</td>\n",
            "<td>TF1</td>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/transformer_infer.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>42.465</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/lstm_seq2seq_train.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>LSTM Encoder + LSTM Generator</td>\n",
            "<td>TF1</td>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/lstm_seq2seq_infer.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>41.250</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/lstm_pointer_train.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>LSTM Encoder + LSTM <a href=\"https://arxiv.org/abs/1704.04368\" rel=\"nofollow\">Pointer-Generator</a></td>\n",
            "<td>TF1</td>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/lstm_pointer_infer.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>36.525</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "<li>\n",
            "<p>If you want to deploy model in Java production</p>\n",
            "<pre><code> └── FreeChatInference\n",
            " \t│\n",
            " \t├── data\n",
            " \t│   └── transformer_export/\n",
            " \t│   └── char.txt\n",
            " \t│   └── libtensorflow-1.14.0.jar\n",
            " \t│   └── tensorflow_jni.dll\n",
            " \t│\n",
            " \t└── src              \n",
            " \t\t└── ModelInference.java\n",
            "</code></pre>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/zhedongzheng/tensorflow-nlp/blob/master/finch/java/FreeChatInference/src/ModelInference.java\">&lt;Notebook&gt; Java Inference</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p>If you don't know the input and output node names in Java, you can display the node names:</p>\n",
            "<pre><code> !saved_model_cli show --dir ../model/xxx/1587959473/ --tag_set serve --signature_def serving_default\n",
            "</code></pre>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>\n",
            "<p>Large Pre-trained <a href=\"https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe\" rel=\"nofollow\">GPT</a></p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Test Case</th>\n",
            "<th>Model</th>\n",
            "<th>Env</th>\n",
            "<th>Author</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/gpt_lccc_base.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>GPT LCCC base</td>\n",
            "<td>TF1 + <a href=\"https://github.com/bojone/bert4keras\">bert4keras</a></td>\n",
            "<td><a href=\"https://github.com/thu-coai/CDial-GPT\">Tsinghua University</a></td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/gpt_lccc_large.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>GPT LCCC large</td>\n",
            "<td>TF1 + <a href=\"https://github.com/bojone/bert4keras\">bert4keras</a></td>\n",
            "<td><a href=\"https://github.com/thu-coai/CDial-GPT\">Tsinghua University</a></td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://nbviewer.jupyter.org/github/zhedongzheng/tensorflow-nlp/blob/master/finch/tensorflow1/free_chat/chinese_lccc/main/nezha_gpt.ipynb\" rel=\"nofollow\">&lt;Notebook&gt;</a></td>\n",
            "<td>NEZHA LM</td>\n",
            "<td>TF1 + <a href=\"https://github.com/bojone/bert4keras\">bert4keras</a></td>\n",
            "<td><a href=\"https://github.com/bojone/nezha_gpt_dialog\">Jianlin Su</a></td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "<li>\n",
            "<p>Diverse Response Comparsion</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>问题 / 模型回复</th>\n",
            "<th>Our RNN</th>\n",
            "<th>GPT base</th>\n",
            "<th>GPT large</th>\n",
            "<th>Nezha</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td>Q: 你好</td>\n",
            "<td>A: 你好</td>\n",
            "<td>A: 你好</td>\n",
            "<td>A: 你好，你好，我是大好人。</td>\n",
            "<td>A: 您好</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 早上好</td>\n",
            "<td>A: 早上好</td>\n",
            "<td>A: 早上好，好久不见</td>\n",
            "<td>A: 早上好！</td>\n",
            "<td>A: 早啊早啊</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 晚上好</td>\n",
            "<td>A: 晚上好</td>\n",
            "<td>A: 你好</td>\n",
            "<td>A: 好久不见</td>\n",
            "<td>A: 晚上好！</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 再见</td>\n",
            "<td>A: 再见</td>\n",
            "<td>A: 不见</td>\n",
            "<td>A: 你是不是不爱我了</td>\n",
            "<td>A: 我要走了</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 好久不见</td>\n",
            "<td>A: 好久不见</td>\n",
            "<td>A: 是呀，好久不见。</td>\n",
            "<td>A: 是啊，好久不见。</td>\n",
            "<td>A: 是啊，最近怎么样啊</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 想死你了</td>\n",
            "<td>A: 我也是</td>\n",
            "<td>A: 我也想你，</td>\n",
            "<td>A: 我不信</td>\n",
            "<td>A: 哈哈哈哈哈</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 谢谢你</td>\n",
            "<td>A: 不客气</td>\n",
            "<td>A: 不用谢</td>\n",
            "<td>A: 不用</td>\n",
            "<td>A: 不谢不谢，</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 爱你</td>\n",
            "<td>A: 爱你</td>\n",
            "<td>A: 你是爱我</td>\n",
            "<td>A: 我也爱你，</td>\n",
            "<td>A: 我也爱你，你最美</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你叫什么名字</td>\n",
            "<td>A: 你猜</td>\n",
            "<td>A: 我也想知道</td>\n",
            "<td>A: 叫你</td>\n",
            "<td>A: 你是不是想说我的名字太俗了</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你几岁了</td>\n",
            "<td>A: 问女孩子年龄可不是绅士所为哦 ~</td>\n",
            "<td>A: 你猜</td>\n",
            "<td>A: 你猜</td>\n",
            "<td>A: 我不知道</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 现在几点了</td>\n",
            "<td>A: 11点</td>\n",
            "<td>A: 你说呢？</td>\n",
            "<td>A: 十二点</td>\n",
            "<td>A: 我也不知道啊</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 今天天气怎么样</td>\n",
            "<td>A: 下雨了</td>\n",
            "<td>A: 今天很冷，但是我还是觉得很冷</td>\n",
            "<td>A: 很好</td>\n",
            "<td>A: 不错</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我们现在在哪里</td>\n",
            "<td>A: 在这里</td>\n",
            "<td>A: 我在这里等你呢</td>\n",
            "<td>A: 我不知道你在说什么</td>\n",
            "<td>A: 在我们宿舍楼下。。。</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你能给我讲个笑话吗</td>\n",
            "<td>A: 不能</td>\n",
            "<td>A: 讲个笑话</td>\n",
            "<td>A: 讲给谁听</td>\n",
            "<td>A: 不能</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你是男孩还是女孩呀</td>\n",
            "<td>A: 我是萌妹纸</td>\n",
            "<td>A: 你猜</td>\n",
            "<td>A: 你猜呀</td>\n",
            "<td>A: 我是萌妹纸</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你会几种语言呀</td>\n",
            "<td>A: 你猜</td>\n",
            "<td>A: 不会</td>\n",
            "<td>A: 你猜呀~</td>\n",
            "<td>A: 只会说一句</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你能陪我玩吗</td>\n",
            "<td>A: 可以啊</td>\n",
            "<td>A: 可以</td>\n",
            "<td>A: 我可以啊</td>\n",
            "<td>A: 你猜</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 说话可以大声一点吗</td>\n",
            "<td>A: 可以</td>\n",
            "<td>A: 可以的</td>\n",
            "<td>A: 不要</td>\n",
            "<td>A: 我不</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 天气真好</td>\n",
            "<td>A: 是的</td>\n",
            "<td>A: 是啊！</td>\n",
            "<td>A: 天气好是因为有你</td>\n",
            "<td>A: 是啊。</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 天气太糟糕了</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 是的</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 下雨了</td>\n",
            "<td>A: 是的</td>\n",
            "<td>A: 你们那边也下雨了</td>\n",
            "<td>A: 是呀</td>\n",
            "<td>A: 我也是</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 雨好大</td>\n",
            "<td>A: 今天下雨了</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 是的，好大雨</td>\n",
            "<td>A: 对呀，我们还穿短袖</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我讨厌艳阳天</td>\n",
            "<td>A: 我也讨厌艳阳天</td>\n",
            "<td>A: 我也是。</td>\n",
            "<td>A: 我喜欢阴天</td>\n",
            "<td>A: 我喜欢</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 好晒啊</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 是啊！！</td>\n",
            "<td>A: 是啊，不是很热的</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 今天好冷</td>\n",
            "<td>A: 多穿点衣服</td>\n",
            "<td>A: 你们都不来</td>\n",
            "<td>A: 你那儿也冷了</td>\n",
            "<td>A: 是啊。</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 今天好热</td>\n",
            "<td>A: 今天热死了</td>\n",
            "<td>A: 热死我了</td>\n",
            "<td>A: 对啊</td>\n",
            "<td>A: 对啊，我都热疯</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 风好大</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 风太大了。。。</td>\n",
            "<td>A: 你不是在上课嘛</td>\n",
            "<td>A: 嗯嗯嗯，风好大，冷</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 雾太大了看不清路</td>\n",
            "<td>A: 雾太大了</td>\n",
            "<td>A: 哈哈，你这是在逗我笑</td>\n",
            "<td>A: 我也看不清</td>\n",
            "<td>A: 你是不是傻啊</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 打雷了好可怕</td>\n",
            "<td>A: 可怕</td>\n",
            "<td>A: 我们这边也是，我们这边下雨了，我们这边也没有下雨。</td>\n",
            "<td>A: 是啊</td>\n",
            "<td>A: 我们这边下雨了</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 下雪了诶</td>\n",
            "<td>A: 下雪了？</td>\n",
            "<td>A: 是滴！</td>\n",
            "<td>A: 嗯嗯嗯！！好大的雪！！</td>\n",
            "<td>A: 下了一天了</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 好烦啊</td>\n",
            "<td>A: 我也好烦</td>\n",
            "<td>A: 我也是</td>\n",
            "<td>A: 我好烦</td>\n",
            "<td>A: 我也是</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 好开心</td>\n",
            "<td>A: 开心就好</td>\n",
            "<td>A: 你是不是想我了</td>\n",
            "<td>A: 是呀</td>\n",
            "<td>A: 哈哈哈，我也是！</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 太激动了</td>\n",
            "<td>A: 激动什么</td>\n",
            "<td>A: 我也激动</td>\n",
            "<td>A: 哈哈，还是不要了</td>\n",
            "<td>A: 哈哈</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我好难过</td>\n",
            "<td>A: 怎么了？</td>\n",
            "<td>A: 我也是，我们都是</td>\n",
            "<td>A: 不要难过，我陪你</td>\n",
            "<td>A: 你怎么难过</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我想哭</td>\n",
            "<td>A: 抱抱</td>\n",
            "<td>A: 抱紧你</td>\n",
            "<td>A: 我不想哭！你不要想太多！</td>\n",
            "<td>A: 哭吧哭吧，我陪你哭</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 太好笑了</td>\n",
            "<td>A: 笑死我了</td>\n",
            "<td>A: 你也笑了？</td>\n",
            "<td>A: 我也觉得</td>\n",
            "<td>A: 哈哈</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我好伤心</td>\n",
            "<td>A: 怎么了</td>\n",
            "<td>A: 你伤心什么</td>\n",
            "<td>A: 我也是</td>\n",
            "<td>A: 你是不是想笑死我！！下次再给你拍</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 心好痛</td>\n",
            "<td>A: 抱抱</td>\n",
            "<td>A: 你心痛什么</td>\n",
            "<td>A: 心碎一地了。</td>\n",
            "<td>A: 怎么了</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 好累啊</td>\n",
            "<td>A: 加油</td>\n",
            "<td>A: 你不会还是一个人在寝室吧</td>\n",
            "<td>A: 好好休息</td>\n",
            "<td>A: 累了就睡觉</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我好疲惫</td>\n",
            "<td>A: 我也是</td>\n",
            "<td>A: 我也差不多。。</td>\n",
            "<td>A: 我好困</td>\n",
            "<td>A: 你不是一直都很疲惫么</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我爱你</td>\n",
            "<td>A: 我也爱你</td>\n",
            "<td>A: 我也是</td>\n",
            "<td>A: 我们都爱你</td>\n",
            "<td>A: 爱我就来见面</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我讨厌你</td>\n",
            "<td>A: 我也讨厌你</td>\n",
            "<td>A: 你不是一直很喜欢我</td>\n",
            "<td>A: 不是我的</td>\n",
            "<td>A: 我也讨厌你</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你真是太棒啦</td>\n",
            "<td>A: 谢谢</td>\n",
            "<td>A: 你最棒</td>\n",
            "<td>A: 你也棒棒哒</td>\n",
            "<td>A: 谢谢！</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 你好厉害啊</td>\n",
            "<td>A: 哈哈</td>\n",
            "<td>A: 是不是</td>\n",
            "<td>A: 你也不差呀</td>\n",
            "<td>A: 你也可以的</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 吓死我了</td>\n",
            "<td>A: 哈哈</td>\n",
            "<td>A: 我都不敢出门了</td>\n",
            "<td>A: 哈哈哈哈哈！！！</td>\n",
            "<td>A: 哈哈哈哈哈笑死我了</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我想回家</td>\n",
            "<td>A: 我也想回家</td>\n",
            "<td>A: 我也想！！</td>\n",
            "<td>A: 想家的时候回来，想家的时候离开。</td>\n",
            "<td>A: 回来吧，家真的好啊！</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 我想爸妈了</td>\n",
            "<td>A: 我也想爸妈</td>\n",
            "<td>A: 哈哈</td>\n",
            "<td>A: 我也想</td>\n",
            "<td>A: 想我吗</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 不知道小孩在家有没有听话</td>\n",
            "<td>A: 我也不知道</td>\n",
            "<td>A: 没有</td>\n",
            "<td>A: 听话的话肯定是会听话的。</td>\n",
            "<td>A: 我也是听不懂啊</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Q: 想回家撸猫</td>\n",
            "<td>A: 我也想回家</td>\n",
            "<td>A: 你也想啊？</td>\n",
            "<td>A: 我们这也有一个</td>\n",
            "<td>A: 回呀回呀</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "</li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "</article>\n",
            "<article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><p align=\"center\">\n",
            "<a href=\"/tensorflow/swift/blob/main/images/logo.png\" rel=\"noopener noreferrer\" target=\"_blank\"><img src=\"/tensorflow/swift/raw/main/images/logo.png\" style=\"max-width:100%;\"/></a>\n",
            "</p>\n",
            "<h1><a aria-hidden=\"true\" class=\"anchor\" href=\"#swift-for-tensorflow\" id=\"user-content-swift-for-tensorflow\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Swift for TensorFlow</h1>\n",
            "<blockquote>\n",
            "<p>Swift for TensorFlow: No boundaries.</p>\n",
            "</blockquote>\n",
            "<p>Swift for TensorFlow is a next-generation platform for machine learning,\n",
            "incorporating the latest research across machine learning, compilers,\n",
            "differentiable programming, systems design, and beyond. This is an early-stage\n",
            "project: it is not feature-complete nor production-ready, but it is ready for\n",
            "<em>pioneers</em> to try in projects, give feedback, and help shape the future!</p>\n",
            "<p>The Swift for TensorFlow project is currently focusing on 2 kinds of users:</p>\n",
            "<ol>\n",
            "<li>\n",
            "<p><strong>Advanced ML researchers</strong> who are limited by current ML frameworks. Swift\n",
            "for TensorFlow's advantages include seamless integration with a modern\n",
            "general-purpose language, allowing for more dynamic and sophisticated\n",
            "models. Fast abstractions can be developed in \"user-space\" (as opposed to in\n",
            "C/C++, aka \"framework-space\"), resulting in modular APIs that can be easily\n",
            "customized.</p>\n",
            "</li>\n",
            "<li>\n",
            "<p><strong>ML learners</strong> who are just getting started with machine learning. Thanks\n",
            "to Swift's support for quality tooling (e.g. context-aware autocompletion),\n",
            "Swift for TensorFlow can be one of the most productive ways to start\n",
            "learning the fundamentals of machine learning.</p>\n",
            "</li>\n",
            "</ol>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#getting-started\" id=\"user-content-getting-started\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Getting started</h2>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#using-swift-for-tensorflow\" id=\"user-content-using-swift-for-tensorflow\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Using Swift for TensorFlow</h3>\n",
            "<ul>\n",
            "<li>\n",
            "<p><strong>Google Colaboratory</strong>: The fastest way to get started is to try out Swift\n",
            "for TensorFlow right in your browser. Just open up <a href=\"#tutorials-\">a tutorial</a>,\n",
            "or start from a <a href=\"https://colab.research.google.com/notebook#create=true&amp;language=swift\" rel=\"nofollow\">blank notebook</a>!\n",
            "Read more in our <a href=\"/tensorflow/swift/blob/main/Usage.md\">usage guide</a>.</p>\n",
            "</li>\n",
            "<li>\n",
            "<p><strong>Install locally</strong>: You can <a href=\"/tensorflow/swift/blob/main/Installation.md\">download a pre-built Swift for TensorFlow\n",
            "package</a>. After installation, you can follow these\n",
            "<a href=\"/tensorflow/swift/blob/main/Usage.md\">step-by-step instructions</a> to build and execute a Swift script on\n",
            "your computer.</p>\n",
            "</li>\n",
            "<li>\n",
            "<p><strong>Run on GCP</strong>: You can spin up a GCE instance using a Swift for TensorFlow\n",
            "<a href=\"https://cloud.google.com/ai-platform/deep-learning-vm/docs\" rel=\"nofollow\">Deep Learning VM</a> image, with all drivers and the toolchain\n",
            "pre-installed. Instructions can be found in the\n",
            "<a href=\"/tensorflow/swift/blob/main/Installation.md\">Installation Guide</a>.</p>\n",
            "</li>\n",
            "<li>\n",
            "<p><strong>Compile from source</strong>: If you'd like to customize Swift for TensorFlow or\n",
            "contribute back, follow our <a href=\"https://github.com/apple/swift/tree/tensorflow#building-swift-for-tensorflow\">instructions</a>\n",
            "on building Swift for TensorFlow from source.</p>\n",
            "</li>\n",
            "</ul>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#tutorials-\" id=\"user-content-tutorials-\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Tutorials <a href=\"https://camo.githubusercontent.com/756e8e5187b778c7b7440cce63e1ca5069313fea0abddc151a92f5b5f536f471/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f636f6c61625f6c6f676f5f333270782e706e67\" rel=\"noopener noreferrer\" target=\"_blank\"><img alt=\"\" data-canonical-src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" src=\"https://camo.githubusercontent.com/756e8e5187b778c7b7440cce63e1ca5069313fea0abddc151a92f5b5f536f471/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f636f6c61625f6c6f676f5f333270782e706e67\" style=\"max-width:100%;\"/></a></h3>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Tutorial</th>\n",
            "<th>Last Updated</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/a_swift_tour.ipynb\" rel=\"nofollow\">A Swift Tour</a></td>\n",
            "<td>March 2019</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/protocol_oriented_generics.ipynb\" rel=\"nofollow\">Protocol-Oriented Programming &amp; Generics</a></td>\n",
            "<td>August 2019</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/python_interoperability.ipynb\" rel=\"nofollow\">Python Interoperability</a></td>\n",
            "<td>March 2019</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/custom_differentiation.ipynb\" rel=\"nofollow\">Custom Differentiation</a></td>\n",
            "<td>March 2019</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/Swift_autodiff_sharp_edges.ipynb\" rel=\"nofollow\">Sharp Edges in Differentiability</a></td>\n",
            "<td>November 2020</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/model_training_walkthrough.ipynb\" rel=\"nofollow\">Model Training Walkthrough</a></td>\n",
            "<td>March 2019</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/raw_tensorflow_operators.ipynb\" rel=\"nofollow\">Raw TensorFlow Operators</a></td>\n",
            "<td>December 2019</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/introducing_x10.ipynb\" rel=\"nofollow\">Introducing X10, an XLA-Based Backend</a></td>\n",
            "<td>May 2020</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#resources\" id=\"user-content-resources\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Resources</h3>\n",
            "<ul>\n",
            "<li><a href=\"https://github.com/tensorflow/swift-models\">Models and Examples</a></li>\n",
            "<li><a href=\"https://www.tensorflow.org/swift/api_docs/Structs/Tensor\" rel=\"nofollow\">TensorFlow Swift API Reference</a></li>\n",
            "<li><a href=\"/tensorflow/swift/blob/main/RELEASES.md\">Release Notes</a></li>\n",
            "<li><a href=\"/tensorflow/swift/blob/main/KNOWN_ISSUES.md\">Known Issues</a></li>\n",
            "<li><a href=\"/tensorflow/swift/blob/main/FAQ.md\">Frequently Asked Questions</a></li>\n",
            "<li><a href=\"https://blog.tensorflow.org/search?label=Swift\" rel=\"nofollow\">TensorFlow Blog Posts</a></li>\n",
            "</ul>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#forums\" id=\"user-content-forums\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Forums</h3>\n",
            "<p>Please join the\n",
            "<a href=\"https://groups.google.com/a/tensorflow.org/d/forum/swift\" rel=\"nofollow\">swift@tensorflow.org mailing list</a>\n",
            "to hear the latest announcements, get help, and share your thoughts!</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#why-swift-for-tensorflow\" id=\"user-content-why-swift-for-tensorflow\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Why Swift for TensorFlow?</h2>\n",
            "<p>Swift for TensorFlow is a new way to develop machine learning models. It\n",
            "gives you the power of\n",
            "<a href=\"https://www.tensorflow.org\" rel=\"nofollow\">TensorFlow</a> directly integrated into the\n",
            "<a href=\"https://swift.org/about\" rel=\"nofollow\">Swift programming language</a>. We believe that\n",
            "machine learning paradigms are so important that they deserve\n",
            "<strong>first-class language and compiler support</strong>.</p>\n",
            "<p>A fundamental primitive in machine learning is gradient-based optimization:\n",
            "computing function derivatives to optimize parameters. With Swift for\n",
            "TensorFlow, you can easily differentiate functions using differential\n",
            "operators like <a href=\"https://www.tensorflow.org/swift/api_docs/Functions#/s:10TensorFlow8gradient2of15CotangentVectorQzxcq_xc_tAA14DifferentiableRzSFR_AaFR_AdaFPQy_Rs_r0_lF\" rel=\"nofollow\"><code>gradient(of:)</code></a>, or differentiate with respect to an entire\n",
            "model by calling method <a href=\"https://www.tensorflow.org/swift/api_docs/Protocols/Differentiable#/s:10TensorFlow14DifferentiablePAAE8gradient2in15CotangentVectorQzqd__xXE_tSFRd__AaBRd__AfCQyd__Rsd__lF\" rel=\"nofollow\"><code>gradient(in:)</code></a>. These differentiation APIs\n",
            "are not just available for <code>Tensor</code>-related concepts—they are\n",
            "generalized for all types that conform to the <a href=\"https://www.tensorflow.org/swift/api_docs/Protocols/Differentiable\" rel=\"nofollow\"><code>Differentiable</code></a>\n",
            "protocol, including <code>Float</code>, <code>Double</code>, SIMD vectors, and your own data\n",
            "structures.</p>\n",
            "<div class=\"highlight highlight-source-swift\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> Custom differentiable type.</span>\n",
            "<span class=\"pl-c\"></span><span class=\"pl-k\">struct</span> <span class=\"pl-en\">Model</span>: <span class=\"pl-e\">Differentiable </span>{\n",
            "    <span class=\"pl-k\">var</span> w<span class=\"pl-k\">:</span> <span class=\"pl-c1\">Float</span>\n",
            "    <span class=\"pl-k\">var</span> b<span class=\"pl-k\">:</span> <span class=\"pl-c1\">Float</span>\n",
            "    <span class=\"pl-k\">func</span> <span class=\"pl-en\">applied</span>(<span class=\"pl-en\">to</span> <span class=\"pl-smi\">input</span>: <span class=\"pl-c1\">Float</span>) <span class=\"pl-k\">-&gt;</span> <span class=\"pl-c1\">Float</span> {\n",
            "        <span class=\"pl-k\">return</span> w <span class=\"pl-k\">*</span> input <span class=\"pl-k\">+</span> b\n",
            "    }\n",
            "}\n",
            "\n",
            "<span class=\"pl-c\"><span class=\"pl-c\">//</span> Differentiate using `gradient(at:_:in:)`.</span>\n",
            "<span class=\"pl-c\"></span><span class=\"pl-k\">let</span> model <span class=\"pl-k\">=</span> <span class=\"pl-c1\">Model</span>(<span class=\"pl-c1\">w</span>: <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">b</span>: <span class=\"pl-c1\">3</span>)\n",
            "<span class=\"pl-k\">let</span> input<span class=\"pl-k\">:</span> <span class=\"pl-c1\">Float</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n",
            "<span class=\"pl-k\">let</span> (𝛁model, 𝛁input) <span class=\"pl-k\">=</span> <span class=\"pl-c1\">gradient</span>(<span class=\"pl-c1\">at</span>: model, input) { model, input <span class=\"pl-k\">in</span>\n",
            "    model.<span class=\"pl-c1\">applied</span>(<span class=\"pl-c1\">to</span>: input)\n",
            "}\n",
            "\n",
            "<span class=\"pl-c1\">print</span>(𝛁model) <span class=\"pl-c\"><span class=\"pl-c\">//</span> Model.TangentVector(w: 2.0, b: 1.0)</span>\n",
            "<span class=\"pl-c\"></span><span class=\"pl-c1\">print</span>(𝛁input) <span class=\"pl-c\"><span class=\"pl-c\">//</span> 4.0</span></pre></div>\n",
            "<p>Beyond derivatives, the Swift for TensorFlow project comes with a sophisticated toolchain\n",
            "to make users more productive. You can run Swift interactively in a Jupyter\n",
            "notebook, and get helpful autocomplete suggestions to help you explore the\n",
            "massive API surface of a modern deep learning library. You can <a href=\"https://colab.research.google.com/github/tensorflow/swift/blob/main/docs/site/tutorials/model_training_walkthrough.ipynb\" rel=\"nofollow\">get started\n",
            "right in your browser in\n",
            "seconds</a>!</p>\n",
            "<p>Migrating to Swift for TensorFlow is really easy thanks to Swift's powerful\n",
            "Python integration. You can incrementally migrate your Python code over (or\n",
            "continue to use your favorite Python libraries), because you can easily call\n",
            "your favorite Python library with a familiar syntax:</p>\n",
            "<div class=\"highlight highlight-source-swift\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-en\">TensorFlow</span>\n",
            "<span class=\"pl-k\">import</span> <span class=\"pl-en\">Python</span>\n",
            "\n",
            "<span class=\"pl-k\">let</span> np <span class=\"pl-k\">=</span> Python.<span class=\"pl-c1\">import</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>numpy<span class=\"pl-pds\">\"</span></span>)\n",
            "\n",
            "<span class=\"pl-k\">let</span> array <span class=\"pl-k\">=</span> np.<span class=\"pl-c1\">arange</span>(<span class=\"pl-c1\">100</span>).<span class=\"pl-c1\">reshape</span>(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10</span>)  <span class=\"pl-c\"><span class=\"pl-c\">//</span> Create a 10x10 numpy array.</span>\n",
            "<span class=\"pl-c\"></span><span class=\"pl-k\">let</span> tensor <span class=\"pl-k\">=</span> Tensor<span class=\"pl-k\">&lt;</span><span class=\"pl-c1\">Float</span><span class=\"pl-k\">&gt;</span>(<span class=\"pl-c1\">numpy</span>: array)  <span class=\"pl-c\"><span class=\"pl-c\">//</span> Seamless integration!</span></pre></div>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#documentation\" id=\"user-content-documentation\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Documentation</h2>\n",
            "<blockquote>\n",
            "<p>Beware: the project is moving very quickly, and thus some of these documents\n",
            "are slightly out of date as compared to the current state-of-the-art.</p>\n",
            "</blockquote>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#overview\" id=\"user-content-overview\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Overview</h3>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Document</th>\n",
            "<th>Last Updated</th>\n",
            "<th>Status</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"/tensorflow/swift/blob/main/docs/WhySwiftForTensorFlow.md\">Why <em>Swift</em> for TensorFlow?</a></td>\n",
            "<td>April 2018</td>\n",
            "<td>Current</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"/tensorflow/swift/blob/main/docs/DesignOverview.md\">Swift for TensorFlow Design Overview</a></td>\n",
            "<td>April 2018</td>\n",
            "<td>Outdated</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"/tensorflow/swift/blob/main/docs/SupportedBackends.md\">Supported Backends</a></td>\n",
            "<td>May 2020</td>\n",
            "<td>Current</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#technology-deep-dive\" id=\"user-content-technology-deep-dive\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Technology deep dive</h3>\n",
            "<p>The Swift for TensorFlow project builds on top of powerful theoretical\n",
            "foundations. For insight into some of the underlying technologies, check\n",
            "out the following documentation.</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Document</th>\n",
            "<th>Last Updated</th>\n",
            "<th>Status</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><a href=\"https://github.com/apple/swift/blob/main/docs/DifferentiableProgramming.md\">Swift Differentiable Programming Manifesto</a></td>\n",
            "<td>January 2020</td>\n",
            "<td>Current</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://docs.google.com/document/d/1_BirmTqdotglwNTOcYAW-ib6mx_jl-gH9Dbg4WmHZh0\" rel=\"nofollow\">Swift Differentiable Programming Implementation Overview</a></td>\n",
            "<td>August 2019</td>\n",
            "<td>Current</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://docs.google.com/document/d/1bPepWLfRQa6CtXqKA8CDQ87uZHixNav-TFjLSisuKag/edit?usp=sharing\" rel=\"nofollow\">Swift Differentiable Programming Design Overview</a></td>\n",
            "<td>June 2019</td>\n",
            "<td>Outdated</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"/tensorflow/swift/blob/main/docs/DifferentiableTypes.md\">Differentiable Types</a></td>\n",
            "<td>March 2019</td>\n",
            "<td>Outdated</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"/tensorflow/swift/blob/main/docs/DifferentiableFunctions.md\">Differentiable Functions and Differentiation APIs</a></td>\n",
            "<td>March 2019</td>\n",
            "<td>Outdated</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"/tensorflow/swift/blob/main/docs/DynamicPropertyIteration.md\">Dynamic Property Iteration using Key Paths</a></td>\n",
            "<td>March 2019</td>\n",
            "<td>Current</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"/tensorflow/swift/blob/main/docs/ParameterOptimization.md\">Hierarchical Parameter Iteration and Optimization</a></td>\n",
            "<td>March 2019</td>\n",
            "<td>Current</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"https://gist.github.com/rxwei/30ba75ce092ab3b0dce4bde1fc2c9f1d\">First-Class Automatic Differentiation in Swift: A Manifesto</a></td>\n",
            "<td>October 2018</td>\n",
            "<td>Outdated</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"/tensorflow/swift/blob/main/docs/AutomaticDifferentiation.md\">Automatic Differentiation Whitepaper</a></td>\n",
            "<td>April 2018</td>\n",
            "<td>Outdated</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"/tensorflow/swift/blob/main/docs/PythonInteroperability.md\">Python Interoperability</a></td>\n",
            "<td>April 2018</td>\n",
            "<td>Current</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><a href=\"/tensorflow/swift/blob/main/docs/GraphProgramExtraction.md\">Graph Program Extraction</a></td>\n",
            "<td>April 2018</td>\n",
            "<td>Outdated</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#source-code\" id=\"user-content-source-code\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Source code</h2>\n",
            "<p>Compiler and standard library development happens on the <code>main</code> branch of the\n",
            "<a href=\"https://github.com/apple/swift/tree/main\">apple/swift</a> repository.</p>\n",
            "<p>Additional code repositories that make up the core of the project include:</p>\n",
            "<ul>\n",
            "<li><a href=\"https://github.com/tensorflow/swift-apis\">Deep learning library</a>: high-level\n",
            "API familiar to Keras users.</li>\n",
            "</ul>\n",
            "<blockquote>\n",
            "<p>Swift for TensorFlow is <strong>no longer</strong> a fork of the official Swift language;\n",
            "development was previously done on the <code>tensorflow</code> branch of the\n",
            "<a href=\"https://github.com/apple/swift/tree/tensorflow\">apple/swift</a> repository.\n",
            "Language additions were designed to fit with the direction of Swift and are\n",
            "going through the <a href=\"https://github.com/apple/swift-evolution\">Swift Evolution</a>\n",
            "process.</p>\n",
            "</blockquote>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#jupyter-notebook-support\" id=\"user-content-jupyter-notebook-support\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Jupyter Notebook support</h3>\n",
            "<p><a href=\"http://jupyter.org/\" rel=\"nofollow\">Jupyter Notebook</a> support for Swift is under development at\n",
            "<a href=\"https://github.com/google/swift-jupyter\">google/swift-jupyter</a>.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#model-garden\" id=\"user-content-model-garden\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Model garden</h3>\n",
            "<p><a href=\"https://github.com/tensorflow/swift-models\">tensorflow/swift-models</a> is a\n",
            "repository of machine learning models built with Swift for TensorFlow. It\n",
            "intended to provide examples of how to use Swift for TensorFlow, to allow for\n",
            "end-to-end tests of machine learning APIs, and to host model benchmarking\n",
            "infrastructure.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#swiftai\" id=\"user-content-swiftai\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>SwiftAI</h3>\n",
            "<p><a href=\"https://github.com/fastai/swiftai\">fastai/swiftai</a> is a high-level API for\n",
            "Swift for TensorFlow, modeled after the\n",
            "<a href=\"https://github.com/fastai/fastai\">fastai Python library</a>.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#community\" id=\"user-content-community\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Community</h2>\n",
            "<p>Swift for TensorFlow discussions happen on the\n",
            "<a href=\"https://groups.google.com/a/tensorflow.org/d/forum/swift\" rel=\"nofollow\">swift@tensorflow.org mailing list</a>.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#bugs-reports-and-feature-requests\" id=\"user-content-bugs-reports-and-feature-requests\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Bugs reports and feature requests</h3>\n",
            "<p>Before reporting an issue, please check the <a href=\"/tensorflow/swift/blob/main/FAQ.md\">Frequently Asked Questions</a>\n",
            "to see if your question has already been addressed.</p>\n",
            "<p>For questions about general use or feature requests, please send an email to\n",
            "the <a href=\"mailto:swift@tensorflow.org\">mailing list</a> or search for relevant issues\n",
            "in the <a href=\"https://bugs.swift.org/projects/TF/issues/?filter=allopenissues\" rel=\"nofollow\">JIRA issue tracker</a>.</p>\n",
            "<p>For the most part, the core team's development is also tracked in\n",
            "<a href=\"https://bugs.swift.org/secure/RapidBoard.jspa?rapidView=17&amp;projectKey=TF&amp;view=planning\" rel=\"nofollow\">JIRA</a>.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#contributing\" id=\"user-content-contributing\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Contributing</h3>\n",
            "<p>We welcome contributions from everyone. Read the <a href=\"/tensorflow/swift/blob/main/Contributing.md\">contributing\n",
            "guide</a> for information on how to get started.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#code-of-conduct\" id=\"user-content-code-of-conduct\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Code of conduct</h3>\n",
            "<p>In the interest of fostering an open and welcoming environment, we as\n",
            "contributors and maintainers pledge to making participation in our project and\n",
            "our community a harassment-free experience for everyone, regardless of age, body\n",
            "size, disability, ethnicity, gender identity and expression, level of\n",
            "experience, education, socio-economic status, nationality, personal appearance,\n",
            "race, religion, or sexual identity and orientation.</p>\n",
            "<p>The Swift for TensorFlow community is guided by our <a href=\"/tensorflow/swift/blob/main/CODE_OF_CONDUCT.md\">Code of\n",
            "Conduct</a>, which we encourage everybody to read before\n",
            "participating.</p>\n",
            "</article>\n",
            "<article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#fast-style-transfer-in-tensorflow\" id=\"user-content-fast-style-transfer-in-tensorflow\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Fast Style Transfer in <a href=\"https://github.com/tensorflow/tensorflow\">TensorFlow</a></h2>\n",
            "<p>Add styles from famous paintings to any photo in a fraction of a second! <a href=\"#video-stylization\">You can even style videos!</a></p>\n",
            "<p align=\"center\">\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/style/udnie.jpg\" rel=\"noopener noreferrer\" target=\"_blank\"><img height=\"246px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/style/udnie.jpg\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/content/stata.jpg\" rel=\"noopener noreferrer\" target=\"_blank\"><img height=\"246px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/content/stata.jpg\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/results/stata_udnie.jpg\"><img src=\"/lengstrom/fast-style-transfer/raw/master/examples/results/stata_udnie_header.jpg\" style=\"max-width:100%;\" width=\"627px\"/></a>\n",
            "</p>\n",
            "<p align=\"center\">\n",
            "It takes 100ms on a 2015 Titan X to style the MIT Stata Center (1024×680) like Udnie, by Francis Picabia.\n",
            "</p>\n",
            "<p>Our implementation is based off of a combination of Gatys' <a href=\"https://arxiv.org/abs/1508.06576\" rel=\"nofollow\">A Neural Algorithm of Artistic Style</a>, Johnson's <a href=\"http://cs.stanford.edu/people/jcjohns/eccv16/\" rel=\"nofollow\">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a>, and Ulyanov's <a href=\"https://arxiv.org/abs/1607.08022\" rel=\"nofollow\">Instance Normalization</a>.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#sponsorship\" id=\"user-content-sponsorship\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Sponsorship</h3>\n",
            "<p>Please consider sponsoring my work on this project!</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#license\" id=\"user-content-license\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>License</h3>\n",
            "<p>Copyright (c) 2016 Logan Engstrom. Contact me for commercial use (or rather any use that is not academic research) (email: engstrom at my university's domain dot edu). Free for research use, as long as proper attribution is given and this copyright notice is retained.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#video-stylization\" id=\"user-content-video-stylization\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Video Stylization</h2>\n",
            "<p>Here we transformed every frame in a video, then combined the results. <a href=\"https://www.youtube.com/watch?v=xVJwwWQlQ1o\" rel=\"nofollow\">Click to go to the full demo on YouTube!</a> The style here is Udnie, as above.</p>\n",
            "<div align=\"center\">\n",
            "<a href=\"https://www.youtube.com/watch?v=xVJwwWQlQ1o\" rel=\"nofollow\">\n",
            "<img alt=\"Stylized fox video. Click to go to YouTube!\" height=\"400px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/results/fox_udnie.gif\" style=\"max-width:100%;\" width=\"800px\"/>\n",
            "</a>\n",
            "</div>\n",
            "<p>See how to generate these videos <a href=\"#stylizing-video\">here</a>!</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#image-stylization\" id=\"user-content-image-stylization\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Image Stylization</h2>\n",
            "<p>We added styles from various paintings to a photo of Chicago. Click on thumbnails to see full applied style images.</p>\n",
            "<div align=\"center\">\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/content/chicago.jpg\" rel=\"noopener noreferrer\" target=\"_blank\"><img height=\"200px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/content/chicago.jpg\" style=\"max-width:100%;\"/></a>\n",
            "</div>\n",
            "<div align=\"center\">\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/style/wave.jpg\"><img height=\"200px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/thumbs/wave.jpg\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/results/chicago_wave.jpg\" rel=\"noopener noreferrer\" target=\"_blank\"><img height=\"200px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/results/chicago_wave.jpg\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/results/chicago_udnie.jpg\" rel=\"noopener noreferrer\" target=\"_blank\"><img height=\"200px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/results/chicago_udnie.jpg\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/style/udnie.jpg\"><img height=\"200px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/thumbs/udnie.jpg\" style=\"max-width:100%;\"/></a>\n",
            "<br/>\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/style/rain_princess.jpg\"><img height=\"200px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/thumbs/rain_princess.jpg\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/results/chicago_rain_princess.jpg\" rel=\"noopener noreferrer\" target=\"_blank\"><img height=\"200px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/results/chicago_rain_princess.jpg\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/results/chicago_la_muse.jpg\" rel=\"noopener noreferrer\" target=\"_blank\"><img height=\"200px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/results/chicago_la_muse.jpg\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/style/la_muse.jpg\"><img height=\"200px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/thumbs/la_muse.jpg\" style=\"max-width:100%;\"/></a>\n",
            "<br/>\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/style/the_shipwreck_of_the_minotaur.jpg\"><img height=\"200px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/thumbs/the_shipwreck_of_the_minotaur.jpg\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/results/chicago_wreck.jpg\" rel=\"noopener noreferrer\" target=\"_blank\"><img height=\"200px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/results/chicago_wreck.jpg\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/results/chicago_the_scream.jpg\" rel=\"noopener noreferrer\" target=\"_blank\"><img height=\"200px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/results/chicago_the_scream.jpg\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"/lengstrom/fast-style-transfer/blob/master/examples/style/the_scream.jpg\"><img height=\"200px\" src=\"/lengstrom/fast-style-transfer/raw/master/examples/thumbs/the_scream.jpg\" style=\"max-width:100%;\"/></a>\n",
            "</div>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#implementation-details\" id=\"user-content-implementation-details\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Implementation Details</h2>\n",
            "<p>Our implementation uses TensorFlow to train a fast style transfer network. We use roughly the same transformation network as described in Johnson, except that batch normalization is replaced with Ulyanov's instance normalization, and the scaling/offset of the output <code>tanh</code> layer is slightly different. We use a loss function close to the one described in Gatys, using VGG19 instead of VGG16 and typically using \"shallower\" layers than in Johnson's implementation (e.g. we use <code>relu1_1</code> rather than <code>relu1_2</code>). Empirically, this results in larger scale style features in transformations.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#vitual-environment-setup-anaconda---windowslinux\" id=\"user-content-vitual-environment-setup-anaconda---windowslinux\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Vitual Environment Setup (Anaconda) - Windows/Linux</h2>\n",
            "<p>Tested on</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Spec</th>\n",
            "<th></th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td>Operating System</td>\n",
            "<td>Windows 10 Home</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>GPU</td>\n",
            "<td>Nvidia GTX 2080 TI</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>CUDA Version</td>\n",
            "<td>11.0</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>Driver Version</td>\n",
            "<td>445.75</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#step-1install-anaconda\" id=\"user-content-step-1install-anaconda\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Step 1：Install Anaconda</h3>\n",
            "<p><a href=\"https://docs.anaconda.com/anaconda/install/\" rel=\"nofollow\">https://docs.anaconda.com/anaconda/install/</a></p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#step-2build-a-virtual-environment\" id=\"user-content-step-2build-a-virtual-environment\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Step 2：Build a virtual environment</h3>\n",
            "<p>Run the following commands in sequence in Anaconda Prompt:</p>\n",
            "<pre><code>conda create -n tf-gpu tensorflow-gpu=2.1.0\n",
            "conda activate tf-gpu\n",
            "conda install jupyterlab\n",
            "jupyter lab\n",
            "</code></pre>\n",
            "<p>Run the following command in the notebook or just conda install the package:</p>\n",
            "<pre><code>!pip install moviepy==1.0.2\n",
            "</code></pre>\n",
            "<p>Follow the commands below to use fast-style-transfer</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#documentation\" id=\"user-content-documentation\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Documentation</h2>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#training-style-transfer-networks\" id=\"user-content-training-style-transfer-networks\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Training Style Transfer Networks</h3>\n",
            "<p>Use <code>style.py</code> to train a new style transfer network. Run <code>python style.py</code> to view all the possible parameters. Training takes 4-6 hours on a Maxwell Titan X. <a href=\"/lengstrom/fast-style-transfer/blob/master/docs.md#stylepy\">More detailed documentation here</a>. <strong>Before you run this, you should run <code>setup.sh</code></strong>. Example usage:</p>\n",
            "<pre><code>python style.py --style path/to/style/img.jpg \\\n",
            "  --checkpoint-dir checkpoint/path \\\n",
            "  --test path/to/test/img.jpg \\\n",
            "  --test-dir path/to/test/dir \\\n",
            "  --content-weight 1.5e1 \\\n",
            "  --checkpoint-iterations 1000 \\\n",
            "  --batch-size 20\n",
            "</code></pre>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#evaluating-style-transfer-networks\" id=\"user-content-evaluating-style-transfer-networks\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Evaluating Style Transfer Networks</h3>\n",
            "<p>Use <code>evaluate.py</code> to evaluate a style transfer network. Run <code>python evaluate.py</code> to view all the possible parameters. Evaluation takes 100 ms per frame (when batch size is 1) on a Maxwell Titan X. <a href=\"/lengstrom/fast-style-transfer/blob/master/docs.md#evaluatepy\">More detailed documentation here</a>. Takes several seconds per frame on a CPU. <strong>Models for evaluation are <a href=\"https://drive.google.com/drive/folders/0B9jhaT37ydSyRk9UX0wwX3BpMzQ?usp=sharing\" rel=\"nofollow\">located here</a></strong>. Example usage:</p>\n",
            "<pre><code>python evaluate.py --checkpoint path/to/style/model.ckpt \\\n",
            "  --in-path dir/of/test/imgs/ \\\n",
            "  --out-path dir/for/results/\n",
            "</code></pre>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#stylizing-video\" id=\"user-content-stylizing-video\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Stylizing Video</h3>\n",
            "<p>Use <code>transform_video.py</code> to transfer style into a video. Run <code>python transform_video.py</code> to view all the possible parameters. Requires <code>ffmpeg</code>. <a href=\"/lengstrom/fast-style-transfer/blob/master/docs.md#transform_videopy\">More detailed documentation here</a>. Example usage:</p>\n",
            "<pre><code>python transform_video.py --in-path path/to/input/vid.mp4 \\\n",
            "  --checkpoint path/to/style/model.ckpt \\\n",
            "  --out-path out/video.mp4 \\\n",
            "  --device /gpu:0 \\\n",
            "  --batch-size 4\n",
            "</code></pre>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#requirements\" id=\"user-content-requirements\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Requirements</h3>\n",
            "<p>You will need the following to run the above:</p>\n",
            "<ul>\n",
            "<li>TensorFlow 0.11.0</li>\n",
            "<li>Python 2.7.9, Pillow 3.4.2, scipy 0.18.1, numpy 1.11.2</li>\n",
            "<li>If you want to train (and don't want to wait for 4 months):\n",
            "<ul>\n",
            "<li>A decent GPU</li>\n",
            "<li>All the required NVIDIA software to run TF on a GPU (cuda, etc)</li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>ffmpeg 3.1.3 if you want to stylize video</li>\n",
            "</ul>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#citation\" id=\"user-content-citation\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Citation</h3>\n",
            "<pre><code>  @misc{engstrom2016faststyletransfer,\n",
            "    author = {Logan Engstrom},\n",
            "    title = {Fast Style Transfer},\n",
            "    year = {2016},\n",
            "    howpublished = {\\url{https://github.com/lengstrom/fast-style-transfer/}},\n",
            "    note = {commit xxxxxxx}\n",
            "  }\n",
            "</code></pre>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#attributionsthanks\" id=\"user-content-attributionsthanks\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Attributions/Thanks</h3>\n",
            "<ul>\n",
            "<li>This project could not have happened without the advice (and GPU access) given by <a href=\"http://www.anishathalye.com/\" rel=\"nofollow\">Anish Athalye</a>.\n",
            "<ul>\n",
            "<li>The project also borrowed some code from Anish's <a href=\"https://github.com/anishathalye/neural-style/\">Neural Style</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li>Some readme/docs formatting was borrowed from Justin Johnson's <a href=\"https://github.com/jcjohnson/fast-neural-style\">Fast Neural Style</a></li>\n",
            "<li>The image of the Stata Center at the very beginning of the README was taken by <a href=\"https://juanpaulo.me/\" rel=\"nofollow\">Juan Paulo</a></li>\n",
            "</ul>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#related-work\" id=\"user-content-related-work\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Related Work</h3>\n",
            "<ul>\n",
            "<li>Michael Ramos ported this network <a href=\"https://medium.com/@rambossa/diy-prisma-fast-style-transfer-app-with-coreml-and-tensorflow-817c3b90dacd\" rel=\"nofollow\">to use CoreML on iOS</a></li>\n",
            "</ul>\n",
            "</article>\n",
            "<article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#this-repository-was-moved-to-githubcomgooglecloudplatformtensorflow-without-a-phdtensorflow-mnist-tutorial\" id=\"user-content-this-repository-was-moved-to-githubcomgooglecloudplatformtensorflow-without-a-phdtensorflow-mnist-tutorial\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>This repository was moved to <a href=\"https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd/tree/master/tensorflow-mnist-tutorial\">github.com/GoogleCloudPlatform/tensorflow-without-a-phd/tensorflow-mnist-tutorial</a></h2>\n",
            "</article>\n",
            "<article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#update\" id=\"user-content-update\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Update</h3>\n",
            "<p>An example on how to integrate this code into your own semantic segmentation pipeline can be found in my <a href=\"https://github.com/MarvinTeichmann/KittiSeg\">KittiSeg</a> project repository.</p>\n",
            "<h1><a aria-hidden=\"true\" class=\"anchor\" href=\"#tensorflow-fcn\" id=\"user-content-tensorflow-fcn\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>tensorflow-fcn</h1>\n",
            "<p>This is a one file Tensorflow implementation of <a href=\"http://arxiv.org/abs/1411.4038\" rel=\"nofollow\">Fully Convolutional Networks</a> in Tensorflow. The code can easily be integrated in your semantic segmentation pipeline. The network can be applied directly or finetuned to perform semantic segmentation using tensorflow training code.</p>\n",
            "<p>Deconvolution Layers are initialized as bilinear upsampling. Conv and FCN layer weights using VGG weights. Numpy load is used to read VGG weights. No Caffe or Caffe-Tensorflow is required to run this. <strong>The .npy file for [VGG16] to be downloaded before using this needwork</strong>. You can find the file here: ftp://mi.eng.cam.ac.uk/pub/mttt2/models/vgg16.npy</p>\n",
            "<p>No Pascal VOC finetuning was applied to the weights. The model is meant to be finetuned on your own data. The model can be applied to an image directly (see <code>test_fcn32_vgg.py</code>) but the result will be rather coarse.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#requirements\" id=\"user-content-requirements\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Requirements</h2>\n",
            "<p>In addition to tensorflow the following packages are required:</p>\n",
            "<p>numpy\n",
            "scipy\n",
            "pillow\n",
            "matplotlib</p>\n",
            "<p>Those packages can be installed by running <code>pip install -r requirements.txt</code> or <code>pip install numpy scipy pillow matplotlib</code>.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#tensorflow-10rc\" id=\"user-content-tensorflow-10rc\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Tensorflow 1.0rc</h3>\n",
            "<p>This code requires <code>Tensorflow Version &gt;= 1.0rc</code> to run. If you want to use older Version you can try using commit <code>bf9400c6303826e1c25bf09a3b032e51cef57e3b</code>. This Commit has been tested using the pip version of <code>0.12</code>, <code>0.11</code> and <code>0.10</code>.</p>\n",
            "<p>Tensorflow 1.0 comes with a large number of breaking api changes. If you are currently running an older tensorflow version, I would suggest creating a new <code>virtualenv</code> and install 1.0rc using:</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0rc0-cp27-none-linux_x86_64.whl\n",
            "pip install --upgrade <span class=\"pl-smi\">$TF_BINARY_URL</span></pre></div>\n",
            "<p>Above commands will install the linux version with gpu support. For other versions follow the instructions <a href=\"https://www.tensorflow.org/versions/r1.0/get_started/os_setup\" rel=\"nofollow\">here</a>.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#usage\" id=\"user-content-usage\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Usage</h2>\n",
            "<p><code>python test_fcn32_vgg.py</code> to test the implementation.</p>\n",
            "<p>Use this to build the VGG object for finetuning:</p>\n",
            "<pre><code>vgg = vgg16.Vgg16()\n",
            "vgg.build(images, train=True, num_classes=num_classes, random_init_fc8=True)\n",
            "</code></pre>\n",
            "<p>The <code>images</code> is a tensor with shape <code>[None, h, w, 3]</code>. Where <code>h</code> and <code>w</code> can have arbitrary size.</p>\n",
            "<blockquote>\n",
            "<p>Trick: the tensor can be a placeholder, a variable or even a constant.</p>\n",
            "</blockquote>\n",
            "<p>Be aware, that <code>num_classes</code> influences the way <code>score_fr</code> (the original <code>fc8</code> layer) is initialized. For finetuning I recommend using the option <code>random_init_fc8=True</code>.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#training\" id=\"user-content-training\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Training</h3>\n",
            "<p>Example code for training can be found in the <a href=\"https://github.com/MarvinTeichmann/KittiSeg\">KittiSeg</a> project repository.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#finetuning-and-training\" id=\"user-content-finetuning-and-training\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Finetuning and training</h3>\n",
            "<p>For training build the graph using <code>vgg.build(images, train=True, num_classes=num_classes)</code> were images is q queue yielding image batches. Use a softmax_cross_entropy loss function on top of the output of vgg.up. An Implementation of the loss function can be found in <code>loss.py</code>.</p>\n",
            "<p>To train the graph you need an input producer and a training script. Have a look at <a href=\"https://github.com/TensorVision/TensorVision/blob/9db59e2f23755a17ddbae558f21ae371a07f1a83/tensorvision/train.py\">TensorVision</a> to see how to build those.</p>\n",
            "<p>I had success finetuning the network using Adam Optimizer with a learning rate of <code>1e-6</code>.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#content\" id=\"user-content-content\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Content</h2>\n",
            "<p>Currently the following Models are provided:</p>\n",
            "<ul>\n",
            "<li>FCN32</li>\n",
            "<li>FCN16</li>\n",
            "<li>FCN8</li>\n",
            "</ul>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#remark\" id=\"user-content-remark\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Remark</h2>\n",
            "<p>The deconv layer of tensorflow allows to provide a shape. The crop layer of the original implementation is therefore not needed.</p>\n",
            "<p>I have slightly altered the naming of the upscore layer.</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#field-of-view\" id=\"user-content-field-of-view\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Field of View</h4>\n",
            "<p>The receptive field (also known as or <code>field of view</code>) of the provided model is:</p>\n",
            "<p><code>( ( ( ( ( 7 ) * 2 + 6 ) * 2 + 6 ) * 2 + 6 ) * 2 + 4 ) * 2 + 4 = 404</code></p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#predecessors\" id=\"user-content-predecessors\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Predecessors</h2>\n",
            "<p>Weights were generated using <a href=\"https://github.com/ethereon/caffe-tensorflow\">Caffe to Tensorflow</a>. The VGG implementation is based on <a href=\"https://github.com/ry/tensorflow-vgg16\">tensorflow-vgg16</a> and numpy loading is based on <a href=\"https://github.com/machrisaa/tensorflow-vgg\">tensorflow-vgg</a>. You do not need any of the above cited code to run the model, not do you need caffe.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#install\" id=\"user-content-install\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Install</h2>\n",
            "<p>Installing matplotlib from pip requires the following packages to be installed <code>libpng-dev</code>, <code>libjpeg8-dev</code>, <code>libfreetype6-dev</code> and <code>pkg-config</code>. On Debian, Linux Mint and Ubuntu Systems type:</p>\n",
            "<p><code>sudo apt-get install libpng-dev libjpeg8-dev libfreetype6-dev pkg-config</code> <br/>\n",
            "<code>pip install -r requirements.txt</code></p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#todo\" id=\"user-content-todo\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>TODO</h2>\n",
            "<ul>\n",
            "<li>Provide finetuned FCN weights.</li>\n",
            "<li>Provide general training code</li>\n",
            "</ul>\n",
            "</article>\n",
            "<article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><h1><a aria-hidden=\"true\" class=\"anchor\" href=\"#tensorflow-world\" id=\"user-content-tensorflow-world\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"http://tensorflow-world.readthedocs.io/en/latest/\" rel=\"nofollow\">TensorFlow World</a></h1>\n",
            "<a href=\"https://github.com/astorfi/TensorFlow-World/issues\"><img alt=\"https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\" data-canonical-src=\"https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\" src=\"https://camo.githubusercontent.com/f5054ffcd4245c10d3ec85ef059e07aacf787b560f83ad4aec2236364437d097/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e7376673f7374796c653d666c6174\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"https://github.com/ellerbrock/open-source-badge/\"><img alt=\"https://badges.frapsoft.com/os/v2/open-source.svg?v=102\" data-canonical-src=\"https://badges.frapsoft.com/os/v2/open-source.svg?v=102\" src=\"https://camo.githubusercontent.com/6905206cef7fe9e5779ef650f326885f79e67319e6b3a995718ceab8ef13964f/68747470733a2f2f6261646765732e66726170736f66742e636f6d2f6f732f76322f6f70656e2d736f757263652e7376673f763d313032\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"https://coveralls.io/github/astorfi/TensorFlow-World?branch=master\" rel=\"nofollow\"><img alt=\"https://coveralls.io/repos/github/astorfi/TensorFlow-World/badge.svg?branch=master\" data-canonical-src=\"https://coveralls.io/repos/github/astorfi/TensorFlow-World/badge.svg?branch=master\" src=\"https://camo.githubusercontent.com/07e35d3341decdc86df113d79cce451bfcc5d43726fdb1a2ae37343a1f6f3839/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f6173746f7266692f54656e736f72466c6f772d576f726c642f62616467652e7376673f6272616e63683d6d6173746572\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"https://zenodo.org/badge/latestdoi/86115145\" rel=\"nofollow\"><img data-canonical-src=\"https://zenodo.org/badge/86115145.svg\" src=\"https://camo.githubusercontent.com/636f5b35f76605049cb34045891c35399f414ade280aaceb2782f82f1cfefe62/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f38363131353134352e737667\" style=\"max-width:100%;\"/>\n",
            "</a>\n",
            "<a href=\"https://twitter.com/amirsinatorfi\" rel=\"nofollow\"><img alt=\"https://img.shields.io/twitter/follow/amirsinatorfi.svg?label=Follow&amp;style=social\" data-canonical-src=\"https://img.shields.io/twitter/follow/amirsinatorfi.svg?label=Follow&amp;style=social\" src=\"https://camo.githubusercontent.com/3e9b1629d0b81c6140ce17d2cfe4d3c7b514c141f31bc7137cd49a48193d3695/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f616d697273696e61746f7266692e7376673f6c6162656c3d466f6c6c6f77267374796c653d736f6369616c\" style=\"max-width:100%;\"/></a>\n",
            "<p>To support maintaining and upgrading this project, please kindly consider <a href=\"https://github.com/sponsors/astorfi/dashboard\">Sponsoring the project developer</a>.</p>\n",
            "<p>Any level of support is a great contribution here <g-emoji alias=\"heart\" class=\"g-emoji\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2764.png\">❤️</g-emoji></p>\n",
            "<div align=\"center\"><a href=\"https://github.com/sponsors/astorfi/dashboard\">\n",
            "<img align=\"center\" height=\"500\" src=\"https://github.com/instillai/TensorFlow-Course/raw/master/_img/mainpage/donation.jpg\" style=\"max-width:100%;\" width=\"600\"/>\n",
            "</a></div><p>This repository aims to provide simple and ready-to-use tutorials for TensorFlow. The explanations are present in the <a href=\"https://github.com/astorfi/TensorFlow-World/wiki\">wiki</a> associated with this repository.</p>\n",
            "<p>Each tutorial includes <code>source code</code> and associated <code>documentation</code>.</p>\n",
            "<div align=\"center\"><a href=\"http://www.machinelearningmindset.com/tensorflow-roadmap-ebook/\" rel=\"nofollow\">\n",
            "<img align=\"center\" height=\"600\" src=\"https://github.com/machinelearningmindset/TensorFlow-Course/raw/master/_img/mainpage/booksubscribe.png\" style=\"max-width:100%;\" width=\"850\"/>\n",
            "</a></div><a name=\"user-content-slack-group\"></a>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#slack-group\" id=\"user-content-slack-group\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Slack Group</h2>\n",
            "<div align=\"center\"><a href=\"https://www.machinelearningmindset.com/slack-group/\" rel=\"nofollow\">\n",
            "<img align=\"center\" height=\"350\" src=\"https://github.com/machinelearningmindset/TensorFlow-Course/raw/master/_img/0-welcome/joinslack.png\" style=\"max-width:100%;\" width=\"1033\"/>\n",
            "</a></div><a name=\"user-content-table-of-contents\"></a>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#table-of-contents\" id=\"user-content-table-of-contents\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Table of Contents</h2>\n",
            "<div id=\"user-content-contents\">\n",
            "<ul>\n",
            "<li><a href=\"#motivation\" id=\"user-content-id2\">Motivation</a><ul>\n",
            "<li><a href=\"#why-use-tensorflow\" id=\"user-content-id3\">Why use TensorFlow?</a></li>\n",
            "<li><a href=\"#what-s-the-point-of-this-repository\" id=\"user-content-id4\">What's the point of this repository?</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li><a href=\"#tensorflow-installation-and-setup-the-environment\" id=\"user-content-id5\">TensorFlow Installation and Setup the Environment</a></li>\n",
            "<li><a href=\"#tensorflow-tutorials\" id=\"user-content-id6\">TensorFlow Tutorials</a><ul>\n",
            "<li><a href=\"#warm-up\" id=\"user-content-id7\">Warm-up</a></li>\n",
            "<li><a href=\"#basics\" id=\"user-content-id8\">Basics</a></li>\n",
            "<li><a href=\"#basic-machine-learning\" id=\"user-content-id9\">Basic Machine Learning</a></li>\n",
            "<li><a href=\"#neural-networks\" id=\"user-content-id10\">Neural Networks</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li><a href=\"#some-useful-tutorials\" id=\"user-content-id11\">Some Useful Tutorials</a></li>\n",
            "<li><a href=\"#contributing\" id=\"user-content-id12\">Contributing</a><ul>\n",
            "<li><a href=\"#pull-request-process\" id=\"user-content-id13\">Pull Request Process</a></li>\n",
            "<li><a href=\"#final-note\" id=\"user-content-id14\">Final Note</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li><a href=\"#acknowledgement\" id=\"user-content-id15\">Acknowledgement</a></li>\n",
            "</ul>\n",
            "</div>\n",
            "<a name=\"user-content-motivation\"></a>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#motivation\" id=\"user-content-motivation\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id2\">Motivation</a></h3>\n",
            "<p>There are different motivations for this open source project. TensorFlow (as we write this document) is one of / the best deep learning frameworks available. The question that should be asked is why has this repository been created when there are so many other tutorials about TensorFlow available on the web?</p>\n",
            "<a name=\"user-content-why-use-tensorflow\"></a>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#why-use-tensorflow\" id=\"user-content-why-use-tensorflow\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id3\">Why use TensorFlow?</a></h4>\n",
            "<p>Deep Learning is in very high interest these days - there's a crucial need for rapid and optimized implementations of the algorithms and architectures. TensorFlow is designed to facilitate this goal.</p>\n",
            "<p>The strong advantage of TensorFlow is it flexibility in designing highly modular models which can also be a disadvantage for beginners since a lot of the pieces must be considered together when creating the model.</p>\n",
            "<p>This issue has been facilitated as well by developing high-level APIs such as <a href=\"https://keras.io/\" rel=\"nofollow\">Keras</a> and <a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/slim/README.md//\">Slim</a> which abstract a lot of the pieces used in designing machine learning algorithms.</p>\n",
            "<p>The interesting thing about TensorFlow is that <strong>it can be found anywhere these days</strong>. Lots of the researchers and developers are using it and <em>its community is growing at the speed of light</em>! So many issues can be dealt with easily since they're usually the same issues that a lot of other people run into considering the large number of people involved in the TensorFlow community.</p>\n",
            "<a name=\"user-content-what-s-the-point-of-this-repository\"></a>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#whats-the-point-of-this-repository\" id=\"user-content-whats-the-point-of-this-repository\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id4\">What's the point of this repository?</a></h4>\n",
            "<p><strong>Developing open source projects for the sake of just developing something is not the reason behind this effort</strong>.\n",
            "Considering the large number of tutorials that are being added to this large community, this repository has been created to break the jump-in and jump-out process that usually happens to most of the open source projects, <strong>but why and how</strong>?</p>\n",
            "<p>First of all, what's the point of putting effort into something that most of the people won't stop by and take a look? What's the point of creating something that does not help anyone in the developers and researchers community? Why spend time for something that can easily be forgotten? But <strong>how we try to do it?</strong> Even up to this\n",
            "very moment there are countless tutorials on TensorFlow whether on the model design or TensorFlow\n",
            "workflow.</p>\n",
            "<p>Most of them are too complicated or suffer from a lack of documentation. There are only a few available tutorials which are concise and well-structured and provide enough insight for their specific implemented models.</p>\n",
            "<p>The goal of this project is to help the community with structured tutorials and simple and optimized code implementations to provide better insight about how to use TensorFlow <em>quick and effectively</em>.</p>\n",
            "<p>It is worth noting that, <strong>the main goal of this project is to provide well-documented tutorials and less-complicated code</strong>!</p>\n",
            "<a name=\"user-content-tensorflow-installation-and-setup-the-environment\"></a>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#tensorflow-installation-and-setup-the-environment\" id=\"user-content-tensorflow-installation-and-setup-the-environment\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id5\">TensorFlow Installation and Setup the Environment</a></h3>\n",
            "<a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/docs/tutorials/installation\"><img alt=\"alternate text\" src=\"/astorfi/TensorFlow-World/raw/master/_img/mainpage/installation-logo.gif\" style=\"max-width:100%;\"/></a>\n",
            "<p>In order to install TensorFlow please refer to the following link:</p>\n",
            "<blockquote>\n",
            "<ul>\n",
            "<li><a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/docs/tutorials/installation\">TensorFlow Installation</a></li>\n",
            "</ul>\n",
            "</blockquote>\n",
            "<a href=\"https://www.youtube.com/watch?v=_3JFEPk4qQY&amp;t=2s\" rel=\"nofollow\"><img alt=\"_img/mainpage/installation.gif\" src=\"/astorfi/TensorFlow-World/raw/master/_img/mainpage/installation.gif\" style=\"max-width:100%;\"/></a>\n",
            "<p>The virtual environment installation is recommended in order to prevent package conflict and having the capacity to customize the working environment.</p>\n",
            "<a name=\"user-content-tensorflow-tutorials\"></a>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#tensorflow-tutorials\" id=\"user-content-tensorflow-tutorials\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id6\">TensorFlow Tutorials</a></h3>\n",
            "<p>The tutorials in this repository are partitioned into relevant categories.</p>\n",
            "<hr/>\n",
            "<a name=\"user-content-warm-up\"></a>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#warm-up\" id=\"user-content-warm-up\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id7\">Warm-up</a></h4>\n",
            "<p><a href=\"/astorfi/TensorFlow-World/blob/master/_img/mainpage/welcome.gif\" rel=\"noopener noreferrer\" target=\"_blank\"><img alt=\"alternate text\" src=\"/astorfi/TensorFlow-World/raw/master/_img/mainpage/welcome.gif\" style=\"max-width:100%;\"/></a></p>\n",
            "<table>\n",
            "<thead valign=\"bottom\">\n",
            "<tr><th>#</th>\n",
            "<th>topic</th>\n",
            "<th>Source Code</th>\n",
            "<th> </th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody valign=\"top\">\n",
            "<tr><td>1</td>\n",
            "<td>Start-up</td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/codes/0-welcome\">Welcome</a>  / <a href=\"https://github.com/astorfi/TensorFlow-World/blob/master/codes/0-welcome/code/0-welcome.ipynb\">IPython</a></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/blob/master/docs/tutorials/0-welcome\">Documentation</a></td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<hr/>\n",
            "<a name=\"user-content-basics\"></a>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#basics\" id=\"user-content-basics\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id8\">Basics</a></h4>\n",
            "<p><a href=\"/astorfi/TensorFlow-World/blob/master/_img/mainpage/basics.gif\" rel=\"noopener noreferrer\" target=\"_blank\"><img alt=\"alternate text\" src=\"/astorfi/TensorFlow-World/raw/master/_img/mainpage/basics.gif\" style=\"max-width:100%;\"/></a></p>\n",
            "<table>\n",
            "<thead valign=\"bottom\">\n",
            "<tr><th>#</th>\n",
            "<th>topic</th>\n",
            "<th>Source Code</th>\n",
            "<th> </th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody valign=\"top\">\n",
            "<tr><td>2</td>\n",
            "<td><em>TensorFLow Basics</em></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/codes/1-basics/basic_math_operations\">Basic Math Operations</a>   / <a href=\"https://github.com/astorfi/TensorFlow-World/blob/master/codes/1-basics/basic_math_operations/code/basic_math_operation.ipynb\">IPython</a></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/blob/master/docs/tutorials/1-basics/basic_math_operations\">Documentation</a></td>\n",
            "</tr>\n",
            "<tr><td>3</td>\n",
            "<td><em>TensorFLow Basics</em></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/blob/master/codes/1-basics/variables/README.rst\">TensorFlow Variables</a>   / <a href=\"https://github.com/astorfi/TensorFlow-World/blob/master/codes/1-basics/variables/code/variables.ipynb\">IPython</a></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/blob/master/docs/tutorials/1-basics/variables\">Documentation</a></td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<hr/>\n",
            "<a name=\"user-content-basic-machine-learning\"></a>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#basic-machine-learning\" id=\"user-content-basic-machine-learning\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id9\">Basic Machine Learning</a></h4>\n",
            "<p><a href=\"/astorfi/TensorFlow-World/blob/master/_img/mainpage/basicmodels.gif\" rel=\"noopener noreferrer\" target=\"_blank\"><img alt=\"alternate text\" src=\"/astorfi/TensorFlow-World/raw/master/_img/mainpage/basicmodels.gif\" style=\"max-width:100%;\"/></a></p>\n",
            "<table>\n",
            "<thead valign=\"bottom\">\n",
            "<tr><th>#</th>\n",
            "<th>topic</th>\n",
            "<th>Source Code</th>\n",
            "<th> </th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody valign=\"top\">\n",
            "<tr><td>4</td>\n",
            "<td><em>Linear Models</em></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/codes/2-basics_in_machine_learning/linear_regression\">Linear Regression</a>  / <a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/codes/2-basics_in_machine_learning/linear_regression/code/linear_regression.ipynb\">IPython</a></td>\n",
            "<td><a href=\"https://www.machinelearningmindset.com/linear-regression-with-tensorflow/\" rel=\"nofollow\">Documentation</a></td>\n",
            "</tr>\n",
            "<tr><td>5</td>\n",
            "<td><em>Predictive Models</em></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/codes/2-basics_in_machine_learning/logistic_regression\">Logistic Regression</a>  / <a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/codes/2-basics_in_machine_learning/logistic_regression/code/logistic_regression.ipynb\">IPython</a></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/docs/tutorials/2-basics_in_machine_learning/logistic_regression\">Documentation</a></td>\n",
            "</tr>\n",
            "<tr><td>6</td>\n",
            "<td><em>Support Vector Machines</em></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/codes/2-basics_in_machine_learning/linear_svm\">Linear SVM</a>  / <a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/codes/2-basics_in_machine_learning/linear_svm/code/linear_svm.ipynb\">IPython</a></td>\n",
            "<td> </td>\n",
            "</tr>\n",
            "<tr><td>7</td>\n",
            "<td><em>Support Vector Machines</em></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/blob/master/codes/2-basics_in_machine_learning/multiclass_svm\">MultiClass Kernel SVM</a>  / <a href=\"https://github.com/astorfi/TensorFlow-World/blob/master/codes/2-basics_in_machine_learning/multiclass_svm/code/multiclass_svm.ipynb\">IPython</a></td>\n",
            "<td> </td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<hr/>\n",
            "<a name=\"user-content-neural-networks\"></a>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#neural-networks\" id=\"user-content-neural-networks\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id10\">Neural Networks</a></h4>\n",
            "<p><a href=\"/astorfi/TensorFlow-World/blob/master/_img/mainpage/CNNs.png\" rel=\"noopener noreferrer\" target=\"_blank\"><img alt=\"alternate text\" src=\"/astorfi/TensorFlow-World/raw/master/_img/mainpage/CNNs.png\" style=\"max-width:100%;\"/></a></p>\n",
            "<table>\n",
            "<thead valign=\"bottom\">\n",
            "<tr><th>#</th>\n",
            "<th>topic</th>\n",
            "<th>Source Code</th>\n",
            "<th> </th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody valign=\"top\">\n",
            "<tr><td>8</td>\n",
            "<td><em>Multi Layer Perceptron</em></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/blob/master/codes/3-neural_networks/multi-layer-perceptron\">Simple Multi Layer Perceptron</a>   / <a href=\"https://github.com/astorfi/TensorFlow-World/blob/master/codes/3-neural_networks/multi-layer-perceptron/code/train_mlp.ipynb\">IPython</a></td>\n",
            "<td> </td>\n",
            "</tr>\n",
            "<tr><td>9</td>\n",
            "<td><em>Convolutional Neural Network</em></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/codes/3-neural_networks/convolutional-neural-network\">Simple Convolutional Neural Networks</a></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/blob/master/docs/tutorials/3-neural_network/convolutiona_neural_network\">Documentation</a></td>\n",
            "</tr>\n",
            "<tr><td>10</td>\n",
            "<td><em>Autoencoder</em></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/codes/3-neural_networks/undercomplete-autoencoder\">Undercomplete Autoencoder</a></td>\n",
            "<td><a href=\"https://github.com/astorfi/TensorFlow-World/tree/master/docs/tutorials/3-neural_network/autoencoder\">Documentation</a></td>\n",
            "</tr>\n",
            "<tr><td>11</td>\n",
            "<td><em>Recurrent Neural Network</em></td>\n",
            "<td><a href=\"/astorfi/TensorFlow-World/blob/master/codes/3-neural_networks/recurrent-neural-networks/code/rnn.py\">RNN</a>  / <a href=\"/astorfi/TensorFlow-World/blob/master/codes/3-neural_networks/recurrent-neural-networks/code/rnn.py\">IPython</a></td>\n",
            "<td> </td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<a name=\"user-content-some-useful-tutorials\"></a>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#some-useful-tutorials\" id=\"user-content-some-useful-tutorials\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id11\">Some Useful Tutorials</a></h3>\n",
            "<blockquote>\n",
            "<ul>\n",
            "<li><a href=\"https://github.com/aymericdamien/TensorFlow-Examples\">TensorFlow Examples</a> - TensorFlow tutorials and code examples for beginners</li>\n",
            "<li><a href=\"https://github.com/sjchoi86/Tensorflow-101\">Sungjoon's TensorFlow-101</a> - TensorFlow tutorials written in Python with Jupyter Notebook</li>\n",
            "<li><a href=\"https://github.com/terryum/TensorFlow_Exercises\">Terry Um’s TensorFlow Exercises</a> - Re-create the codes from other TensorFlow examples</li>\n",
            "<li><a href=\"https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\">Classification on time series</a> - Recurrent Neural Network classification in TensorFlow with LSTM on cellphone sensor data</li>\n",
            "</ul>\n",
            "</blockquote>\n",
            "<a name=\"user-content-contributing\"></a>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#contributing\" id=\"user-content-contributing\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id12\">Contributing</a></h3>\n",
            "<p>When contributing to this repository, please first discuss the change you wish to make via issue,\n",
            "email, or any other method with the owners of this repository before making a change. <em>For typos, please\n",
            "do not create a pull request. Instead, declare them in issues or email the repository owner</em>.</p>\n",
            "<p>Please note we have a code of conduct, please follow it in all your interactions with the project.</p>\n",
            "<a name=\"user-content-pull-request-process\"></a>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#pull-request-process\" id=\"user-content-pull-request-process\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id13\">Pull Request Process</a></h4>\n",
            "<p>Please consider the following criterions in order to help us in a better way:</p>\n",
            "<blockquote>\n",
            "<ul>\n",
            "<li>The pull request is mainly expected to be a code script suggestion or improvement.</li>\n",
            "<li>A pull request related to non-code-script sections is expected to make a significant difference in the documentation. Otherwise, it is expected to be announced in the issues section.</li>\n",
            "<li>Ensure any install or build dependencies are removed before the end of the layer when doing a build and creating a pull request.</li>\n",
            "<li>Add comments with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters.</li>\n",
            "<li>You may merge the Pull Request in once you have the sign-off of at least one other developer, or if you do not have permission to do that, you may request the owner to merge it for you if you believe all checks are passed.</li>\n",
            "</ul>\n",
            "</blockquote>\n",
            "<a name=\"user-content-final-note\"></a>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#final-note\" id=\"user-content-final-note\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id14\">Final Note</a></h4>\n",
            "<p>We are looking forward to your kind feedback. Please help us to improve this open source project and make our work better.\n",
            "For contribution, please create a pull request and we will investigate it promptly. Once again, we appreciate\n",
            "your kind feedback and elaborate code inspections.</p>\n",
            "<a name=\"user-content-acknowledgement\"></a>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#acknowledgement\" id=\"user-content-acknowledgement\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"#id15\">Acknowledgement</a></h3>\n",
            "<p>I have taken huge efforts in this project for hopefully being a small part of TensorFlow world. However, it would not have been plausible without the kind support and help of my friend and colleague <a href=\"https://github.com/vonclites/\">Domenick Poster</a> for his valuable advices. He helped me for having a better understanding of TensorFlow and my special appreciation goes to him.</p>\n",
            "</article>\n",
            "<article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#文本分类项目\" id=\"user-content-文本分类项目\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>文本分类项目</h3>\n",
            "<hr/>\n",
            "<p><strong>本项目为基于CNN，RNN 和NLP中预训练模型构建的多个常见的文本分类模型。</strong></p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#requirements\" id=\"user-content-requirements\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>requirements</h4>\n",
            "<ul>\n",
            "<li>python==3.5.6</li>\n",
            "<li>tensorflow-gpu==1.10.0</li>\n",
            "</ul>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#1-数据集\" id=\"user-content-1-数据集\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>1. 数据集</h4>\n",
            "<p>  数据集为IMDB电影评论的情感分析数据集，总共有三个部分：</p>\n",
            "<ul>\n",
            "<li>带标签的训练集：labeledTrainData.tsv</li>\n",
            "<li>不带标签的训练集：unlabeledTrainData.tsv</li>\n",
            "<li>测试集：testData.tsv</li>\n",
            "</ul>\n",
            "<p>  字段的含义：</p>\n",
            "<ul>\n",
            "<li>id  电影评论的id</li>\n",
            "<li>review  电影评论的内容</li>\n",
            "<li>sentiment  情感分类的标签（只有labeledTrainData.tsv数据集中有）</li>\n",
            "</ul>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#2-数据预处理\" id=\"user-content-2-数据预处理\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>2. 数据预处理</h4>\n",
            "<p>  数据预处理方法/dataHelper/processData.ipynb</p>\n",
            "<p>  将原始数据处理成干净的数据，处理后的数据存储在/data/preProcess下，数据预处理包括：</p>\n",
            "<ul>\n",
            "<li>去除各种标点符号</li>\n",
            "<li>生成训练word2vec模型的输入数据 /data/preProcess/wordEmbedding.txt</li>\n",
            "</ul>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#3-训练word2vec词向量\" id=\"user-content-3-训练word2vec词向量\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>3. 训练word2vec词向量</h4>\n",
            "<p>  预训练word2vec词向量/word2vec/genWord2Vec.ipynb</p>\n",
            "<ul>\n",
            "<li>预训练的词向量保存为bin格式 /word2vec/word2Vec.bin</li>\n",
            "</ul>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#4-textcnn-文本分类\" id=\"user-content-4-textcnn-文本分类\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>4. textCNN 文本分类</h4>\n",
            "<p>  textCNN模型来源于论文<a href=\"https://arxiv.org/abs/1408.5882\" rel=\"nofollow\">Convolutional Neural Networks for Sentence Classification</a></p>\n",
            "<p>  textCNN可以看作是一个由三个单层的卷积网络的输出结果进行拼接的融合模型，作者提出了三种大小的卷积核[3, 4, 5]，卷积核的滑动使得其\n",
            "类似于NLP中的n-grams，因此当你需要更多尺度的n-grams时，你可以选择增加不同大小的卷积核，比如大小为2的卷积核可以代表\n",
            "2-grams.</p>\n",
            "<p>  textCNN代码在/textCNN/textCNN.ipynb。实现包括四个部分：</p>\n",
            "<ul>\n",
            "<li>参数配置类 Config （包括训练参数，模型参数和其他参数）</li>\n",
            "<li>数据预处理类 Dataset （包括生成词汇空间，获得预训练词向量，分割训练集和验证集）</li>\n",
            "<li>textCNN模型类 TextCNN</li>\n",
            "<li>模型训练</li>\n",
            "</ul>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#5-charcnn-文本分类\" id=\"user-content-5-charcnn-文本分类\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>5. charCNN 文本分类</h4>\n",
            "<p>  textCNN模型来源于论文<a href=\"https://arxiv.org/abs/1509.01626\" rel=\"nofollow\">Character-level Convolutional Networks for Text\n",
            "Classification</a></p>\n",
            "<p>  char-CNN是一种基于字符级的文本分类器，将所有的文本都用字符表示，\n",
            "<em>注意这里的数据预处理时不可以去掉标点符号或者其他的各种符号，最好是保存论文中提出的69种字符，我一开始使用去掉特殊符号的字符后的文本输入到模型中会无法收敛</em>。\n",
            "此外由于训练数据集比较少，即使论文中最小的网络也无法收敛，此时可以减小模型的复杂度，包括去掉一些卷积层等。</p>\n",
            "<p>  charCNN代码在/charCNN/charCNN.ipynb。实现也包括四个部分，也textCNN一致，但是在这里的数据预处理有很大不一样，剩下\n",
            "的就是模型结构不同，此外模型中可以引入BN层来对每一层的输出做归一化处理。</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#6-bi-lstm-文本分类\" id=\"user-content-6-bi-lstm-文本分类\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>6. Bi-LSTM 文本分类</h4>\n",
            "<p>  Bi-LSTM可以参考我的博客<a href=\"https://www.cnblogs.com/jiangxinyang/p/9362922.html\" rel=\"nofollow\">深度学习之从RNN到LSTM</a></p>\n",
            "<p>  Bi-LSTM是双向LSTM，LSTM是RNN的一种，是一种时序模型，Bi-LSTM是双向LSTM，旨在同时捕获文本中上下文的信息，\n",
            "在情感分析类的问题中有良好的表现。</p>\n",
            "<p>  Bi-LSTM的代码在/Bi-LSTM/Bi-LSTM.ipynb中。除了模型类的代码有改动，其余代码几乎和textCNN一样。</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#7-bi-lstm--attention-文本分类\" id=\"user-content-7-bi-lstm--attention-文本分类\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>7. Bi-LSTM + Attention 文本分类</h4>\n",
            "<p>  Bi-LSTM + Attention模型来源于论文<a href=\"http://aclweb.org/anthology/Y/Y15/Y15-1009.pdf\" rel=\"nofollow\">Attention-Based Bidirectional Long Short-Term Memory Networks for\n",
            "Relation Classification</a></p>\n",
            "<p>  Bi-LSTM + Attention 就是在Bi-LSTM的模型上加入Attention层，在Bi-LSTM中我们会用最后一个时序的输出向量\n",
            "作为特征向量，然后进行softmax分类。Attention是先计算每个时序的权重，然后将所有时序\n",
            "的向量进行加权和作为特征向量，然后进行softmax分类。在实验中，加上Attention确实对结果有所提升。</p>\n",
            "<p>  Bi-LSTM + Attention的代码在/Bi-LSTM+Attention/Bi-LSTMAttention.ipynb中，除了模型类中\n",
            "加入Attention层，其余代码和Bi-LSTM一致。</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#8-rcnn-文本分类\" id=\"user-content-8-rcnn-文本分类\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>8. RCNN 文本分类</h4>\n",
            "<p>  RCNN模型来源于论文<a href=\"https://arxiv.org/abs/1609.04243\" rel=\"nofollow\">Recurrent Convolutional Neural Networks for Text Classification</a></p>\n",
            "<p>  RCNN 整体的模型构建流程如下：</p>\n",
            "<ul>\n",
            "<li>利用Bi-LSTM获得上下文的信息，类似于语言模型</li>\n",
            "<li>将Bi-LSTM获得的隐层输出和词向量拼接[fwOutput, wordEmbedding, bwOutput]</li>\n",
            "<li>将拼接后的向量非线性映射到低维</li>\n",
            "<li>向量中的每一个位置的值都取所有时序上的最大值，得到最终的特征向量，该过程类似于max-pool</li>\n",
            "<li>softmax分类</li>\n",
            "</ul>\n",
            "<p>  RCNN的代码在/RCNN/RCNN.ipynb中。</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#9-adversariallstm-文本分类\" id=\"user-content-9-adversariallstm-文本分类\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>9. adversarialLSTM 文本分类</h4>\n",
            "<p>  Adversarial LSTM模型来源于论文<a href=\"https://arxiv.org/abs/1605.07725\" rel=\"nofollow\">Adversarial Training Methods\n",
            "For Semi-Supervised Text Classification</a></p>\n",
            "<p>  adversarialLSTM的核心思想是通过对word Embedding上添加噪音生成对抗样本，将对抗样本以和原始样本\n",
            "同样的形式喂给模型，得到一个Adversarial Loss，通过和原始样本的loss相加得到新的损失，通过优化该新\n",
            "的损失来训练模型，作者认为这种方法能对word embedding加上正则化，避免过拟合。</p>\n",
            "<p>  adversarialLSTM的代码在/adversarialLSTM/adversarialLSTM.ipynb中。</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#10-transformer-文本分类\" id=\"user-content-10-transformer-文本分类\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>10. Transformer 文本分类</h4>\n",
            "<p>  Transformer模型来源于论文<a href=\"https://arxiv.org/abs/1706.03762\" rel=\"nofollow\">Attention Is All You Need</a></p>\n",
            "<p>  Transformer模型有两个结构：Encoder和Decoder，在进行文本分类时只需要用到\n",
            "Encoder结构，Decoder结构是生成式模型，用于自然语言生成的。Transformer的核心结构是\n",
            "self-Attention机制，具体的介绍见<a href=\"https://www.cnblogs.com/jiangxinyang/p/10069330.html\" rel=\"nofollow\">Transformer模型（Atention is all you need）</a>。</p>\n",
            "<p>  Transformer模型的代码在/Transformer/transformer.ipynb中。</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#11-elmo预训练模型-文本分类\" id=\"user-content-11-elmo预训练模型-文本分类\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>11. ELMo预训练模型 文本分类</h4>\n",
            "<p>  ELMo模型来源于论文<a href=\"https://arxiv.org/abs/1802.05365?context=cs\" rel=\"nofollow\">Deep contextualized word representations</a></p>\n",
            "<p>  ELMo的结构是BiLM（双向语言模型），基于ELMo的预训练模型能动态地生成\n",
            "词的向量表示，具体的介绍见<a href=\"https://www.cnblogs.com/jiangxinyang/p/10060887.html\" rel=\"nofollow\">ELMO模型（Deep contextualized word representation）</a></p>\n",
            "<p>  ELMo预训练模型用于文本分类的代码位于/ELMo/elmo.ipynb中。\n",
            "/ELMo/bilm/下是ELMo项目中的源码，/ELMo/modelParams/下是各种文件。</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#12-bert预训练模型-文本分类\" id=\"user-content-12-bert预训练模型-文本分类\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>12. Bert预训练模型 文本分类</h4>\n",
            "<p>  BERT模型来源于论文<a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">BERT: Pre-training of Deep Bidirectional Transformers for\n",
            "Language Understanding</a></p>\n",
            "<p>  BERT模型是基于双向Transformer实现的语言模型，集预训练和下游任务于一个模型中，\n",
            "因此在使用的时候我们不需要搭建自己的下游任务模型，直接用BERT模型即可，我们将谷歌开源的源码下载\n",
            "下来放在bert文件夹中，在进行文本分类只需要修改run_classifier.py文件即可，另外我们需要将训练集\n",
            "和验证集分割后保存在两个不同的文件中，放置在/BERT/data下。然后还需要下载谷歌预训练好的模型放置在\n",
            "/BERT/modelParams文件夹下，还需要建一个/BERT/output文件夹用来放置训练后的模型文件</p>\n",
            "<p>  做完上面的步骤之后只要执行下面的脚本即可</p>\n",
            "<p>  export BERT_BASE_DIR=../modelParams/uncased_L-12_H-768_A-12</p>\n",
            "<p>  export DATASET=../data/</p>\n",
            "<p>  python run_classifier.py <br/>\n",
            "    --data_dir=$MY_DATASET <br/>\n",
            "    --task_name=imdb <br/>\n",
            "    --vocab_file=$BERT_BASE_DIR/vocab.txt <br/>\n",
            "    --bert_config_file=$BERT_BASE_DIR/bert_config.json <br/>\n",
            "    --output_dir=../output/ <br/>\n",
            "    --do_train=true <br/>\n",
            "    --do_eval=true <br/>\n",
            "    --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt <br/>\n",
            "    --max_seq_length=200 <br/>\n",
            "    --train_batch_size=16 <br/>\n",
            "    --learning_rate=5e-5<br/>\n",
            "    --num_train_epochs=3.0</p>\n",
            "<p>  BERT模型用于文本分类的详细使用可以看我的博客\n",
            "<a href=\"https://www.cnblogs.com/jiangxinyang/p/10241243.html\" rel=\"nofollow\">文本分类实战（十）—— BERT 预训练模型</a></p>\n",
            "</article>\n",
            "<article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><p align=\"center\">\n",
            "<a href=\"#readme\">\n",
            "<img alt=\"logo\" src=\"/huseinzol05/NLP-Models-Tensorflow/raw/master/nlp-tf.png\" style=\"max-width:100%;\" width=\"40%\"/>\n",
            "</a>\n",
            "</p>\n",
            "<p align=\"center\">\n",
            "<a href=\"https://github.com/huseinzol05/NLP-Models-Tensorflow/blob/master/LICENSE\"><img alt=\"MIT License\" data-canonical-src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" src=\"https://camo.githubusercontent.com/78f47a09877ba9d28da1887a93e5c3bc2efb309c1e910eb21135becd2998238a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667\" style=\"max-width:100%;\"/></a>\n",
            "<a href=\"#\"><img data-canonical-src=\"https://img.shields.io/badge/total%20notebooks-335--models-blue.svg\" src=\"https://camo.githubusercontent.com/41e35f3b9f81f7b3097e78842feaaebdcc9ab7d9cea7fe50b2a98e522ce024a0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f746f74616c2532306e6f7465626f6f6b732d3333352d2d6d6f64656c732d626c75652e737667\" style=\"max-width:100%;\"/></a>\n",
            "</p>\n",
            "<hr/>\n",
            "<p><strong>NLP-Models-Tensorflow</strong>, Gathers machine learning and tensorflow deep learning models for NLP problems, <strong>code simplify inside Jupyter Notebooks 100%</strong>.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#table-of-contents\" id=\"user-content-table-of-contents\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Table of contents</h2>\n",
            "<ul>\n",
            "<li><a href=\"#abstractive-summarization\">Abstractive Summarization</a></li>\n",
            "<li><a href=\"#chatbot\">Chatbot</a></li>\n",
            "<li><a href=\"#dependency-parser\">Dependency Parser</a></li>\n",
            "<li><a href=\"#entity-tagging\">Entity Tagging</a></li>\n",
            "<li><a href=\"#extractive-summarization\">Extractive Summarization</a></li>\n",
            "<li><a href=\"#generator\">Generator</a></li>\n",
            "<li><a href=\"#language-detection\">Language Detection</a></li>\n",
            "<li><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/neural-machine-translation\">Neural Machine Translation</a></li>\n",
            "<li><a href=\"#ocr-optical-character-recognition\">OCR</a></li>\n",
            "<li><a href=\"#pos-tagging\">POS Tagging</a></li>\n",
            "<li><a href=\"#question-answers\">Question-Answers</a></li>\n",
            "<li><a href=\"#sentence-pair\">Sentence pairs</a></li>\n",
            "<li><a href=\"#speech-to-text\">Speech-to-Text</a></li>\n",
            "<li><a href=\"#spelling-correction\">Spelling correction</a></li>\n",
            "<li><a href=\"#squad-question-answers\">SQUAD Question-Answers</a></li>\n",
            "<li><a href=\"#stemming\">Stemming</a></li>\n",
            "<li><a href=\"#text-augmentation\">Text Augmentation</a></li>\n",
            "<li><a href=\"#text-classification\">Text Classification</a></li>\n",
            "<li><a href=\"#text-similarity\">Text Similarity</a></li>\n",
            "<li><a href=\"#text-to-speech\">Text-to-Speech</a></li>\n",
            "<li><a href=\"#topic-generator\">Topic Generator</a></li>\n",
            "<li><a href=\"#topic-modeling\">Topic Modeling</a></li>\n",
            "<li><a href=\"#unsupervised-extractive-summarization\">Unsupervised Extractive Summarization</a></li>\n",
            "<li><a href=\"#vectorizer\">Vectorizer</a></li>\n",
            "<li><a href=\"#old-to-young-vocoder\">Old-to-Young Vocoder</a></li>\n",
            "<li><a href=\"#visualization\">Visualization</a></li>\n",
            "<li><a href=\"#attention\">Attention</a></li>\n",
            "</ul>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#objective\" id=\"user-content-objective\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Objective</h2>\n",
            "<p>Original implementations are quite complex and not really beginner friendly. So I tried to simplify most of it. Also, there are tons of not-yet release papers implementation. So feel free to use it for your own research!</p>\n",
            "<p>I will attached github repositories for models that I not implemented from scratch, basically I copy, paste and fix those code for deprecated issues.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#tensorflow-version\" id=\"user-content-tensorflow-version\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Tensorflow version</h2>\n",
            "<p>Tensorflow version 1.13 and above only, not included 2.X version. 1.13 &lt; Tensorflow &lt; 2.0</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre>pip install -r requirements.txt</pre></div>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#contents\" id=\"user-content-contents\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Contents</h2>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#abstractive-summarization\" id=\"user-content-abstractive-summarization\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/abstractive-summarization\">Abstractive Summarization</a></h3>\n",
            "<p>Trained on <a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/abstractive-summarization/dataset\">India news</a>.</p>\n",
            "<p>Accuracy based on 10 epochs only, calculated using word positions.</p>\n",
            "<details><summary>Complete list (12 notebooks)</summary>\n",
            "<ol>\n",
            "<li>LSTM Seq2Seq using topic modelling, test accuracy 13.22%</li>\n",
            "<li>LSTM Seq2Seq + Luong Attention using topic modelling, test accuracy 12.39%</li>\n",
            "<li>LSTM Seq2Seq + Beam Decoder using topic modelling, test accuracy 10.67%</li>\n",
            "<li>LSTM Bidirectional + Luong Attention + Beam Decoder using topic modelling, test accuracy 8.29%</li>\n",
            "<li>Pointer-Generator + Bahdanau, <a href=\"https://github.com/xueyouluo/my_seq2seq\">https://github.com/xueyouluo/my_seq2seq</a>, test accuracy 15.51%</li>\n",
            "<li>Copynet, test accuracy 11.15%</li>\n",
            "<li>Pointer-Generator + Luong, <a href=\"https://github.com/xueyouluo/my_seq2seq\">https://github.com/xueyouluo/my_seq2seq</a>, test accuracy 16.51%</li>\n",
            "<li>Dilated Seq2Seq, test accuracy 10.88%</li>\n",
            "<li>Dilated Seq2Seq + Self Attention, test accuracy 11.54%</li>\n",
            "<li>BERT + Dilated CNN Seq2seq, test accuracy 13.5%</li>\n",
            "<li>self-attention + Pointer-Generator, test accuracy 4.34%</li>\n",
            "<li>Dilated-CNN Seq2seq + Pointer-Generator, test accuracy 5.57%</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#chatbot\" id=\"user-content-chatbot\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/chatbot\">Chatbot</a></h3>\n",
            "<p>Trained on <a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/chatbot/dataset.tar.gz\">Cornell Movie Dialog corpus</a>, accuracy table in <a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/chatbot\">chatbot</a>.</p>\n",
            "<details><summary>Complete list (54 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Basic cell Seq2Seq-manual</li>\n",
            "<li>LSTM Seq2Seq-manual</li>\n",
            "<li>GRU Seq2Seq-manual</li>\n",
            "<li>Basic cell Seq2Seq-API Greedy</li>\n",
            "<li>LSTM Seq2Seq-API Greedy</li>\n",
            "<li>GRU Seq2Seq-API Greedy</li>\n",
            "<li>Basic cell Bidirectional Seq2Seq-manual</li>\n",
            "<li>LSTM Bidirectional Seq2Seq-manual</li>\n",
            "<li>GRU Bidirectional Seq2Seq-manual</li>\n",
            "<li>Basic cell Bidirectional Seq2Seq-API Greedy</li>\n",
            "<li>LSTM Bidirectional Seq2Seq-API Greedy</li>\n",
            "<li>GRU Bidirectional Seq2Seq-API Greedy</li>\n",
            "<li>Basic cell Seq2Seq-manual + Luong Attention</li>\n",
            "<li>LSTM Seq2Seq-manual + Luong Attention</li>\n",
            "<li>GRU Seq2Seq-manual + Luong Attention</li>\n",
            "<li>Basic cell Seq2Seq-manual + Bahdanau Attention</li>\n",
            "<li>LSTM Seq2Seq-manual + Bahdanau Attention</li>\n",
            "<li>GRU Seq2Seq-manual + Bahdanau Attention</li>\n",
            "<li>LSTM Bidirectional Seq2Seq-manual + Luong Attention</li>\n",
            "<li>GRU Bidirectional Seq2Seq-manual + Luong Attention</li>\n",
            "<li>LSTM Bidirectional Seq2Seq-manual + Bahdanau Attention</li>\n",
            "<li>GRU Bidirectional Seq2Seq-manual + Bahdanau Attention</li>\n",
            "<li>LSTM Bidirectional Seq2Seq-manual + backward Bahdanau + forward Luong</li>\n",
            "<li>GRU Bidirectional Seq2Seq-manual + backward Bahdanau + forward Luong</li>\n",
            "<li>LSTM Seq2Seq-API Greedy + Luong Attention</li>\n",
            "<li>GRU Seq2Seq-API Greedy + Luong Attention</li>\n",
            "<li>LSTM Seq2Seq-API Greedy + Bahdanau Attention</li>\n",
            "<li>GRU Seq2Seq-API Greedy + Bahdanau Attention</li>\n",
            "<li>LSTM Seq2Seq-API Beam Decoder</li>\n",
            "<li>GRU Seq2Seq-API Beam Decoder</li>\n",
            "<li>LSTM Bidirectional Seq2Seq-API + Luong Attention + Beam Decoder</li>\n",
            "<li>GRU Bidirectional Seq2Seq-API + Luong Attention + Beam Decoder</li>\n",
            "<li>LSTM Bidirectional Seq2Seq-API + backward Bahdanau + forward Luong + Stack Bahdanau Luong Attention + Beam Decoder</li>\n",
            "<li>GRU Bidirectional Seq2Seq-API + backward Bahdanau + forward Luong + Stack Bahdanau Luong Attention + Beam Decoder</li>\n",
            "<li>Bytenet</li>\n",
            "<li>LSTM Seq2Seq + tf.estimator</li>\n",
            "<li>Capsule layers + LSTM Seq2Seq-API Greedy</li>\n",
            "<li>Capsule layers + LSTM Seq2Seq-API + Luong Attention + Beam Decoder</li>\n",
            "<li>LSTM Bidirectional Seq2Seq-API + backward Bahdanau + forward Luong + Stack Bahdanau Luong Attention + Beam Decoder + Dropout + L2</li>\n",
            "<li>DNC Seq2Seq</li>\n",
            "<li>LSTM Bidirectional Seq2Seq-API + Luong Monotic Attention + Beam Decoder</li>\n",
            "<li>LSTM Bidirectional Seq2Seq-API + Bahdanau Monotic Attention + Beam Decoder</li>\n",
            "<li>End-to-End Memory Network + Basic cell</li>\n",
            "<li>End-to-End Memory Network + LSTM cell</li>\n",
            "<li>Attention is all you need</li>\n",
            "<li>Transformer-XL</li>\n",
            "<li>Attention is all you need + Beam Search</li>\n",
            "<li>Transformer-XL + LSTM</li>\n",
            "<li>GPT-2 + LSTM</li>\n",
            "<li>CNN Seq2seq</li>\n",
            "<li>Conv-Encoder + LSTM</li>\n",
            "<li>Tacotron + Greedy decoder</li>\n",
            "<li>Tacotron + Beam decoder</li>\n",
            "<li>Google NMT</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#dependency-parser\" id=\"user-content-dependency-parser\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/dependency-parser\">Dependency-Parser</a></h3>\n",
            "<p>Trained on <a href=\"https://github.com/UniversalDependencies/UD_English-EWT\">CONLL English Dependency</a>. Train set to train, dev and test sets to test.</p>\n",
            "<p>Stackpointer and Biaffine-attention originally from <a href=\"https://github.com/XuezheMax/NeuroNLP2\">https://github.com/XuezheMax/NeuroNLP2</a> written in Pytorch.</p>\n",
            "<p>Accuracy based on arc, types and root accuracies after 15 epochs only.</p>\n",
            "<details><summary>Complete list (8 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Bidirectional RNN + CRF + Biaffine, arc accuracy 70.48%, types accuracy 65.18%, root accuracy 66.4%</li>\n",
            "<li>Bidirectional RNN + Bahdanau + CRF + Biaffine, arc accuracy 70.82%, types accuracy 65.33%, root accuracy 66.77%</li>\n",
            "<li>Bidirectional RNN + Luong + CRF + Biaffine, arc accuracy 71.22%, types accuracy 65.73%, root accuracy 67.23%</li>\n",
            "<li>BERT Base + CRF + Biaffine, arc accuracy 64.30%, types accuracy 62.89%, root accuracy 74.19%</li>\n",
            "<li>Bidirectional RNN + Biaffine Attention + Cross Entropy, arc accuracy 72.42%, types accuracy 63.53%, root accuracy 68.51%</li>\n",
            "<li>BERT Base + Biaffine Attention + Cross Entropy, arc accuracy 72.85%, types accuracy 67.11%, root accuracy 73.93%</li>\n",
            "<li>Bidirectional RNN + Stackpointer, arc accuracy 61.88%, types accuracy 48.20%, root accuracy 89.39%</li>\n",
            "<li>XLNET Base + Biaffine Attention + Cross Entropy, arc accuracy 74.41%, types accuracy 71.37%, root accuracy 73.17%</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#entity-tagging\" id=\"user-content-entity-tagging\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/entity-tagging\">Entity-Tagging</a></h3>\n",
            "<p>Trained on <a href=\"https://cogcomp.org/page/resource_view/81\" rel=\"nofollow\">CONLL NER</a>.</p>\n",
            "<details><summary>Complete list (9 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Bidirectional RNN + CRF, test accuracy 96%</li>\n",
            "<li>Bidirectional RNN + Luong Attention + CRF, test accuracy 93%</li>\n",
            "<li>Bidirectional RNN + Bahdanau Attention + CRF, test accuracy 95%</li>\n",
            "<li>Char Ngrams + Bidirectional RNN + Bahdanau Attention + CRF, test accuracy 96%</li>\n",
            "<li>Char Ngrams + Bidirectional RNN + Bahdanau Attention + CRF, test accuracy 96%</li>\n",
            "<li>Char Ngrams + Residual Network + Bahdanau Attention + CRF, test accuracy 69%</li>\n",
            "<li>Char Ngrams + Attention is you all Need + CRF, test accuracy 90%</li>\n",
            "<li>BERT, test accuracy 99%</li>\n",
            "<li>XLNET-Base, test accuracy 99%</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#extractive-summarization\" id=\"user-content-extractive-summarization\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/extractive-summarization\">Extractive Summarization</a></h3>\n",
            "<p>Trained on <a href=\"https://cs.nyu.edu/~kcho/DMQA/\" rel=\"nofollow\">CNN News dataset</a>.</p>\n",
            "<p>Accuracy based on ROUGE-2.</p>\n",
            "<details><summary>Complete list (4 notebooks)</summary>\n",
            "<ol>\n",
            "<li>LSTM RNN, test accuracy 16.13%</li>\n",
            "<li>Dilated-CNN, test accuracy 15.54%</li>\n",
            "<li>Multihead Attention, test accuracy 26.33%</li>\n",
            "<li>BERT-Base</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#generator\" id=\"user-content-generator\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/generator\">Generator</a></h3>\n",
            "<p>Trained on <a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/generator/shakespeare.txt\">Shakespeare dataset</a>.</p>\n",
            "<details><summary>Complete list (15 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Character-wise RNN + LSTM</li>\n",
            "<li>Character-wise RNN + Beam search</li>\n",
            "<li>Character-wise RNN + LSTM + Embedding</li>\n",
            "<li>Word-wise RNN + LSTM</li>\n",
            "<li>Word-wise RNN + LSTM + Embedding</li>\n",
            "<li>Character-wise + Seq2Seq + GRU</li>\n",
            "<li>Word-wise + Seq2Seq + GRU</li>\n",
            "<li>Character-wise RNN + LSTM + Bahdanau Attention</li>\n",
            "<li>Character-wise RNN + LSTM + Luong Attention</li>\n",
            "<li>Word-wise + Seq2Seq + GRU + Beam</li>\n",
            "<li>Character-wise + Seq2Seq + GRU + Bahdanau Attention</li>\n",
            "<li>Word-wise + Seq2Seq + GRU + Bahdanau Attention</li>\n",
            "<li>Character-wise Dilated CNN + Beam search</li>\n",
            "<li>Transformer + Beam search</li>\n",
            "<li>Transformer XL + Beam search</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#language-detection\" id=\"user-content-language-detection\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/language-detection\">Language-detection</a></h3>\n",
            "<p>Trained on <a href=\"http://downloads.tatoeba.org/exports/sentences.tar.bz2\" rel=\"nofollow\">Tatoeba dataset</a>.</p>\n",
            "<details><summary>Complete list (1 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Fast-text Char N-Grams</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#neural-machine-translation\" id=\"user-content-neural-machine-translation\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/neural-machine-translation\">Neural Machine Translation</a></h3>\n",
            "<p>Trained on <a href=\"https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/translate_enfr.py\">English-French</a>, accuracy table in <a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/neural-machine-translation\">neural-machine-translation</a>.</p>\n",
            "<details><summary>Complete list (53 notebooks)</summary>\n",
            "<p>1.basic-seq2seq\n",
            "2.lstm-seq2seq\n",
            "3.gru-seq2seq\n",
            "4.basic-seq2seq-contrib-greedy\n",
            "5.lstm-seq2seq-contrib-greedy\n",
            "6.gru-seq2seq-contrib-greedy\n",
            "7.basic-birnn-seq2seq\n",
            "8.lstm-birnn-seq2seq\n",
            "9.gru-birnn-seq2seq\n",
            "10.basic-birnn-seq2seq-contrib-greedy\n",
            "11.lstm-birnn-seq2seq-contrib-greedy\n",
            "12.gru-birnn-seq2seq-contrib-greedy\n",
            "13.basic-seq2seq-luong\n",
            "14.lstm-seq2seq-luong\n",
            "15.gru-seq2seq-luong\n",
            "16.basic-seq2seq-bahdanau\n",
            "17.lstm-seq2seq-bahdanau\n",
            "18.gru-seq2seq-bahdanau\n",
            "19.basic-birnn-seq2seq-bahdanau\n",
            "20.lstm-birnn-seq2seq-bahdanau\n",
            "21.gru-birnn-seq2seq-bahdanau\n",
            "22.basic-birnn-seq2seq-luong\n",
            "23.lstm-birnn-seq2seq-luong\n",
            "24.gru-birnn-seq2seq-luong\n",
            "25.lstm-seq2seq-contrib-greedy-luong\n",
            "26.gru-seq2seq-contrib-greedy-luong\n",
            "27.lstm-seq2seq-contrib-greedy-bahdanau\n",
            "28.gru-seq2seq-contrib-greedy-bahdanau\n",
            "29.lstm-seq2seq-contrib-beam-luong\n",
            "30.gru-seq2seq-contrib-beam-luong\n",
            "31.lstm-seq2seq-contrib-beam-bahdanau\n",
            "32.gru-seq2seq-contrib-beam-bahdanau\n",
            "33.lstm-birnn-seq2seq-contrib-beam-bahdanau\n",
            "34.lstm-birnn-seq2seq-contrib-beam-luong\n",
            "35.gru-birnn-seq2seq-contrib-beam-bahdanau\n",
            "36.gru-birnn-seq2seq-contrib-beam-luong\n",
            "37.lstm-birnn-seq2seq-contrib-beam-luongmonotonic\n",
            "38.gru-birnn-seq2seq-contrib-beam-luongmonotic\n",
            "39.lstm-birnn-seq2seq-contrib-beam-bahdanaumonotonic\n",
            "40.gru-birnn-seq2seq-contrib-beam-bahdanaumonotic\n",
            "41.residual-lstm-seq2seq-greedy-luong\n",
            "42.residual-gru-seq2seq-greedy-luong\n",
            "43.residual-lstm-seq2seq-greedy-bahdanau\n",
            "44.residual-gru-seq2seq-greedy-bahdanau\n",
            "45.memory-network-lstm-decoder-greedy\n",
            "46.google-nmt\n",
            "47.transformer-encoder-transformer-decoder\n",
            "48.transformer-encoder-lstm-decoder-greedy\n",
            "49.bertmultilanguage-encoder-bertmultilanguage-decoder\n",
            "50.bertmultilanguage-encoder-lstm-decoder\n",
            "51.bertmultilanguage-encoder-transformer-decoder\n",
            "52.bertenglish-encoder-transformer-decoder\n",
            "53.transformer-t2t-2gpu</p>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#ocr-optical-character-recognition\" id=\"user-content-ocr-optical-character-recognition\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/ocr\">OCR (optical character recognition)</a></h3>\n",
            "<details><summary>Complete list (2 notebooks)</summary>\n",
            "<ol>\n",
            "<li>CNN + LSTM RNN, test accuracy 100%</li>\n",
            "<li>Im2Latex, test accuracy 100%</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#pos-tagging\" id=\"user-content-pos-tagging\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/pos-tagging\">POS-Tagging</a></h3>\n",
            "<p>Trained on <a href=\"https://cogcomp.org/page/resource_view/81\" rel=\"nofollow\">CONLL POS</a>.</p>\n",
            "<details><summary>Complete list (8 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Bidirectional RNN + CRF, test accuracy 92%</li>\n",
            "<li>Bidirectional RNN + Luong Attention + CRF, test accuracy 91%</li>\n",
            "<li>Bidirectional RNN + Bahdanau Attention + CRF, test accuracy 91%</li>\n",
            "<li>Char Ngrams + Bidirectional RNN + Bahdanau Attention + CRF, test accuracy 91%</li>\n",
            "<li>Char Ngrams + Bidirectional RNN + Bahdanau Attention + CRF, test accuracy 91%</li>\n",
            "<li>Char Ngrams + Residual Network + Bahdanau Attention + CRF, test accuracy 3%</li>\n",
            "<li>Char Ngrams + Attention is you all Need + CRF, test accuracy 89%</li>\n",
            "<li>BERT, test accuracy 99%</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#question-answers\" id=\"user-content-question-answers\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/question-answer\">Question-Answers</a></h3>\n",
            "<p>Trained on <a href=\"https://research.fb.com/downloads/babi/\" rel=\"nofollow\">bAbI Dataset</a>.</p>\n",
            "<details><summary>Complete list (4 notebooks)</summary>\n",
            "<ol>\n",
            "<li>End-to-End Memory Network + Basic cell</li>\n",
            "<li>End-to-End Memory Network + GRU cell</li>\n",
            "<li>End-to-End Memory Network + LSTM cell</li>\n",
            "<li>Dynamic Memory</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#sentence-pair\" id=\"user-content-sentence-pair\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/sentence-pair\">Sentence-pair</a></h3>\n",
            "<p>Trained on <a href=\"https://people.mpi-sws.org/~cristian/Cornell_Movie-Dialogs_Corpus.html\" rel=\"nofollow\">Cornell Movie--Dialogs Corpus</a></p>\n",
            "<details><summary>Complete list (1 notebooks)</summary>\n",
            "<ol>\n",
            "<li>BERT</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#speech-to-text\" id=\"user-content-speech-to-text\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/speech-to-text\">Speech to Text</a></h3>\n",
            "<p>Trained on <a href=\"https://tspace.library.utoronto.ca/handle/1807/24487\" rel=\"nofollow\">Toronto speech dataset</a>.</p>\n",
            "<details><summary>Complete list (11 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Tacotron, <a href=\"https://github.com/Kyubyong/tacotron_asr\">https://github.com/Kyubyong/tacotron_asr</a>, test accuracy 77.09%</li>\n",
            "<li>BiRNN LSTM, test accuracy 84.66%</li>\n",
            "<li>BiRNN Seq2Seq + Luong Attention + Cross Entropy, test accuracy 87.86%</li>\n",
            "<li>BiRNN Seq2Seq + Bahdanau Attention + Cross Entropy, test accuracy 89.28%</li>\n",
            "<li>BiRNN Seq2Seq + Bahdanau Attention + CTC, test accuracy 86.35%</li>\n",
            "<li>BiRNN Seq2Seq + Luong Attention + CTC, test accuracy 80.30%</li>\n",
            "<li>CNN RNN + Bahdanau Attention, test accuracy 80.23%</li>\n",
            "<li>Dilated CNN RNN, test accuracy 31.60%</li>\n",
            "<li>Wavenet, test accuracy 75.11%</li>\n",
            "<li>Deep Speech 2, test accuracy 81.40%</li>\n",
            "<li>Wav2Vec Transfer learning BiRNN LSTM, test accuracy 83.24%</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#spelling-correction\" id=\"user-content-spelling-correction\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/spelling-correction\">Spelling correction</a></h3>\n",
            "<details><summary>Complete list (4 notebooks)</summary>\n",
            "<ol>\n",
            "<li>BERT-Base</li>\n",
            "<li>XLNET-Base</li>\n",
            "<li>BERT-Base Fast</li>\n",
            "<li>BERT-Base accurate</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#squad-question-answers\" id=\"user-content-squad-question-answers\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/squad-qa\">SQUAD Question-Answers</a></h3>\n",
            "<p>Trained on <a href=\"https://rajpurkar.github.io/SQuAD-explorer/\" rel=\"nofollow\">SQUAD Dataset</a>.</p>\n",
            "<details><summary>Complete list (1 notebooks)</summary>\n",
            "<ol>\n",
            "<li>BERT,</li>\n",
            "</ol>\n",
            "<div class=\"highlight highlight-source-json\"><pre>{<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>exact_match<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">77.57805108798486</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f1<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">86.18327335287402</span>}</pre></div>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#stemming\" id=\"user-content-stemming\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/stemming\">Stemming</a></h3>\n",
            "<p>Trained on <a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/stemming/lemmatization-en.txt\">English Lemmatization</a>.</p>\n",
            "<details><summary>Complete list (6 notebooks)</summary>\n",
            "<ol>\n",
            "<li>LSTM + Seq2Seq + Beam</li>\n",
            "<li>GRU + Seq2Seq + Beam</li>\n",
            "<li>LSTM + BiRNN + Seq2Seq + Beam</li>\n",
            "<li>GRU + BiRNN + Seq2Seq + Beam</li>\n",
            "<li>DNC + Seq2Seq + Greedy</li>\n",
            "<li>BiRNN + Bahdanau + Copynet</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#text-augmentation\" id=\"user-content-text-augmentation\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/text-augmentation\">Text Augmentation</a></h3>\n",
            "<details><summary>Complete list (8 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Pretrained Glove</li>\n",
            "<li>GRU VAE-seq2seq-beam TF-probability</li>\n",
            "<li>LSTM VAE-seq2seq-beam TF-probability</li>\n",
            "<li>GRU VAE-seq2seq-beam + Bahdanau Attention TF-probability</li>\n",
            "<li>VAE + Deterministic Bahdanau Attention, <a href=\"https://github.com/HareeshBahuleyan/tf-var-attention\">https://github.com/HareeshBahuleyan/tf-var-attention</a></li>\n",
            "<li>VAE + VAE Bahdanau Attention, <a href=\"https://github.com/HareeshBahuleyan/tf-var-attention\">https://github.com/HareeshBahuleyan/tf-var-attention</a></li>\n",
            "<li>BERT-Base + Nucleus Sampling</li>\n",
            "<li>XLNET-Base + Nucleus Sampling</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#text-classification\" id=\"user-content-text-classification\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/text-classification\">Text classification</a></h3>\n",
            "<p>Trained on <a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/text-classification/data\">English sentiment dataset</a>, accuracy table in <a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/text-classification\">text-classification</a>.</p>\n",
            "<details><summary>Complete list (79 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Basic cell RNN</li>\n",
            "<li>Basic cell RNN + Hinge</li>\n",
            "<li>Basic cell RNN + Huber</li>\n",
            "<li>Basic cell Bidirectional RNN</li>\n",
            "<li>Basic cell Bidirectional RNN + Hinge</li>\n",
            "<li>Basic cell Bidirectional RNN + Huber</li>\n",
            "<li>LSTM cell RNN</li>\n",
            "<li>LSTM cell RNN + Hinge</li>\n",
            "<li>LSTM cell RNN + Huber</li>\n",
            "<li>LSTM cell Bidirectional RNN</li>\n",
            "<li>LSTM cell Bidirectional RNN + Huber</li>\n",
            "<li>LSTM cell RNN + Dropout + L2</li>\n",
            "<li>GRU cell RNN</li>\n",
            "<li>GRU cell RNN + Hinge</li>\n",
            "<li>GRU cell RNN + Huber</li>\n",
            "<li>GRU cell Bidirectional RNN</li>\n",
            "<li>GRU cell Bidirectional RNN + Hinge</li>\n",
            "<li>GRU cell Bidirectional RNN + Huber</li>\n",
            "<li>LSTM RNN + Conv2D</li>\n",
            "<li>K-max Conv1d</li>\n",
            "<li>LSTM RNN + Conv1D + Highway</li>\n",
            "<li>LSTM RNN + Basic Attention</li>\n",
            "<li>LSTM Dilated RNN</li>\n",
            "<li>Layer-Norm LSTM cell RNN</li>\n",
            "<li>Only Attention Neural Network</li>\n",
            "<li>Multihead-Attention Neural Network</li>\n",
            "<li>Neural Turing Machine</li>\n",
            "<li>LSTM Seq2Seq</li>\n",
            "<li>LSTM Seq2Seq + Luong Attention</li>\n",
            "<li>LSTM Seq2Seq + Bahdanau Attention</li>\n",
            "<li>LSTM Seq2Seq + Beam Decoder</li>\n",
            "<li>LSTM Bidirectional Seq2Seq</li>\n",
            "<li>Pointer Net</li>\n",
            "<li>LSTM cell RNN + Bahdanau Attention</li>\n",
            "<li>LSTM cell RNN + Luong Attention</li>\n",
            "<li>LSTM cell RNN + Stack Bahdanau Luong Attention</li>\n",
            "<li>LSTM cell Bidirectional RNN + backward Bahdanau + forward Luong</li>\n",
            "<li>Bytenet</li>\n",
            "<li>Fast-slow LSTM</li>\n",
            "<li>Siamese Network</li>\n",
            "<li>LSTM Seq2Seq + tf.estimator</li>\n",
            "<li>Capsule layers + RNN LSTM</li>\n",
            "<li>Capsule layers + LSTM Seq2Seq</li>\n",
            "<li>Capsule layers + LSTM Bidirectional Seq2Seq</li>\n",
            "<li>Nested LSTM</li>\n",
            "<li>LSTM Seq2Seq + Highway</li>\n",
            "<li>Triplet loss + LSTM</li>\n",
            "<li>DNC (Differentiable Neural Computer)</li>\n",
            "<li>ConvLSTM</li>\n",
            "<li>Temporal Convd Net</li>\n",
            "<li>Batch-all Triplet-loss + LSTM</li>\n",
            "<li>Fast-text</li>\n",
            "<li>Gated Convolution Network</li>\n",
            "<li>Simple Recurrent Unit</li>\n",
            "<li>LSTM Hierarchical Attention Network</li>\n",
            "<li>Bidirectional Transformers</li>\n",
            "<li>Dynamic Memory Network</li>\n",
            "<li>Entity Network</li>\n",
            "<li>End-to-End Memory Network</li>\n",
            "<li>BOW-Chars Deep sparse Network</li>\n",
            "<li>Residual Network using Atrous CNN</li>\n",
            "<li>Residual Network using Atrous CNN + Bahdanau Attention</li>\n",
            "<li>Deep pyramid CNN</li>\n",
            "<li>Transformer-XL</li>\n",
            "<li>Transfer learning GPT-2 345M</li>\n",
            "<li>Quasi-RNN</li>\n",
            "<li>Tacotron</li>\n",
            "<li>Slice GRU</li>\n",
            "<li>Slice GRU + Bahdanau</li>\n",
            "<li>Wavenet</li>\n",
            "<li>Transfer learning BERT Base</li>\n",
            "<li>Transfer learning XL-net Large</li>\n",
            "<li>LSTM BiRNN global Max and average pooling</li>\n",
            "<li>Transfer learning BERT Base drop 6 layers</li>\n",
            "<li>Transfer learning BERT Large drop 12 layers</li>\n",
            "<li>Transfer learning XL-net Base</li>\n",
            "<li>Transfer learning ALBERT</li>\n",
            "<li>Transfer learning ELECTRA Base</li>\n",
            "<li>Transfer learning ELECTRA Large</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#text-similarity\" id=\"user-content-text-similarity\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/text-similarity\">Text Similarity</a></h3>\n",
            "<p>Trained on <a href=\"https://cims.nyu.edu/~sbowman/multinli/\" rel=\"nofollow\">MNLI</a>.</p>\n",
            "<details><summary>Complete list (10 notebooks)</summary>\n",
            "<ol>\n",
            "<li>BiRNN + Contrastive loss, test accuracy 73.032%</li>\n",
            "<li>BiRNN + Cross entropy, test accuracy 74.265%</li>\n",
            "<li>BiRNN + Circle loss, test accuracy 75.857%</li>\n",
            "<li>BiRNN + Proxy loss, test accuracy 48.37%</li>\n",
            "<li>BERT Base + Cross entropy, test accuracy 91.123%</li>\n",
            "<li>BERT Base + Circle loss, test accuracy 89.903%</li>\n",
            "<li>ELECTRA Base + Cross entropy, test accuracy 96.317%</li>\n",
            "<li>ELECTRA Base + Circle loss, test accuracy 95.603%</li>\n",
            "<li>XLNET Base + Cross entropy, test accuracy 93.998%</li>\n",
            "<li>XLNET Base + Circle loss, test accuracy 94.033%</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#text-to-speech\" id=\"user-content-text-to-speech\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/text-to-speech\">Text to Speech</a></h3>\n",
            "<p>Trained on <a href=\"https://tspace.library.utoronto.ca/handle/1807/24487\" rel=\"nofollow\">Toronto speech dataset</a>.</p>\n",
            "<details><summary>Complete list (8 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Tacotron, <a href=\"https://github.com/Kyubyong/tacotron\">https://github.com/Kyubyong/tacotron</a></li>\n",
            "<li>CNN Seq2seq + Dilated CNN vocoder</li>\n",
            "<li>Seq2Seq + Bahdanau Attention</li>\n",
            "<li>Seq2Seq + Luong Attention</li>\n",
            "<li>Dilated CNN + Monothonic Attention + Dilated CNN vocoder</li>\n",
            "<li>Dilated CNN + Self Attention + Dilated CNN vocoder</li>\n",
            "<li>Deep CNN + Monothonic Attention + Dilated CNN vocoder</li>\n",
            "<li>Deep CNN + Self Attention + Dilated CNN vocoder</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#topic-generator\" id=\"user-content-topic-generator\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/topic-generator\">Topic Generator</a></h3>\n",
            "<p>Trained on <a href=\"https://github.com/huseinzol05/Malaya-Dataset/raw/master/news/news.zip\">Malaysia news</a>.</p>\n",
            "<details><summary>Complete list (4 notebooks)</summary>\n",
            "<ol>\n",
            "<li>TAT-LSTM</li>\n",
            "<li>TAV-LSTM</li>\n",
            "<li>MTA-LSTM</li>\n",
            "<li>Dilated CNN Seq2seq</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#topic-modeling\" id=\"user-content-topic-modeling\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/topic-model\">Topic Modeling</a></h3>\n",
            "<p>Extracted from <a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/text-classification/data\">English sentiment dataset</a>.</p>\n",
            "<details><summary>Complete list (3 notebooks)</summary>\n",
            "<ol>\n",
            "<li>LDA2Vec</li>\n",
            "<li>BERT Attention</li>\n",
            "<li>XLNET Attention</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#unsupervised-extractive-summarization\" id=\"user-content-unsupervised-extractive-summarization\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/unsupervised-extractive-summarization\">Unsupervised Extractive Summarization</a></h3>\n",
            "<p>Trained on <a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/extractive-summarization/books\">random books</a>.</p>\n",
            "<details><summary>Complete list (3 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Skip-thought Vector</li>\n",
            "<li>Residual Network using Atrous CNN</li>\n",
            "<li>Residual Network using Atrous CNN + Bahdanau Attention</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#vectorizer\" id=\"user-content-vectorizer\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/vectorizer\">Vectorizer</a></h3>\n",
            "<p>Trained on <a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/text-classification/data\">English sentiment dataset</a>.</p>\n",
            "<details><summary>Complete list (11 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Word Vector using CBOW sample softmax</li>\n",
            "<li>Word Vector using CBOW noise contrastive estimation</li>\n",
            "<li>Word Vector using skipgram sample softmax</li>\n",
            "<li>Word Vector using skipgram noise contrastive estimation</li>\n",
            "<li>Supervised Embedded</li>\n",
            "<li>Triplet-loss + LSTM</li>\n",
            "<li>LSTM Auto-Encoder</li>\n",
            "<li>Batch-All Triplet-loss LSTM</li>\n",
            "<li>Fast-text</li>\n",
            "<li>ELMO (biLM)</li>\n",
            "<li>Triplet-loss + BERT</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#visualization\" id=\"user-content-visualization\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/visualization\">Visualization</a></h3>\n",
            "<details><summary>Complete list (4 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Attention heatmap on Bahdanau Attention</li>\n",
            "<li>Attention heatmap on Luong Attention</li>\n",
            "<li>BERT attention, <a href=\"https://github.com/hsm207/bert_attn_viz\">https://github.com/hsm207/bert_attn_viz</a></li>\n",
            "<li>XLNET attention</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#old-to-young-vocoder\" id=\"user-content-old-to-young-vocoder\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/vocoder\">Old-to-Young Vocoder</a></h3>\n",
            "<p>Trained on <a href=\"https://tspace.library.utoronto.ca/handle/1807/24487\" rel=\"nofollow\">Toronto speech dataset</a>.</p>\n",
            "<details><summary>Complete list (1 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Dilated CNN</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#attention\" id=\"user-content-attention\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/attention\">Attention</a></h3>\n",
            "<details><summary>Complete list (8 notebooks)</summary>\n",
            "<ol>\n",
            "<li>Bahdanau</li>\n",
            "<li>Luong</li>\n",
            "<li>Hierarchical</li>\n",
            "<li>Additive</li>\n",
            "<li>Soft</li>\n",
            "<li>Attention-over-Attention</li>\n",
            "<li>Bahdanau API</li>\n",
            "<li>Luong API</li>\n",
            "</ol>\n",
            "</details>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#not-deep-learning\" id=\"user-content-not-deep-learning\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><a href=\"/huseinzol05/NLP-Models-Tensorflow/blob/master/not-deep-learning\">Not-deep-learning</a></h3>\n",
            "<ol>\n",
            "<li>Markov chatbot</li>\n",
            "<li>Decomposition summarization (3 notebooks)</li>\n",
            "</ol>\n",
            "</article>\n",
            "<article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><h1><a aria-hidden=\"true\" class=\"anchor\" href=\"#tensorflow-for-machine-intelligence\" id=\"user-content-tensorflow-for-machine-intelligence\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a><em>TensorFlow for Machine Intelligence</em></h1>\n",
            "<p><a href=\"/backstopmedia/tensorflowbook/blob/master/img/book_cover.jpg\" rel=\"noopener noreferrer\" target=\"_blank\"><img alt=\"TensorFlow for Machine Intelligence book cover\" src=\"/backstopmedia/tensorflowbook/raw/master/img/book_cover.jpg\" style=\"max-width:100%;\"/></a></p>\n",
            "<p>Welcome to the official book repository for <a href=\"https://bleedingedgepress.com/tensor-flow-for-machine-intelligence/\" rel=\"nofollow\"><em>TensorFlow for Machine Intelligence</em></a>! Here, you'll find code from the book for easy testing on your own machine, as well as errata, and any additional content we can squeeze in down the line.</p>\n",
            "<ul>\n",
            "<li><strong>Code:</strong> You'll find code for each chapter inside of the <a href=\"https://github.com/backstopmedia/tensorflowbook/tree/master/chapters\">chapters directory</a></li>\n",
            "<li><strong>Errata:</strong> Errata will be added to the <a href=\"https://github.com/backstopmedia/tensorflowbook/tree/master/errata\">errata</a> directory as they are discovered. Send in a pull request if you have errata to report!</li>\n",
            "</ul>\n",
            "</article>\n",
            "<article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><h1><a aria-hidden=\"true\" class=\"anchor\" href=\"#bert\" id=\"user-content-bert\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>BERT</h1>\n",
            "<p><strong>***** New March 11th, 2020: Smaller BERT Models *****</strong></p>\n",
            "<p>This is a release of 24 smaller BERT models (English only, uncased, trained with WordPiece masking) referenced in <a href=\"https://arxiv.org/abs/1908.08962\" rel=\"nofollow\">Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</a>.</p>\n",
            "<p>We have shown that the standard BERT recipe (including model architecture and training objective) is effective on a wide range of model sizes, beyond BERT-Base and BERT-Large. The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher.</p>\n",
            "<p>Our goal is to enable research in institutions with fewer computational resources and encourage the community to seek directions of innovation alternative to increasing model capacity.</p>\n",
            "<p>You can download all 24 from <a href=\"https://storage.googleapis.com/bert_models/2020_02_20/all_bert_models.zip\" rel=\"nofollow\">here</a>, or individually from the table below:</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th></th>\n",
            "<th align=\"center\">H=128</th>\n",
            "<th align=\"center\">H=256</th>\n",
            "<th align=\"center\">H=512</th>\n",
            "<th align=\"center\">H=768</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><strong>L=2</strong></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-128_A-2.zip\" rel=\"nofollow\"><strong>2/128 (BERT-Tiny)</strong></a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-256_A-4.zip\" rel=\"nofollow\">2/256</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-512_A-8.zip\" rel=\"nofollow\">2/512</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-768_A-12.zip\" rel=\"nofollow\">2/768</a></td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><strong>L=4</strong></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-4_H-128_A-2.zip\" rel=\"nofollow\">4/128</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-4_H-256_A-4.zip\" rel=\"nofollow\"><strong>4/256 (BERT-Mini)</strong></a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-4_H-512_A-8.zip\" rel=\"nofollow\"><strong>4/512 (BERT-Small)</strong></a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-4_H-768_A-12.zip\" rel=\"nofollow\">4/768</a></td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><strong>L=6</strong></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-6_H-128_A-2.zip\" rel=\"nofollow\">6/128</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-6_H-256_A-4.zip\" rel=\"nofollow\">6/256</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-6_H-512_A-8.zip\" rel=\"nofollow\">6/512</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-6_H-768_A-12.zip\" rel=\"nofollow\">6/768</a></td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><strong>L=8</strong></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-8_H-128_A-2.zip\" rel=\"nofollow\">8/128</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-8_H-256_A-4.zip\" rel=\"nofollow\">8/256</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-8_H-512_A-8.zip\" rel=\"nofollow\"><strong>8/512 (BERT-Medium)</strong></a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-8_H-768_A-12.zip\" rel=\"nofollow\">8/768</a></td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><strong>L=10</strong></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-10_H-128_A-2.zip\" rel=\"nofollow\">10/128</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-10_H-256_A-4.zip\" rel=\"nofollow\">10/256</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-10_H-512_A-8.zip\" rel=\"nofollow\">10/512</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-10_H-768_A-12.zip\" rel=\"nofollow\">10/768</a></td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><strong>L=12</strong></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-128_A-2.zip\" rel=\"nofollow\">12/128</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-256_A-4.zip\" rel=\"nofollow\">12/256</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-512_A-8.zip\" rel=\"nofollow\">12/512</a></td>\n",
            "<td align=\"center\"><a href=\"https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip\" rel=\"nofollow\"><strong>12/768 (BERT-Base)</strong></a></td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<p>Note that the BERT-Base model in this release is included for completeness only; it was re-trained under the same regime as the original model.</p>\n",
            "<p>Here are the corresponding GLUE scores on the test set:</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Model</th>\n",
            "<th align=\"center\">Score</th>\n",
            "<th align=\"center\">CoLA</th>\n",
            "<th align=\"center\">SST-2</th>\n",
            "<th align=\"center\">MRPC</th>\n",
            "<th align=\"center\">STS-B</th>\n",
            "<th align=\"center\">QQP</th>\n",
            "<th align=\"center\">MNLI-m</th>\n",
            "<th align=\"center\">MNLI-mm</th>\n",
            "<th align=\"center\">QNLI(v2)</th>\n",
            "<th align=\"center\">RTE</th>\n",
            "<th align=\"center\">WNLI</th>\n",
            "<th align=\"center\">AX</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td>BERT-Tiny</td>\n",
            "<td align=\"center\">64.2</td>\n",
            "<td align=\"center\">0.0</td>\n",
            "<td align=\"center\">83.2</td>\n",
            "<td align=\"center\">81.1/71.1</td>\n",
            "<td align=\"center\">74.3/73.6</td>\n",
            "<td align=\"center\">62.2/83.4</td>\n",
            "<td align=\"center\">70.2</td>\n",
            "<td align=\"center\">70.3</td>\n",
            "<td align=\"center\">81.5</td>\n",
            "<td align=\"center\">57.2</td>\n",
            "<td align=\"center\">62.3</td>\n",
            "<td align=\"center\">21.0</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>BERT-Mini</td>\n",
            "<td align=\"center\">65.8</td>\n",
            "<td align=\"center\">0.0</td>\n",
            "<td align=\"center\">85.9</td>\n",
            "<td align=\"center\">81.1/71.8</td>\n",
            "<td align=\"center\">75.4/73.3</td>\n",
            "<td align=\"center\">66.4/86.2</td>\n",
            "<td align=\"center\">74.8</td>\n",
            "<td align=\"center\">74.3</td>\n",
            "<td align=\"center\">84.1</td>\n",
            "<td align=\"center\">57.9</td>\n",
            "<td align=\"center\">62.3</td>\n",
            "<td align=\"center\">26.1</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>BERT-Small</td>\n",
            "<td align=\"center\">71.2</td>\n",
            "<td align=\"center\">27.8</td>\n",
            "<td align=\"center\">89.7</td>\n",
            "<td align=\"center\">83.4/76.2</td>\n",
            "<td align=\"center\">78.8/77.0</td>\n",
            "<td align=\"center\">68.1/87.0</td>\n",
            "<td align=\"center\">77.6</td>\n",
            "<td align=\"center\">77.0</td>\n",
            "<td align=\"center\">86.4</td>\n",
            "<td align=\"center\">61.8</td>\n",
            "<td align=\"center\">62.3</td>\n",
            "<td align=\"center\">28.6</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>BERT-Medium</td>\n",
            "<td align=\"center\">73.5</td>\n",
            "<td align=\"center\">38.0</td>\n",
            "<td align=\"center\">89.6</td>\n",
            "<td align=\"center\">86.6/81.6</td>\n",
            "<td align=\"center\">80.4/78.4</td>\n",
            "<td align=\"center\">69.6/87.9</td>\n",
            "<td align=\"center\">80.0</td>\n",
            "<td align=\"center\">79.1</td>\n",
            "<td align=\"center\">87.7</td>\n",
            "<td align=\"center\">62.2</td>\n",
            "<td align=\"center\">62.3</td>\n",
            "<td align=\"center\">30.5</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<p>For each task, we selected the best fine-tuning hyperparameters from the lists below, and trained for 4 epochs:</p>\n",
            "<ul>\n",
            "<li>batch sizes: 8, 16, 32, 64, 128</li>\n",
            "<li>learning rates: 3e-4, 1e-4, 5e-5, 3e-5</li>\n",
            "</ul>\n",
            "<p>If you use these models, please cite the following paper:</p>\n",
            "<pre><code>@article{turc2019,\n",
            "  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},\n",
            "  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n",
            "  journal={arXiv preprint arXiv:1908.08962v2 },\n",
            "  year={2019}\n",
            "}\n",
            "</code></pre>\n",
            "<p><strong>***** New May 31st, 2019: Whole Word Masking Models *****</strong></p>\n",
            "<p>This is a release of several new models which were the result of an improvement\n",
            "the pre-processing code.</p>\n",
            "<p>In the original pre-processing code, we randomly select WordPiece tokens to\n",
            "mask. For example:</p>\n",
            "<p><code>Input Text: the man jumped up , put his basket on phil ##am ##mon ' s head</code>\n",
            "<code>Original Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head</code></p>\n",
            "<p>The new technique is called Whole Word Masking. In this case, we always mask\n",
            "<em>all</em> of the the tokens corresponding to a word at once. The overall masking\n",
            "rate remains the same.</p>\n",
            "<p><code>Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head</code></p>\n",
            "<p>The training is identical -- we still predict each masked WordPiece token\n",
            "independently. The improvement comes from the fact that the original prediction\n",
            "task was too 'easy' for words that had been split into multiple WordPieces.</p>\n",
            "<p>This can be enabled during data generation by passing the flag\n",
            "<code>--do_whole_word_mask=True</code> to <code>create_pretraining_data.py</code>.</p>\n",
            "<p>Pre-trained models with Whole Word Masking are linked below. The data and\n",
            "training were otherwise identical, and the models have identical structure and\n",
            "vocab to the original models. We only include BERT-Large models. When using\n",
            "these models, please make it clear in the paper that you are using the Whole\n",
            "Word Masking variant of BERT-Large.</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p><strong><a href=\"https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\" rel=\"nofollow\"><code>BERT-Large, Uncased (Whole Word Masking)</code></a></strong>:\n",
            "24-layer, 1024-hidden, 16-heads, 340M parameters</p>\n",
            "</li>\n",
            "<li>\n",
            "<p><strong><a href=\"https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip\" rel=\"nofollow\"><code>BERT-Large, Cased (Whole Word Masking)</code></a></strong>:\n",
            "24-layer, 1024-hidden, 16-heads, 340M parameters</p>\n",
            "</li>\n",
            "</ul>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>Model</th>\n",
            "<th align=\"center\">SQUAD 1.1 F1/EM</th>\n",
            "<th align=\"center\">Multi NLI Accuracy</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td>BERT-Large, Uncased (Original)</td>\n",
            "<td align=\"center\">91.0/84.3</td>\n",
            "<td align=\"center\">86.05</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>BERT-Large, Uncased (Whole Word Masking)</td>\n",
            "<td align=\"center\">92.8/86.7</td>\n",
            "<td align=\"center\">87.07</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>BERT-Large, Cased (Original)</td>\n",
            "<td align=\"center\">91.5/84.8</td>\n",
            "<td align=\"center\">86.09</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>BERT-Large, Cased (Whole Word Masking)</td>\n",
            "<td align=\"center\">92.9/86.7</td>\n",
            "<td align=\"center\">86.46</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<p><strong>***** New February 7th, 2019: TfHub Module *****</strong></p>\n",
            "<p>BERT has been uploaded to <a href=\"https://tfhub.dev\" rel=\"nofollow\">TensorFlow Hub</a>. See\n",
            "<code>run_classifier_with_tfhub.py</code> for an example of how to use the TF Hub module,\n",
            "or run an example in the browser on\n",
            "<a href=\"https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\" rel=\"nofollow\">Colab</a>.</p>\n",
            "<p><strong>***** New November 23rd, 2018: Un-normalized multilingual model + Thai +\n",
            "Mongolian *****</strong></p>\n",
            "<p>We uploaded a new multilingual model which does <em>not</em> perform any normalization\n",
            "on the input (no lower casing, accent stripping, or Unicode normalization), and\n",
            "additionally inclues Thai and Mongolian.</p>\n",
            "<p><strong>It is recommended to use this version for developing multilingual models,\n",
            "especially on languages with non-Latin alphabets.</strong></p>\n",
            "<p>This does not require any code changes, and can be downloaded here:</p>\n",
            "<ul>\n",
            "<li><strong><a href=\"https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\" rel=\"nofollow\"><code>BERT-Base, Multilingual Cased</code></a></strong>:\n",
            "104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters</li>\n",
            "</ul>\n",
            "<p><strong>***** New November 15th, 2018: SOTA SQuAD 2.0 System *****</strong></p>\n",
            "<p>We released code changes to reproduce our 83% F1 SQuAD 2.0 system, which is\n",
            "currently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of the\n",
            "README for details.</p>\n",
            "<p><strong>***** New November 5th, 2018: Third-party PyTorch and Chainer versions of\n",
            "BERT available *****</strong></p>\n",
            "<p>NLP researchers from HuggingFace made a\n",
            "<a href=\"https://github.com/huggingface/pytorch-pretrained-BERT\">PyTorch version of BERT available</a>\n",
            "which is compatible with our pre-trained checkpoints and is able to reproduce\n",
            "our results. Sosuke Kobayashi also made a\n",
            "<a href=\"https://github.com/soskek/bert-chainer\">Chainer version of BERT available</a>\n",
            "(Thanks!) We were not involved in the creation or maintenance of the PyTorch\n",
            "implementation so please direct any questions towards the authors of that\n",
            "repository.</p>\n",
            "<p><strong>***** New November 3rd, 2018: Multilingual and Chinese models available\n",
            "*****</strong></p>\n",
            "<p>We have made two new BERT models available:</p>\n",
            "<ul>\n",
            "<li><strong><a href=\"https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip\" rel=\"nofollow\"><code>BERT-Base, Multilingual</code></a>\n",
            "(Not recommended, use <code>Multilingual Cased</code> instead)</strong>: 102 languages,\n",
            "12-layer, 768-hidden, 12-heads, 110M parameters</li>\n",
            "<li><strong><a href=\"https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\" rel=\"nofollow\"><code>BERT-Base, Chinese</code></a></strong>:\n",
            "Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M\n",
            "parameters</li>\n",
            "</ul>\n",
            "<p>We use character-based tokenization for Chinese, and WordPiece tokenization for\n",
            "all other languages. Both models should work out-of-the-box without any code\n",
            "changes. We did update the implementation of <code>BasicTokenizer</code> in\n",
            "<code>tokenization.py</code> to support Chinese character tokenization, so please update if\n",
            "you forked it. However, we did not change the tokenization API.</p>\n",
            "<p>For more, see the\n",
            "<a href=\"https://github.com/google-research/bert/blob/master/multilingual.md\">Multilingual README</a>.</p>\n",
            "<p><strong>***** End new information *****</strong></p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#introduction\" id=\"user-content-introduction\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Introduction</h2>\n",
            "<p><strong>BERT</strong>, or <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from\n",
            "<strong>T</strong>ransformers, is a new method of pre-training language representations which\n",
            "obtains state-of-the-art results on a wide array of Natural Language Processing\n",
            "(NLP) tasks.</p>\n",
            "<p>Our academic paper which describes BERT in detail and provides full results on a\n",
            "number of tasks can be found here:\n",
            "<a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">https://arxiv.org/abs/1810.04805</a>.</p>\n",
            "<p>To give a few numbers, here are the results on the\n",
            "<a href=\"https://rajpurkar.github.io/SQuAD-explorer/\" rel=\"nofollow\">SQuAD v1.1</a> question answering\n",
            "task:</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>SQuAD v1.1 Leaderboard (Oct 8th 2018)</th>\n",
            "<th align=\"center\">Test EM</th>\n",
            "<th align=\"center\">Test F1</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td>1st Place Ensemble - BERT</td>\n",
            "<td align=\"center\"><strong>87.4</strong></td>\n",
            "<td align=\"center\"><strong>93.2</strong></td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>2nd Place Ensemble - nlnet</td>\n",
            "<td align=\"center\">86.0</td>\n",
            "<td align=\"center\">91.7</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>1st Place Single Model - BERT</td>\n",
            "<td align=\"center\"><strong>85.1</strong></td>\n",
            "<td align=\"center\"><strong>91.8</strong></td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>2nd Place Single Model - nlnet</td>\n",
            "<td align=\"center\">83.5</td>\n",
            "<td align=\"center\">90.1</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<p>And several natural language inference tasks:</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>System</th>\n",
            "<th align=\"center\">MultiNLI</th>\n",
            "<th align=\"center\">Question NLI</th>\n",
            "<th align=\"center\">SWAG</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td>BERT</td>\n",
            "<td align=\"center\"><strong>86.7</strong></td>\n",
            "<td align=\"center\"><strong>91.1</strong></td>\n",
            "<td align=\"center\"><strong>86.3</strong></td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>OpenAI GPT (Prev. SOTA)</td>\n",
            "<td align=\"center\">82.2</td>\n",
            "<td align=\"center\">88.1</td>\n",
            "<td align=\"center\">75.0</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<p>Plus many other tasks.</p>\n",
            "<p>Moreover, these results were all obtained with almost no task-specific neural\n",
            "network architecture design.</p>\n",
            "<p>If you already know what BERT is and you just want to get started, you can\n",
            "<a href=\"#pre-trained-models\">download the pre-trained models</a> and\n",
            "<a href=\"#fine-tuning-with-bert\">run a state-of-the-art fine-tuning</a> in only a few\n",
            "minutes.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#what-is-bert\" id=\"user-content-what-is-bert\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>What is BERT?</h2>\n",
            "<p>BERT is a method of pre-training language representations, meaning that we train\n",
            "a general-purpose \"language understanding\" model on a large text corpus (like\n",
            "Wikipedia), and then use that model for downstream NLP tasks that we care about\n",
            "(like question answering). BERT outperforms previous methods because it is the\n",
            "first <em>unsupervised</em>, <em>deeply bidirectional</em> system for pre-training NLP.</p>\n",
            "<p><em>Unsupervised</em> means that BERT was trained using only a plain text corpus, which\n",
            "is important because an enormous amount of plain text data is publicly available\n",
            "on the web in many languages.</p>\n",
            "<p>Pre-trained representations can also either be <em>context-free</em> or <em>contextual</em>,\n",
            "and contextual representations can further be <em>unidirectional</em> or\n",
            "<em>bidirectional</em>. Context-free models such as\n",
            "<a href=\"https://www.tensorflow.org/tutorials/representation/word2vec\" rel=\"nofollow\">word2vec</a> or\n",
            "<a href=\"https://nlp.stanford.edu/projects/glove/\" rel=\"nofollow\">GloVe</a> generate a single \"word\n",
            "embedding\" representation for each word in the vocabulary, so <code>bank</code> would have\n",
            "the same representation in <code>bank deposit</code> and <code>river bank</code>. Contextual models\n",
            "instead generate a representation of each word that is based on the other words\n",
            "in the sentence.</p>\n",
            "<p>BERT was built upon recent work in pre-training contextual representations —\n",
            "including <a href=\"https://arxiv.org/abs/1511.01432\" rel=\"nofollow\">Semi-supervised Sequence Learning</a>,\n",
            "<a href=\"https://blog.openai.com/language-unsupervised/\" rel=\"nofollow\">Generative Pre-Training</a>,\n",
            "<a href=\"https://allennlp.org/elmo\" rel=\"nofollow\">ELMo</a>, and\n",
            "<a href=\"http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html\" rel=\"nofollow\">ULMFit</a>\n",
            "— but crucially these models are all <em>unidirectional</em> or <em>shallowly\n",
            "bidirectional</em>. This means that each word is only contextualized using the words\n",
            "to its left (or right). For example, in the sentence <code>I made a bank deposit</code> the\n",
            "unidirectional representation of <code>bank</code> is only based on <code>I made a</code> but not\n",
            "<code>deposit</code>. Some previous work does combine the representations from separate\n",
            "left-context and right-context models, but only in a \"shallow\" manner. BERT\n",
            "represents \"bank\" using both its left and right context — <code>I made a ... deposit</code>\n",
            "— starting from the very bottom of a deep neural network, so it is <em>deeply\n",
            "bidirectional</em>.</p>\n",
            "<p>BERT uses a simple approach for this: We mask out 15% of the words in the input,\n",
            "run the entire sequence through a deep bidirectional\n",
            "<a href=\"https://arxiv.org/abs/1706.03762\" rel=\"nofollow\">Transformer</a> encoder, and then predict only\n",
            "the masked words. For example:</p>\n",
            "<pre><code>Input: the man went to the [MASK1] . he bought a [MASK2] of milk.\n",
            "Labels: [MASK1] = store; [MASK2] = gallon\n",
            "</code></pre>\n",
            "<p>In order to learn relationships between sentences, we also train on a simple\n",
            "task which can be generated from any monolingual corpus: Given two sentences <code>A</code>\n",
            "and <code>B</code>, is <code>B</code> the actual next sentence that comes after <code>A</code>, or just a random\n",
            "sentence from the corpus?</p>\n",
            "<pre><code>Sentence A: the man went to the store .\n",
            "Sentence B: he bought a gallon of milk .\n",
            "Label: IsNextSentence\n",
            "</code></pre>\n",
            "<pre><code>Sentence A: the man went to the store .\n",
            "Sentence B: penguins are flightless .\n",
            "Label: NotNextSentence\n",
            "</code></pre>\n",
            "<p>We then train a large model (12-layer to 24-layer Transformer) on a large corpus\n",
            "(Wikipedia + <a href=\"http://yknzhu.wixsite.com/mbweb\" rel=\"nofollow\">BookCorpus</a>) for a long time (1M\n",
            "update steps), and that's BERT.</p>\n",
            "<p>Using BERT has two stages: <em>Pre-training</em> and <em>fine-tuning</em>.</p>\n",
            "<p><strong>Pre-training</strong> is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a\n",
            "one-time procedure for each language (current models are English-only, but\n",
            "multilingual models will be released in the near future). We are releasing a\n",
            "number of pre-trained models from the paper which were pre-trained at Google.\n",
            "Most NLP researchers will never need to pre-train their own model from scratch.</p>\n",
            "<p><strong>Fine-tuning</strong> is inexpensive. All of the results in the paper can be\n",
            "replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU,\n",
            "starting from the exact same pre-trained model. SQuAD, for example, can be\n",
            "trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of\n",
            "91.0%, which is the single system state-of-the-art.</p>\n",
            "<p>The other important aspect of BERT is that it can be adapted to many types of\n",
            "NLP tasks very easily. In the paper, we demonstrate state-of-the-art results on\n",
            "sentence-level (e.g., SST-2), sentence-pair-level (e.g., MultiNLI), word-level\n",
            "(e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specific\n",
            "modifications.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#what-has-been-released-in-this-repository\" id=\"user-content-what-has-been-released-in-this-repository\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>What has been released in this repository?</h2>\n",
            "<p>We are releasing the following:</p>\n",
            "<ul>\n",
            "<li>TensorFlow code for the BERT model architecture (which is mostly a standard\n",
            "<a href=\"https://arxiv.org/abs/1706.03762\" rel=\"nofollow\">Transformer</a> architecture).</li>\n",
            "<li>Pre-trained checkpoints for both the lowercase and cased version of\n",
            "<code>BERT-Base</code> and <code>BERT-Large</code> from the paper.</li>\n",
            "<li>TensorFlow code for push-button replication of the most important\n",
            "fine-tuning experiments from the paper, including SQuAD, MultiNLI, and MRPC.</li>\n",
            "</ul>\n",
            "<p>All of the code in this repository works out-of-the-box with CPU, GPU, and Cloud\n",
            "TPU.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#pre-trained-models\" id=\"user-content-pre-trained-models\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Pre-trained models</h2>\n",
            "<p>We are releasing the <code>BERT-Base</code> and <code>BERT-Large</code> models from the paper.\n",
            "<code>Uncased</code> means that the text has been lowercased before WordPiece tokenization,\n",
            "e.g., <code>John Smith</code> becomes <code>john smith</code>. The <code>Uncased</code> model also strips out any\n",
            "accent markers. <code>Cased</code> means that the true case and accent markers are\n",
            "preserved. Typically, the <code>Uncased</code> model is better unless you know that case\n",
            "information is important for your task (e.g., Named Entity Recognition or\n",
            "Part-of-Speech tagging).</p>\n",
            "<p>These models are all released under the same license as the source code (Apache\n",
            "2.0).</p>\n",
            "<p>For information about the Multilingual and Chinese model, see the\n",
            "<a href=\"https://github.com/google-research/bert/blob/master/multilingual.md\">Multilingual README</a>.</p>\n",
            "<p><strong>When using a cased model, make sure to pass <code>--do_lower=False</code> to the training\n",
            "scripts. (Or pass <code>do_lower_case=False</code> directly to <code>FullTokenizer</code> if you're\n",
            "using your own script.)</strong></p>\n",
            "<p>The links to the models are here (right-click, 'Save link as...' on the name):</p>\n",
            "<ul>\n",
            "<li><strong><a href=\"https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\" rel=\"nofollow\"><code>BERT-Large, Uncased (Whole Word Masking)</code></a></strong>:\n",
            "24-layer, 1024-hidden, 16-heads, 340M parameters</li>\n",
            "<li><strong><a href=\"https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip\" rel=\"nofollow\"><code>BERT-Large, Cased (Whole Word Masking)</code></a></strong>:\n",
            "24-layer, 1024-hidden, 16-heads, 340M parameters</li>\n",
            "<li><strong><a href=\"https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\" rel=\"nofollow\"><code>BERT-Base, Uncased</code></a></strong>:\n",
            "12-layer, 768-hidden, 12-heads, 110M parameters</li>\n",
            "<li><strong><a href=\"https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\" rel=\"nofollow\"><code>BERT-Large, Uncased</code></a></strong>:\n",
            "24-layer, 1024-hidden, 16-heads, 340M parameters</li>\n",
            "<li><strong><a href=\"https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\" rel=\"nofollow\"><code>BERT-Base, Cased</code></a></strong>:\n",
            "12-layer, 768-hidden, 12-heads , 110M parameters</li>\n",
            "<li><strong><a href=\"https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip\" rel=\"nofollow\"><code>BERT-Large, Cased</code></a></strong>:\n",
            "24-layer, 1024-hidden, 16-heads, 340M parameters</li>\n",
            "<li><strong><a href=\"https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\" rel=\"nofollow\"><code>BERT-Base, Multilingual Cased (New, recommended)</code></a></strong>:\n",
            "104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters</li>\n",
            "<li><strong><a href=\"https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip\" rel=\"nofollow\"><code>BERT-Base, Multilingual Uncased (Orig, not recommended)</code></a>\n",
            "(Not recommended, use <code>Multilingual Cased</code> instead)</strong>: 102 languages,\n",
            "12-layer, 768-hidden, 12-heads, 110M parameters</li>\n",
            "<li><strong><a href=\"https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\" rel=\"nofollow\"><code>BERT-Base, Chinese</code></a></strong>:\n",
            "Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M\n",
            "parameters</li>\n",
            "</ul>\n",
            "<p>Each .zip file contains three items:</p>\n",
            "<ul>\n",
            "<li>A TensorFlow checkpoint (<code>bert_model.ckpt</code>) containing the pre-trained\n",
            "weights (which is actually 3 files).</li>\n",
            "<li>A vocab file (<code>vocab.txt</code>) to map WordPiece to word id.</li>\n",
            "<li>A config file (<code>bert_config.json</code>) which specifies the hyperparameters of\n",
            "the model.</li>\n",
            "</ul>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#fine-tuning-with-bert\" id=\"user-content-fine-tuning-with-bert\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Fine-tuning with BERT</h2>\n",
            "<p><strong>Important</strong>: All results on the paper were fine-tuned on a single Cloud TPU,\n",
            "which has 64GB of RAM. It is currently not possible to re-produce most of the\n",
            "<code>BERT-Large</code> results on the paper using a GPU with 12GB - 16GB of RAM, because\n",
            "the maximum batch size that can fit in memory is too small. We are working on\n",
            "adding code to this repository which allows for much larger effective batch size\n",
            "on the GPU. See the section on <a href=\"#out-of-memory-issues\">out-of-memory issues</a> for\n",
            "more details.</p>\n",
            "<p>This code was tested with TensorFlow 1.11.0. It was tested with Python2 and\n",
            "Python3 (but more thoroughly with Python2, since this is what's used internally\n",
            "in Google).</p>\n",
            "<p>The fine-tuning examples which use <code>BERT-Base</code> should be able to run on a GPU\n",
            "that has at least 12GB of RAM using the hyperparameters given.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#fine-tuning-with-cloud-tpus\" id=\"user-content-fine-tuning-with-cloud-tpus\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Fine-tuning with Cloud TPUs</h3>\n",
            "<p>Most of the examples below assumes that you will be running training/evaluation\n",
            "on your local machine, using a GPU like a Titan X or GTX 1080.</p>\n",
            "<p>However, if you have access to a Cloud TPU that you want to train on, just add\n",
            "the following flags to <code>run_classifier.py</code> or <code>run_squad.py</code>:</p>\n",
            "<pre><code>  --use_tpu=True \\\n",
            "  --tpu_name=$TPU_NAME\n",
            "</code></pre>\n",
            "<p>Please see the\n",
            "<a href=\"https://cloud.google.com/tpu/docs/tutorials/mnist\" rel=\"nofollow\">Google Cloud TPU tutorial</a>\n",
            "for how to use Cloud TPUs. Alternatively, you can use the Google Colab notebook\n",
            "\"<a href=\"https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb\" rel=\"nofollow\">BERT FineTuning with Cloud TPUs</a>\".</p>\n",
            "<p>On Cloud TPUs, the pretrained model and the output directory will need to be on\n",
            "Google Cloud Storage. For example, if you have a bucket named <code>some_bucket</code>, you\n",
            "might use the following flags instead:</p>\n",
            "<pre><code>  --output_dir=gs://some_bucket/my_output_dir/\n",
            "</code></pre>\n",
            "<p>The unzipped pre-trained model files can also be found in the Google Cloud\n",
            "Storage folder <code>gs://bert_models/2018_10_18</code>. For example:</p>\n",
            "<pre><code>export BERT_BASE_DIR=gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12\n",
            "</code></pre>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#sentence-and-sentence-pair-classification-tasks\" id=\"user-content-sentence-and-sentence-pair-classification-tasks\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Sentence (and sentence-pair) classification tasks</h3>\n",
            "<p>Before running this example you must download the\n",
            "<a href=\"https://gluebenchmark.com/tasks\" rel=\"nofollow\">GLUE data</a> by running\n",
            "<a href=\"https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e\">this script</a>\n",
            "and unpack it to some directory <code>$GLUE_DIR</code>. Next, download the <code>BERT-Base</code>\n",
            "checkpoint and unzip it to some directory <code>$BERT_BASE_DIR</code>.</p>\n",
            "<p>This example code fine-tunes <code>BERT-Base</code> on the Microsoft Research Paraphrase\n",
            "Corpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in a\n",
            "few minutes on most GPUs.</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12\n",
            "<span class=\"pl-k\">export</span> GLUE_DIR=/path/to/glue\n",
            "\n",
            "python run_classifier.py \\\n",
            "  --task_name=MRPC \\\n",
            "  --do_train=true \\\n",
            "  --do_eval=true \\\n",
            "  --data_dir=<span class=\"pl-smi\">$GLUE_DIR</span>/MRPC \\\n",
            "  --vocab_file=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/vocab.txt \\\n",
            "  --bert_config_file=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/bert_config.json \\\n",
            "  --init_checkpoint=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/bert_model.ckpt \\\n",
            "  --max_seq_length=128 \\\n",
            "  --train_batch_size=32 \\\n",
            "  --learning_rate=2e-5 \\\n",
            "  --num_train_epochs=3.0 \\\n",
            "  --output_dir=/tmp/mrpc_output/</pre></div>\n",
            "<p>You should see output like this:</p>\n",
            "<pre><code>***** Eval results *****\n",
            "  eval_accuracy = 0.845588\n",
            "  eval_loss = 0.505248\n",
            "  global_step = 343\n",
            "  loss = 0.505248\n",
            "</code></pre>\n",
            "<p>This means that the Dev set accuracy was 84.55%. Small sets like MRPC have a\n",
            "high variance in the Dev set accuracy, even when starting from the same\n",
            "pre-training checkpoint. If you re-run multiple times (making sure to point to\n",
            "different <code>output_dir</code>), you should see results between 84% and 88%.</p>\n",
            "<p>A few other pre-trained models are implemented off-the-shelf in\n",
            "<code>run_classifier.py</code>, so it should be straightforward to follow those examples to\n",
            "use BERT for any single-sentence or sentence-pair classification task.</p>\n",
            "<p>Note: You might see a message <code>Running train on CPU</code>. This really just means\n",
            "that it's running on something other than a Cloud TPU, which includes a GPU.</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#prediction-from-classifier\" id=\"user-content-prediction-from-classifier\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Prediction from classifier</h4>\n",
            "<p>Once you have trained your classifier you can use it in inference mode by using\n",
            "the --do_predict=true command. You need to have a file named test.tsv in the\n",
            "input folder. Output will be created in file called test_results.tsv in the\n",
            "output folder. Each line will contain output for each sample, columns are the\n",
            "class probabilities.</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12\n",
            "<span class=\"pl-k\">export</span> GLUE_DIR=/path/to/glue\n",
            "<span class=\"pl-k\">export</span> TRAINED_CLASSIFIER=/path/to/fine/tuned/classifier\n",
            "\n",
            "python run_classifier.py \\\n",
            "  --task_name=MRPC \\\n",
            "  --do_predict=true \\\n",
            "  --data_dir=<span class=\"pl-smi\">$GLUE_DIR</span>/MRPC \\\n",
            "  --vocab_file=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/vocab.txt \\\n",
            "  --bert_config_file=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/bert_config.json \\\n",
            "  --init_checkpoint=<span class=\"pl-smi\">$TRAINED_CLASSIFIER</span> \\\n",
            "  --max_seq_length=128 \\\n",
            "  --output_dir=/tmp/mrpc_output/</pre></div>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#squad-11\" id=\"user-content-squad-11\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>SQuAD 1.1</h3>\n",
            "<p>The Stanford Question Answering Dataset (SQuAD) is a popular question answering\n",
            "benchmark dataset. BERT (at the time of the release) obtains state-of-the-art\n",
            "results on SQuAD with almost no task-specific network architecture modifications\n",
            "or data augmentation. However, it does require semi-complex data pre-processing\n",
            "and post-processing to deal with (a) the variable-length nature of SQuAD context\n",
            "paragraphs, and (b) the character-level answer annotations which are used for\n",
            "SQuAD training. This processing is implemented and documented in <code>run_squad.py</code>.</p>\n",
            "<p>To run on SQuAD, you will first need to download the dataset. The\n",
            "<a href=\"https://rajpurkar.github.io/SQuAD-explorer/\" rel=\"nofollow\">SQuAD website</a> does not seem to\n",
            "link to the v1.1 datasets any longer, but the necessary files can be found here:</p>\n",
            "<ul>\n",
            "<li><a href=\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\" rel=\"nofollow\">train-v1.1.json</a></li>\n",
            "<li><a href=\"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\" rel=\"nofollow\">dev-v1.1.json</a></li>\n",
            "<li><a href=\"https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py\">evaluate-v1.1.py</a></li>\n",
            "</ul>\n",
            "<p>Download these to some directory <code>$SQUAD_DIR</code>.</p>\n",
            "<p>The state-of-the-art SQuAD results from the paper currently cannot be reproduced\n",
            "on a 12GB-16GB GPU due to memory constraints (in fact, even batch size 1 does\n",
            "not seem to fit on a 12GB GPU using <code>BERT-Large</code>). However, a reasonably strong\n",
            "<code>BERT-Base</code> model can be trained on the GPU with these hyperparameters:</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre>python run_squad.py \\\n",
            "  --vocab_file=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/vocab.txt \\\n",
            "  --bert_config_file=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/bert_config.json \\\n",
            "  --init_checkpoint=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/bert_model.ckpt \\\n",
            "  --do_train=True \\\n",
            "  --train_file=<span class=\"pl-smi\">$SQUAD_DIR</span>/train-v1.1.json \\\n",
            "  --do_predict=True \\\n",
            "  --predict_file=<span class=\"pl-smi\">$SQUAD_DIR</span>/dev-v1.1.json \\\n",
            "  --train_batch_size=12 \\\n",
            "  --learning_rate=3e-5 \\\n",
            "  --num_train_epochs=2.0 \\\n",
            "  --max_seq_length=384 \\\n",
            "  --doc_stride=128 \\\n",
            "  --output_dir=/tmp/squad_base/</pre></div>\n",
            "<p>The dev set predictions will be saved into a file called <code>predictions.json</code> in\n",
            "the <code>output_dir</code>:</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre>python <span class=\"pl-smi\">$SQUAD_DIR</span>/evaluate-v1.1.py <span class=\"pl-smi\">$SQUAD_DIR</span>/dev-v1.1.json ./squad/predictions.json</pre></div>\n",
            "<p>Which should produce an output like this:</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre>{<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f1<span class=\"pl-pds\">\"</span></span>: 88.41249612335034, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>exact_match<span class=\"pl-pds\">\"</span></span>: 81.2488174077578}</pre></div>\n",
            "<p>You should see a result similar to the 88.5% reported in the paper for\n",
            "<code>BERT-Base</code>.</p>\n",
            "<p>If you have access to a Cloud TPU, you can train with <code>BERT-Large</code>. Here is a\n",
            "set of hyperparameters (slightly different than the paper) which consistently\n",
            "obtain around 90.5%-91.0% F1 single-system trained only on SQuAD:</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre>python run_squad.py \\\n",
            "  --vocab_file=<span class=\"pl-smi\">$BERT_LARGE_DIR</span>/vocab.txt \\\n",
            "  --bert_config_file=<span class=\"pl-smi\">$BERT_LARGE_DIR</span>/bert_config.json \\\n",
            "  --init_checkpoint=<span class=\"pl-smi\">$BERT_LARGE_DIR</span>/bert_model.ckpt \\\n",
            "  --do_train=True \\\n",
            "  --train_file=<span class=\"pl-smi\">$SQUAD_DIR</span>/train-v1.1.json \\\n",
            "  --do_predict=True \\\n",
            "  --predict_file=<span class=\"pl-smi\">$SQUAD_DIR</span>/dev-v1.1.json \\\n",
            "  --train_batch_size=24 \\\n",
            "  --learning_rate=3e-5 \\\n",
            "  --num_train_epochs=2.0 \\\n",
            "  --max_seq_length=384 \\\n",
            "  --doc_stride=128 \\\n",
            "  --output_dir=gs://some_bucket/squad_large/ \\\n",
            "  --use_tpu=True \\\n",
            "  --tpu_name=<span class=\"pl-smi\">$TPU_NAME</span></pre></div>\n",
            "<p>For example, one random run with these parameters produces the following Dev\n",
            "scores:</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre>{<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f1<span class=\"pl-pds\">\"</span></span>: 90.87081895814865, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>exact_match<span class=\"pl-pds\">\"</span></span>: 84.38978240302744}</pre></div>\n",
            "<p>If you fine-tune for one epoch on\n",
            "<a href=\"http://nlp.cs.washington.edu/triviaqa/\" rel=\"nofollow\">TriviaQA</a> before this the results will\n",
            "be even better, but you will need to convert TriviaQA into the SQuAD json\n",
            "format.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#squad-20\" id=\"user-content-squad-20\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>SQuAD 2.0</h3>\n",
            "<p>This model is also implemented and documented in <code>run_squad.py</code>.</p>\n",
            "<p>To run on SQuAD 2.0, you will first need to download the dataset. The necessary\n",
            "files can be found here:</p>\n",
            "<ul>\n",
            "<li><a href=\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\" rel=\"nofollow\">train-v2.0.json</a></li>\n",
            "<li><a href=\"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\" rel=\"nofollow\">dev-v2.0.json</a></li>\n",
            "<li><a href=\"https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\" rel=\"nofollow\">evaluate-v2.0.py</a></li>\n",
            "</ul>\n",
            "<p>Download these to some directory <code>$SQUAD_DIR</code>.</p>\n",
            "<p>On Cloud TPU you can run with BERT-Large as follows:</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre>python run_squad.py \\\n",
            "  --vocab_file=<span class=\"pl-smi\">$BERT_LARGE_DIR</span>/vocab.txt \\\n",
            "  --bert_config_file=<span class=\"pl-smi\">$BERT_LARGE_DIR</span>/bert_config.json \\\n",
            "  --init_checkpoint=<span class=\"pl-smi\">$BERT_LARGE_DIR</span>/bert_model.ckpt \\\n",
            "  --do_train=True \\\n",
            "  --train_file=<span class=\"pl-smi\">$SQUAD_DIR</span>/train-v2.0.json \\\n",
            "  --do_predict=True \\\n",
            "  --predict_file=<span class=\"pl-smi\">$SQUAD_DIR</span>/dev-v2.0.json \\\n",
            "  --train_batch_size=24 \\\n",
            "  --learning_rate=3e-5 \\\n",
            "  --num_train_epochs=2.0 \\\n",
            "  --max_seq_length=384 \\\n",
            "  --doc_stride=128 \\\n",
            "  --output_dir=gs://some_bucket/squad_large/ \\\n",
            "  --use_tpu=True \\\n",
            "  --tpu_name=<span class=\"pl-smi\">$TPU_NAME</span> \\\n",
            "  --version_2_with_negative=True</pre></div>\n",
            "<p>We assume you have copied everything from the output directory to a local\n",
            "directory called ./squad/. The initial dev set predictions will be at\n",
            "./squad/predictions.json and the differences between the score of no answer (\"\")\n",
            "and the best non-null answer for each question will be in the file\n",
            "./squad/null_odds.json</p>\n",
            "<p>Run this script to tune a threshold for predicting null versus non-null answers:</p>\n",
            "<p>python $SQUAD_DIR/evaluate-v2.0.py $SQUAD_DIR/dev-v2.0.json\n",
            "./squad/predictions.json --na-prob-file ./squad/null_odds.json</p>\n",
            "<p>Assume the script outputs \"best_f1_thresh\" THRESH. (Typical values are between\n",
            "-1.0 and -5.0). You can now re-run the model to generate predictions with the\n",
            "derived threshold or alternatively you can extract the appropriate answers from\n",
            "./squad/nbest_predictions.json.</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre>python run_squad.py \\\n",
            "  --vocab_file=<span class=\"pl-smi\">$BERT_LARGE_DIR</span>/vocab.txt \\\n",
            "  --bert_config_file=<span class=\"pl-smi\">$BERT_LARGE_DIR</span>/bert_config.json \\\n",
            "  --init_checkpoint=<span class=\"pl-smi\">$BERT_LARGE_DIR</span>/bert_model.ckpt \\\n",
            "  --do_train=False \\\n",
            "  --train_file=<span class=\"pl-smi\">$SQUAD_DIR</span>/train-v2.0.json \\\n",
            "  --do_predict=True \\\n",
            "  --predict_file=<span class=\"pl-smi\">$SQUAD_DIR</span>/dev-v2.0.json \\\n",
            "  --train_batch_size=24 \\\n",
            "  --learning_rate=3e-5 \\\n",
            "  --num_train_epochs=2.0 \\\n",
            "  --max_seq_length=384 \\\n",
            "  --doc_stride=128 \\\n",
            "  --output_dir=gs://some_bucket/squad_large/ \\\n",
            "  --use_tpu=True \\\n",
            "  --tpu_name=<span class=\"pl-smi\">$TPU_NAME</span> \\\n",
            "  --version_2_with_negative=True \\\n",
            "  --null_score_diff_threshold=<span class=\"pl-smi\">$THRESH</span></pre></div>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#out-of-memory-issues\" id=\"user-content-out-of-memory-issues\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Out-of-memory issues</h3>\n",
            "<p>All experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB of\n",
            "device RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likely\n",
            "to encounter out-of-memory issues if you use the same hyperparameters described\n",
            "in the paper.</p>\n",
            "<p>The factors that affect memory usage are:</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p><strong><code>max_seq_length</code></strong>: The released models were trained with sequence lengths\n",
            "up to 512, but you can fine-tune with a shorter max sequence length to save\n",
            "substantial memory. This is controlled by the <code>max_seq_length</code> flag in our\n",
            "example code.</p>\n",
            "</li>\n",
            "<li>\n",
            "<p><strong><code>train_batch_size</code></strong>: The memory usage is also directly proportional to\n",
            "the batch size.</p>\n",
            "</li>\n",
            "<li>\n",
            "<p><strong>Model type, <code>BERT-Base</code> vs. <code>BERT-Large</code></strong>: The <code>BERT-Large</code> model\n",
            "requires significantly more memory than <code>BERT-Base</code>.</p>\n",
            "</li>\n",
            "<li>\n",
            "<p><strong>Optimizer</strong>: The default optimizer for BERT is Adam, which requires a lot\n",
            "of extra memory to store the <code>m</code> and <code>v</code> vectors. Switching to a more memory\n",
            "efficient optimizer can reduce memory usage, but can also affect the\n",
            "results. We have not experimented with other optimizers for fine-tuning.</p>\n",
            "</li>\n",
            "</ul>\n",
            "<p>Using the default training scripts (<code>run_classifier.py</code> and <code>run_squad.py</code>), we\n",
            "benchmarked the maximum batch size on single Titan X GPU (12GB RAM) with\n",
            "TensorFlow 1.11.0:</p>\n",
            "<table>\n",
            "<thead>\n",
            "<tr>\n",
            "<th>System</th>\n",
            "<th>Seq Length</th>\n",
            "<th>Max Batch Size</th>\n",
            "</tr>\n",
            "</thead>\n",
            "<tbody>\n",
            "<tr>\n",
            "<td><code>BERT-Base</code></td>\n",
            "<td>64</td>\n",
            "<td>64</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>...</td>\n",
            "<td>128</td>\n",
            "<td>32</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>...</td>\n",
            "<td>256</td>\n",
            "<td>16</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>...</td>\n",
            "<td>320</td>\n",
            "<td>14</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>...</td>\n",
            "<td>384</td>\n",
            "<td>12</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>...</td>\n",
            "<td>512</td>\n",
            "<td>6</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td><code>BERT-Large</code></td>\n",
            "<td>64</td>\n",
            "<td>12</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>...</td>\n",
            "<td>128</td>\n",
            "<td>6</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>...</td>\n",
            "<td>256</td>\n",
            "<td>2</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>...</td>\n",
            "<td>320</td>\n",
            "<td>1</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>...</td>\n",
            "<td>384</td>\n",
            "<td>0</td>\n",
            "</tr>\n",
            "<tr>\n",
            "<td>...</td>\n",
            "<td>512</td>\n",
            "<td>0</td>\n",
            "</tr>\n",
            "</tbody>\n",
            "</table>\n",
            "<p>Unfortunately, these max batch sizes for <code>BERT-Large</code> are so small that they\n",
            "will actually harm the model accuracy, regardless of the learning rate used. We\n",
            "are working on adding code to this repository which will allow much larger\n",
            "effective batch sizes to be used on the GPU. The code will be based on one (or\n",
            "both) of the following techniques:</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p><strong>Gradient accumulation</strong>: The samples in a minibatch are typically\n",
            "independent with respect to gradient computation (excluding batch\n",
            "normalization, which is not used here). This means that the gradients of\n",
            "multiple smaller minibatches can be accumulated before performing the weight\n",
            "update, and this will be exactly equivalent to a single larger update.</p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/openai/gradient-checkpointing\"><strong>Gradient checkpointing</strong></a>:\n",
            "The major use of GPU/TPU memory during DNN training is caching the\n",
            "intermediate activations in the forward pass that are necessary for\n",
            "efficient computation in the backward pass. \"Gradient checkpointing\" trades\n",
            "memory for compute time by re-computing the activations in an intelligent\n",
            "way.</p>\n",
            "</li>\n",
            "</ul>\n",
            "<p><strong>However, this is not implemented in the current release.</strong></p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#using-bert-to-extract-fixed-feature-vectors-like-elmo\" id=\"user-content-using-bert-to-extract-fixed-feature-vectors-like-elmo\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Using BERT to extract fixed feature vectors (like ELMo)</h2>\n",
            "<p>In certain cases, rather than fine-tuning the entire pre-trained model\n",
            "end-to-end, it can be beneficial to obtained <em>pre-trained contextual\n",
            "embeddings</em>, which are fixed contextual representations of each input token\n",
            "generated from the hidden layers of the pre-trained model. This should also\n",
            "mitigate most of the out-of-memory issues.</p>\n",
            "<p>As an example, we include the script <code>extract_features.py</code> which can be used\n",
            "like this:</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Sentence A and Sentence B are separated by the ||| delimiter for sentence</span>\n",
            "<span class=\"pl-c\"><span class=\"pl-c\">#</span> pair tasks like question answering and entailment.</span>\n",
            "<span class=\"pl-c\"><span class=\"pl-c\">#</span> For single sentence inputs, put one sentence per line and DON'T use the</span>\n",
            "<span class=\"pl-c\"><span class=\"pl-c\">#</span> delimiter.</span>\n",
            "<span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Who was Jim Henson ? ||| Jim Henson was a puppeteer<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">&gt;</span> /tmp/input.txt\n",
            "\n",
            "python extract_features.py \\\n",
            "  --input_file=/tmp/input.txt \\\n",
            "  --output_file=/tmp/output.jsonl \\\n",
            "  --vocab_file=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/vocab.txt \\\n",
            "  --bert_config_file=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/bert_config.json \\\n",
            "  --init_checkpoint=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/bert_model.ckpt \\\n",
            "  --layers=-1,-2,-3,-4 \\\n",
            "  --max_seq_length=128 \\\n",
            "  --batch_size=8</pre></div>\n",
            "<p>This will create a JSON file (one line per line of input) containing the BERT\n",
            "activations from each Transformer layer specified by <code>layers</code> (-1 is the final\n",
            "hidden layer of the Transformer, etc.)</p>\n",
            "<p>Note that this script will produce very large output files (by default, around\n",
            "15kb for every input token).</p>\n",
            "<p>If you need to maintain alignment between the original and tokenized words (for\n",
            "projecting training labels), see the <a href=\"#tokenization\">Tokenization</a> section\n",
            "below.</p>\n",
            "<p><strong>Note:</strong> You may see a message like <code>Could not find trained model in model_dir: /tmp/tmpuB5g5c, running initialization to predict.</code> This message is expected, it\n",
            "just means that we are using the <code>init_from_checkpoint()</code> API rather than the\n",
            "saved model API. If you don't specify a checkpoint or specify an invalid\n",
            "checkpoint, this script will complain.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#tokenization\" id=\"user-content-tokenization\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Tokenization</h2>\n",
            "<p>For sentence-level tasks (or sentence-pair) tasks, tokenization is very simple.\n",
            "Just follow the example code in <code>run_classifier.py</code> and <code>extract_features.py</code>.\n",
            "The basic procedure for sentence-level tasks is:</p>\n",
            "<ol>\n",
            "<li>\n",
            "<p>Instantiate an instance of <code>tokenizer = tokenization.FullTokenizer</code></p>\n",
            "</li>\n",
            "<li>\n",
            "<p>Tokenize the raw text with <code>tokens = tokenizer.tokenize(raw_text)</code>.</p>\n",
            "</li>\n",
            "<li>\n",
            "<p>Truncate to the maximum sequence length. (You can use up to 512, but you\n",
            "probably want to use shorter if possible for memory and speed reasons.)</p>\n",
            "</li>\n",
            "<li>\n",
            "<p>Add the <code>[CLS]</code> and <code>[SEP]</code> tokens in the right place.</p>\n",
            "</li>\n",
            "</ol>\n",
            "<p>Word-level and span-level tasks (e.g., SQuAD and NER) are more complex, since\n",
            "you need to maintain alignment between your input text and output text so that\n",
            "you can project your training labels. SQuAD is a particularly complex example\n",
            "because the input labels are <em>character</em>-based, and SQuAD paragraphs are often\n",
            "longer than our maximum sequence length. See the code in <code>run_squad.py</code> to show\n",
            "how we handle this.</p>\n",
            "<p>Before we describe the general recipe for handling word-level tasks, it's\n",
            "important to understand what exactly our tokenizer is doing. It has three main\n",
            "steps:</p>\n",
            "<ol>\n",
            "<li>\n",
            "<p><strong>Text normalization</strong>: Convert all whitespace characters to spaces, and\n",
            "(for the <code>Uncased</code> model) lowercase the input and strip out accent markers.\n",
            "E.g., <code>John Johanson's, → john johanson's,</code>.</p>\n",
            "</li>\n",
            "<li>\n",
            "<p><strong>Punctuation splitting</strong>: Split <em>all</em> punctuation characters on both sides\n",
            "(i.e., add whitespace around all punctuation characters). Punctuation\n",
            "characters are defined as (a) Anything with a <code>P*</code> Unicode class, (b) any\n",
            "non-letter/number/space ASCII character (e.g., characters like <code>$</code> which are\n",
            "technically not punctuation). E.g., <code>john johanson's, → john johanson ' s ,</code></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><strong>WordPiece tokenization</strong>: Apply whitespace tokenization to the output of\n",
            "the above procedure, and apply\n",
            "<a href=\"https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py\">WordPiece</a>\n",
            "tokenization to each token separately. (Our implementation is directly based\n",
            "on the one from <code>tensor2tensor</code>, which is linked). E.g., <code>john johanson ' s , → john johan ##son ' s ,</code></p>\n",
            "</li>\n",
            "</ol>\n",
            "<p>The advantage of this scheme is that it is \"compatible\" with most existing\n",
            "English tokenizers. For example, imagine that you have a part-of-speech tagging\n",
            "task which looks like this:</p>\n",
            "<pre><code>Input:  John Johanson 's   house\n",
            "Labels: NNP  NNP      POS NN\n",
            "</code></pre>\n",
            "<p>The tokenized output will look like this:</p>\n",
            "<pre><code>Tokens: john johan ##son ' s house\n",
            "</code></pre>\n",
            "<p>Crucially, this would be the same output as if the raw text were <code>John Johanson's house</code> (with no space before the <code>'s</code>).</p>\n",
            "<p>If you have a pre-tokenized representation with word-level annotations, you can\n",
            "simply tokenize each input word independently, and deterministically maintain an\n",
            "original-to-tokenized alignment:</p>\n",
            "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\">### Input</span>\n",
            "<span class=\"pl-s1\">orig_tokens</span> <span class=\"pl-c1\">=</span> [<span class=\"pl-s\">\"John\"</span>, <span class=\"pl-s\">\"Johanson\"</span>, <span class=\"pl-s\">\"'s\"</span>,  <span class=\"pl-s\">\"house\"</span>]\n",
            "<span class=\"pl-s1\">labels</span>      <span class=\"pl-c1\">=</span> [<span class=\"pl-s\">\"NNP\"</span>,  <span class=\"pl-s\">\"NNP\"</span>,      <span class=\"pl-s\">\"POS\"</span>, <span class=\"pl-s\">\"NN\"</span>]\n",
            "\n",
            "<span class=\"pl-c\">### Output</span>\n",
            "<span class=\"pl-s1\">bert_tokens</span> <span class=\"pl-c1\">=</span> []\n",
            "\n",
            "<span class=\"pl-c\"># Token map will be an int -&gt; int mapping between the `orig_tokens` index and</span>\n",
            "<span class=\"pl-c\"># the `bert_tokens` index.</span>\n",
            "<span class=\"pl-s1\">orig_to_tok_map</span> <span class=\"pl-c1\">=</span> []\n",
            "\n",
            "<span class=\"pl-s1\">tokenizer</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">tokenization</span>.<span class=\"pl-v\">FullTokenizer</span>(\n",
            "    <span class=\"pl-s1\">vocab_file</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">vocab_file</span>, <span class=\"pl-s1\">do_lower_case</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">True</span>)\n",
            "\n",
            "<span class=\"pl-s1\">bert_tokens</span>.<span class=\"pl-en\">append</span>(<span class=\"pl-s\">\"[CLS]\"</span>)\n",
            "<span class=\"pl-k\">for</span> <span class=\"pl-s1\">orig_token</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">orig_tokens</span>:\n",
            "  <span class=\"pl-s1\">orig_to_tok_map</span>.<span class=\"pl-en\">append</span>(<span class=\"pl-en\">len</span>(<span class=\"pl-s1\">bert_tokens</span>))\n",
            "  <span class=\"pl-s1\">bert_tokens</span>.<span class=\"pl-en\">extend</span>(<span class=\"pl-s1\">tokenizer</span>.<span class=\"pl-en\">tokenize</span>(<span class=\"pl-s1\">orig_token</span>))\n",
            "<span class=\"pl-s1\">bert_tokens</span>.<span class=\"pl-en\">append</span>(<span class=\"pl-s\">\"[SEP]\"</span>)\n",
            "\n",
            "<span class=\"pl-c\"># bert_tokens == [\"[CLS]\", \"john\", \"johan\", \"##son\", \"'\", \"s\", \"house\", \"[SEP]\"]</span>\n",
            "<span class=\"pl-c\"># orig_to_tok_map == [1, 2, 4, 6]</span></pre></div>\n",
            "<p>Now <code>orig_to_tok_map</code> can be used to project <code>labels</code> to the tokenized\n",
            "representation.</p>\n",
            "<p>There are common English tokenization schemes which will cause a slight mismatch\n",
            "between how BERT was pre-trained. For example, if your input tokenization splits\n",
            "off contractions like <code>do n't</code>, this will cause a mismatch. If it is possible to\n",
            "do so, you should pre-process your data to convert these back to raw-looking\n",
            "text, but if it's not possible, this mismatch is likely not a big deal.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#pre-training-with-bert\" id=\"user-content-pre-training-with-bert\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Pre-training with BERT</h2>\n",
            "<p>We are releasing code to do \"masked LM\" and \"next sentence prediction\" on an\n",
            "arbitrary text corpus. Note that this is <em>not</em> the exact code that was used for\n",
            "the paper (the original code was written in C++, and had some additional\n",
            "complexity), but this code does generate pre-training data as described in the\n",
            "paper.</p>\n",
            "<p>Here's how to run the data generation. The input is a plain text file, with one\n",
            "sentence per line. (It is important that these be actual sentences for the \"next\n",
            "sentence prediction\" task). Documents are delimited by empty lines. The output\n",
            "is a set of <code>tf.train.Example</code>s serialized into <code>TFRecord</code> file format.</p>\n",
            "<p>You can perform sentence segmentation with an off-the-shelf NLP toolkit such as\n",
            "<a href=\"https://spacy.io/\" rel=\"nofollow\">spaCy</a>. The <code>create_pretraining_data.py</code> script will\n",
            "concatenate segments until they reach the maximum sequence length to minimize\n",
            "computational waste from padding (see the script for more details). However, you\n",
            "may want to intentionally add a slight amount of noise to your input data (e.g.,\n",
            "randomly truncate 2% of input segments) to make it more robust to non-sentential\n",
            "input during fine-tuning.</p>\n",
            "<p>This script stores all of the examples for the entire input file in memory, so\n",
            "for large data files you should shard the input file and call the script\n",
            "multiple times. (You can pass in a file glob to <code>run_pretraining.py</code>, e.g.,\n",
            "<code>tf_examples.tf_record*</code>.)</p>\n",
            "<p>The <code>max_predictions_per_seq</code> is the maximum number of masked LM predictions per\n",
            "sequence. You should set this to around <code>max_seq_length</code> * <code>masked_lm_prob</code> (the\n",
            "script doesn't do that automatically because the exact value needs to be passed\n",
            "to both scripts).</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre>python create_pretraining_data.py \\\n",
            "  --input_file=./sample_text.txt \\\n",
            "  --output_file=/tmp/tf_examples.tfrecord \\\n",
            "  --vocab_file=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/vocab.txt \\\n",
            "  --do_lower_case=True \\\n",
            "  --max_seq_length=128 \\\n",
            "  --max_predictions_per_seq=20 \\\n",
            "  --masked_lm_prob=0.15 \\\n",
            "  --random_seed=12345 \\\n",
            "  --dupe_factor=5</pre></div>\n",
            "<p>Here's how to run the pre-training. Do not include <code>init_checkpoint</code> if you are\n",
            "pre-training from scratch. The model configuration (including vocab size) is\n",
            "specified in <code>bert_config_file</code>. This demo code only pre-trains for a small\n",
            "number of steps (20), but in practice you will probably want to set\n",
            "<code>num_train_steps</code> to 10000 steps or more. The <code>max_seq_length</code> and\n",
            "<code>max_predictions_per_seq</code> parameters passed to <code>run_pretraining.py</code> must be the\n",
            "same as <code>create_pretraining_data.py</code>.</p>\n",
            "<div class=\"highlight highlight-source-shell\"><pre>python run_pretraining.py \\\n",
            "  --input_file=/tmp/tf_examples.tfrecord \\\n",
            "  --output_dir=/tmp/pretraining_output \\\n",
            "  --do_train=True \\\n",
            "  --do_eval=True \\\n",
            "  --bert_config_file=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/bert_config.json \\\n",
            "  --init_checkpoint=<span class=\"pl-smi\">$BERT_BASE_DIR</span>/bert_model.ckpt \\\n",
            "  --train_batch_size=32 \\\n",
            "  --max_seq_length=128 \\\n",
            "  --max_predictions_per_seq=20 \\\n",
            "  --num_train_steps=20 \\\n",
            "  --num_warmup_steps=10 \\\n",
            "  --learning_rate=2e-5</pre></div>\n",
            "<p>This will produce an output like this:</p>\n",
            "<pre><code>***** Eval results *****\n",
            "  global_step = 20\n",
            "  loss = 0.0979674\n",
            "  masked_lm_accuracy = 0.985479\n",
            "  masked_lm_loss = 0.0979328\n",
            "  next_sentence_accuracy = 1.0\n",
            "  next_sentence_loss = 3.45724e-05\n",
            "</code></pre>\n",
            "<p>Note that since our <code>sample_text.txt</code> file is very small, this example training\n",
            "will overfit that data in only a few steps and produce unrealistically high\n",
            "accuracy numbers.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#pre-training-tips-and-caveats\" id=\"user-content-pre-training-tips-and-caveats\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Pre-training tips and caveats</h3>\n",
            "<ul>\n",
            "<li><strong>If using your own vocabulary, make sure to change <code>vocab_size</code> in\n",
            "<code>bert_config.json</code>. If you use a larger vocabulary without changing this,\n",
            "you will likely get NaNs when training on GPU or TPU due to unchecked\n",
            "out-of-bounds access.</strong></li>\n",
            "<li>If your task has a large domain-specific corpus available (e.g., \"movie\n",
            "reviews\" or \"scientific papers\"), it will likely be beneficial to run\n",
            "additional steps of pre-training on your corpus, starting from the BERT\n",
            "checkpoint.</li>\n",
            "<li>The learning rate we used in the paper was 1e-4. However, if you are doing\n",
            "additional steps of pre-training starting from an existing BERT checkpoint,\n",
            "you should use a smaller learning rate (e.g., 2e-5).</li>\n",
            "<li>Current BERT models are English-only, but we do plan to release a\n",
            "multilingual model which has been pre-trained on a lot of languages in the\n",
            "near future (hopefully by the end of November 2018).</li>\n",
            "<li>Longer sequences are disproportionately expensive because attention is\n",
            "quadratic to the sequence length. In other words, a batch of 64 sequences of\n",
            "length 512 is much more expensive than a batch of 256 sequences of\n",
            "length 128. The fully-connected/convolutional cost is the same, but the\n",
            "attention cost is far greater for the 512-length sequences. Therefore, one\n",
            "good recipe is to pre-train for, say, 90,000 steps with a sequence length of\n",
            "128 and then for 10,000 additional steps with a sequence length of 512. The\n",
            "very long sequences are mostly needed to learn positional embeddings, which\n",
            "can be learned fairly quickly. Note that this does require generating the\n",
            "data twice with different values of <code>max_seq_length</code>.</li>\n",
            "<li>If you are pre-training from scratch, be prepared that pre-training is\n",
            "computationally expensive, especially on GPUs. If you are pre-training from\n",
            "scratch, our recommended recipe is to pre-train a <code>BERT-Base</code> on a single\n",
            "<a href=\"https://cloud.google.com/tpu/docs/pricing\" rel=\"nofollow\">preemptible Cloud TPU v2</a>, which\n",
            "takes about 2 weeks at a cost of about $500 USD (based on the pricing in\n",
            "October 2018). You will have to scale down the batch size when only training\n",
            "on a single Cloud TPU, compared to what was used in the paper. It is\n",
            "recommended to use the largest batch size that fits into TPU memory.</li>\n",
            "</ul>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#pre-training-data\" id=\"user-content-pre-training-data\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Pre-training data</h3>\n",
            "<p>We will <strong>not</strong> be able to release the pre-processed datasets used in the paper.\n",
            "For Wikipedia, the recommended pre-processing is to download\n",
            "<a href=\"https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\" rel=\"nofollow\">the latest dump</a>,\n",
            "extract the text with\n",
            "<a href=\"https://github.com/attardi/wikiextractor\"><code>WikiExtractor.py</code></a>, and then apply\n",
            "any necessary cleanup to convert it into plain text.</p>\n",
            "<p>Unfortunately the researchers who collected the\n",
            "<a href=\"http://yknzhu.wixsite.com/mbweb\" rel=\"nofollow\">BookCorpus</a> no longer have it available for\n",
            "public download. The\n",
            "<a href=\"https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html\" rel=\"nofollow\">Project Guttenberg Dataset</a>\n",
            "is a somewhat smaller (200M word) collection of older books that are public\n",
            "domain.</p>\n",
            "<p><a href=\"http://commoncrawl.org/\" rel=\"nofollow\">Common Crawl</a> is another very large collection of\n",
            "text, but you will likely have to do substantial pre-processing and cleanup to\n",
            "extract a usable corpus for pre-training BERT.</p>\n",
            "<h3><a aria-hidden=\"true\" class=\"anchor\" href=\"#learning-a-new-wordpiece-vocabulary\" id=\"user-content-learning-a-new-wordpiece-vocabulary\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Learning a new WordPiece vocabulary</h3>\n",
            "<p>This repository does not include code for <em>learning</em> a new WordPiece vocabulary.\n",
            "The reason is that the code used in the paper was implemented in C++ with\n",
            "dependencies on Google's internal libraries. For English, it is almost always\n",
            "better to just start with our vocabulary and pre-trained models. For learning\n",
            "vocabularies of other languages, there are a number of open source options\n",
            "available. However, keep in mind that these are not compatible with our\n",
            "<code>tokenization.py</code> library:</p>\n",
            "<ul>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/google/sentencepiece\">Google's SentencePiece library</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py\">tensor2tensor's WordPiece generation script</a></p>\n",
            "</li>\n",
            "<li>\n",
            "<p><a href=\"https://github.com/rsennrich/subword-nmt\">Rico Sennrich's Byte Pair Encoding library</a></p>\n",
            "</li>\n",
            "</ul>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#using-bert-in-colab\" id=\"user-content-using-bert-in-colab\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Using BERT in Colab</h2>\n",
            "<p>If you want to use BERT with <a href=\"https://colab.research.google.com\" rel=\"nofollow\">Colab</a>, you can\n",
            "get started with the notebook\n",
            "\"<a href=\"https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb\" rel=\"nofollow\">BERT FineTuning with Cloud TPUs</a>\".\n",
            "<strong>At the time of this writing (October 31st, 2018), Colab users can access a\n",
            "Cloud TPU completely for free.</strong> Note: One per user, availability limited,\n",
            "requires a Google Cloud Platform account with storage (although storage may be\n",
            "purchased with free credit for signing up with GCP), and this capability may not\n",
            "longer be available in the future. Click on the BERT Colab that was just linked\n",
            "for more information.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#faq\" id=\"user-content-faq\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>FAQ</h2>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#is-this-code-compatible-with-cloud-tpus-what-about-gpus\" id=\"user-content-is-this-code-compatible-with-cloud-tpus-what-about-gpus\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Is this code compatible with Cloud TPUs? What about GPUs?</h4>\n",
            "<p>Yes, all of the code in this repository works out-of-the-box with CPU, GPU, and\n",
            "Cloud TPU. However, GPU training is single-GPU only.</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#i-am-getting-out-of-memory-errors-what-is-wrong\" id=\"user-content-i-am-getting-out-of-memory-errors-what-is-wrong\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>I am getting out-of-memory errors, what is wrong?</h4>\n",
            "<p>See the section on <a href=\"#out-of-memory-issues\">out-of-memory issues</a> for more\n",
            "information.</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#is-there-a-pytorch-version-available\" id=\"user-content-is-there-a-pytorch-version-available\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Is there a PyTorch version available?</h4>\n",
            "<p>There is no official PyTorch implementation. However, NLP researchers from\n",
            "HuggingFace made a\n",
            "<a href=\"https://github.com/huggingface/pytorch-pretrained-BERT\">PyTorch version of BERT available</a>\n",
            "which is compatible with our pre-trained checkpoints and is able to reproduce\n",
            "our results. We were not involved in the creation or maintenance of the PyTorch\n",
            "implementation so please direct any questions towards the authors of that\n",
            "repository.</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#is-there-a-chainer-version-available\" id=\"user-content-is-there-a-chainer-version-available\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Is there a Chainer version available?</h4>\n",
            "<p>There is no official Chainer implementation. However, Sosuke Kobayashi made a\n",
            "<a href=\"https://github.com/soskek/bert-chainer\">Chainer version of BERT available</a>\n",
            "which is compatible with our pre-trained checkpoints and is able to reproduce\n",
            "our results. We were not involved in the creation or maintenance of the Chainer\n",
            "implementation so please direct any questions towards the authors of that\n",
            "repository.</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#will-models-in-other-languages-be-released\" id=\"user-content-will-models-in-other-languages-be-released\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Will models in other languages be released?</h4>\n",
            "<p>Yes, we plan to release a multi-lingual BERT model in the near future. We cannot\n",
            "make promises about exactly which languages will be included, but it will likely\n",
            "be a single model which includes <em>most</em> of the languages which have a\n",
            "significantly-sized Wikipedia.</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#will-models-larger-than-bert-large-be-released\" id=\"user-content-will-models-larger-than-bert-large-be-released\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Will models larger than <code>BERT-Large</code> be released?</h4>\n",
            "<p>So far we have not attempted to train anything larger than <code>BERT-Large</code>. It is\n",
            "possible that we will release larger models if we are able to obtain significant\n",
            "improvements.</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#what-license-is-this-library-released-under\" id=\"user-content-what-license-is-this-library-released-under\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>What license is this library released under?</h4>\n",
            "<p>All code <em>and</em> models are released under the Apache 2.0 license. See the\n",
            "<code>LICENSE</code> file for more information.</p>\n",
            "<h4><a aria-hidden=\"true\" class=\"anchor\" href=\"#how-do-i-cite-bert\" id=\"user-content-how-do-i-cite-bert\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>How do I cite BERT?</h4>\n",
            "<p>For now, cite <a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">the Arxiv paper</a>:</p>\n",
            "<pre><code>@article{devlin2018bert,\n",
            "  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\n",
            "  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n",
            "  journal={arXiv preprint arXiv:1810.04805},\n",
            "  year={2018}\n",
            "}\n",
            "</code></pre>\n",
            "<p>If we submit the paper to a conference or journal, we will update the BibTeX.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#disclaimer\" id=\"user-content-disclaimer\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Disclaimer</h2>\n",
            "<p>This is not an official Google product.</p>\n",
            "<h2><a aria-hidden=\"true\" class=\"anchor\" href=\"#contact-information\" id=\"user-content-contact-information\"><svg aria-hidden=\"true\" class=\"octicon octicon-link\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\" fill-rule=\"evenodd\"></path></svg></a>Contact information</h2>\n",
            "<p>For help or issues using BERT, please submit a GitHub issue.</p>\n",
            "<p>For personal communication related to BERT, please contact Jacob Devlin\n",
            "(<code>jacobdevlin@google.com</code>), Ming-Wei Chang (<code>mingweichang@google.com</code>), or\n",
            "Kenton Lee (<code>kentonl@google.com</code>).</p>\n",
            "</article>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg5B04Pf3etR",
        "outputId": "60e48dd0-9556-47c5-c669-a57f033bae7b"
      },
      "source": [
        "text_labels = {}\r\n",
        "for n in gd:\r\n",
        "    b = bs(str(n), 'html.parser')\r\n",
        "    bt = b.find('article')\r\n",
        "    for text in bt.children:\r\n",
        "        doc = str(text).split('\\n')\r\n",
        "        prep_doc = list()\r\n",
        "        if len(doc) > 2:\r\n",
        "            for d in doc:\r\n",
        "                temp_text = list()\r\n",
        "                prep_doc.append(d)\r\n",
        "                temp_text.append(d)\r\n",
        "                try:\r\n",
        "                    if d[0] != '<':\r\n",
        "                        prep_doc[-2] = prep_doc[-2] + ' ' + d\r\n",
        "                        del prep_doc[-1]\r\n",
        "                    del temp_text[0]\r\n",
        "                except:\r\n",
        "                    pass\r\n",
        "        else:\r\n",
        "            pass\r\n",
        "\r\n",
        "        tags = list()\r\n",
        "        for _d in prep_doc:\r\n",
        "#             print(\"========\")\r\n",
        "#             print(_d)\r\n",
        "            na_d = re.sub('<a.*?>.*?</a>|<table>.*?</table>|<t.*?>|<a.*?>', '', _d, 0)\r\n",
        "            net_d = re.sub('</.*?>|class=.*?\"|<img.*?>|<g-emoji.*?>', '', na_d, 0)\r\n",
        "            notag_text = re.sub('<.*?>', '', na_d, 0).rstrip('\\n').strip()\r\n",
        "            en_text = re.sub('\\W', '', notag_text, 0)\r\n",
        "\r\n",
        "            try:\r\n",
        "                tag = re.findall('<.*?>',net_d, re.I)[0]\r\n",
        "            except:\r\n",
        "                pass\r\n",
        "\r\n",
        "            if tag not in tags:\r\n",
        "                tags.append(tag)\r\n",
        "\r\n",
        "            if len(notag_text) == 0 or len(en_text) == 0:\r\n",
        "                continue\r\n",
        "            else:\r\n",
        "                text_labels[notag_text] = tags\r\n",
        "\r\n",
        "for k, v in text_labels.items():\r\n",
        "  print(k, v)   \r\n",
        "# print(text_labels)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documentation ['<li>', '<strong>']\n",
            "is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of ['<p>']\n",
            ", and ['<p>', '<em>', '<code>']\n",
            "resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications. ['<p>']\n",
            "TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google's Machine Intelligence Research organization to conduct machine learning and deep neural networks research. The system is general enough to be applicable in a wide variety of other domains, as well. ['<p>']\n",
            "TensorFlow provides stable  and  APIs, as well as non-guaranteed backward compatible API for ['<p>']\n",
            "Keep up-to-date with release announcements and security updates by subscribing to ['<p>']\n",
            ". See all the . ['<p>']\n",
            "See the  for the ['<p>']\n",
            ", to ['<p>']\n",
            ", use a ['<p>']\n",
            "To install the current release, which includes support for ['<p>', '<em>']\n",
            "(Ubuntu and Windows): ['<p>', '<em>']\n",
            "Nightly binaries are available for testing using the ['<p>']\n",
            "and ['<p>']\n",
            "packages on PyPi. ['<p>']\n",
            "&gt;&gt;&gt; import tensorflow as tf ['<div highlight highlight-source-python\">', '<span pl-c1\">', '<span pl-s\">']\n",
            "&gt;&gt;&gt; tf.add(1, 2).numpy() ['<div highlight highlight-source-python\">', '<span pl-c1\">', '<span pl-s\">']\n",
            "3 ['<li>', '<em>']\n",
            "&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!') ['<div highlight highlight-source-python\">', '<span pl-c1\">', '<span pl-s\">']\n",
            "&gt;&gt;&gt; hello.numpy() ['<div highlight highlight-source-python\">', '<span pl-c1\">', '<span pl-s\">']\n",
            "b'Hello, TensorFlow!' ['<div highlight highlight-source-python\">', '<span pl-c1\">', '<span pl-s\">']\n",
            "If you want to contribute to TensorFlow, be sure to review the ['<p>']\n",
            ". This project adheres to TensorFlow's ['<p>']\n",
            ". By participating, you are expected to uphold this code. ['<p>']\n",
            "We use  for tracking requests and bugs, please see ['<p>']\n",
            "for general questions and discussion, and please direct specific questions to ['<p>']\n",
            "Build Type ['<strong>', '<br/>']\n",
            "Status ['<p>']\n",
            "Artifacts ['<strong>', '<br/>']\n",
            "Linux CPU ['<span pl-s\">', '<strong>']\n",
            "Linux GPU ['<span pl-s\">', '<strong>']\n",
            "Linux XLA ['<span pl-s\">', '<strong>']\n",
            "TBA ['<strong>', '<br/>']\n",
            "macOS ['<p>', '<strong>']\n",
            "Windows CPU ['<span pl-s\">', '<strong>']\n",
            "Windows GPU ['<span pl-s\">', '<strong>']\n",
            "Android ['<span pl-s\">', '<strong>']\n",
            "Raspberry Pi 0 and 1 ['<span pl-s\">', '<strong>']\n",
            "Raspberry Pi 2 and 3 ['<span pl-s\">', '<strong>']\n",
            "Libtensorflow MacOS CPU ['<p>', '<strong>']\n",
            "Libtensorflow Linux CPU ['<p>', '<strong>']\n",
            "Libtensorflow Linux GPU ['<p>', '<strong>']\n",
            "Libtensorflow Windows CPU ['<p>', '<strong>']\n",
            "Libtensorflow Windows GPU ['<p>', '<strong>']\n",
            "Linux AMD ROCm GPU Nightly ['<strong>']\n",
            "Linux AMD ROCm GPU Stable Release ['<strong>']\n",
            "Release  / ['<strong>']\n",
            "Linux s390x Nightly ['<strong>']\n",
            "Linux s390x CPU Stable Release ['<strong>']\n",
            "Linux ppc64le CPU Nightly ['<strong>']\n",
            "Linux ppc64le CPU Stable Release ['<strong>']\n",
            "Linux ppc64le GPU Nightly ['<strong>']\n",
            "Linux ppc64le GPU Stable Release ['<strong>']\n",
            "Linux aarch64 CPU Nightly (Linaro) ['<strong>']\n",
            "Linux aarch64 CPU Stable Release (Linaro) ['<strong>']\n",
            "Release ['<strong>']\n",
            "Linux aarch64 CPU Nightly (OpenLab) Python 3.6 ['<strong>']\n",
            "Linux aarch64 CPU Stable Release (OpenLab) ['<strong>']\n",
            "Linux CPU with Intel oneAPI Deep Neural Network Library (oneDNN) Nightly ['<strong>']\n",
            "Linux CPU with Intel oneAPI Deep Neural Network Library (oneDNN) Stable Release ['<strong>']\n",
            "Red Hat® Enterprise Linux® 7.6 CPU &amp; GPU  Python 2.7, 3.6 ['<strong>']\n",
            "Container Type ['<strong>']\n",
            "TensorFlow aarch64 Neoverse-N1 CPU Stable (Linaro) Debian ['<strong>']\n",
            "Static ['<strong>']\n",
            "Learn more about the ['<p>']\n",
            "and how to ['<p>']\n",
            "Eager Execution / tf.function ['<ul>', '<li>']\n",
            "Integration of the Keras API ['<ul>', '<li>']\n",
            "Facilitated distributed training ['<ul>', '<li>']\n",
            "TF Data ['<ul>', '<li>']\n",
            "TF SavedModel ['<ul>', '<li>']\n",
            "TensorFlow Hub ['<ul>', '<li>']\n",
            "TensorFlow Serving ['<ul>', '<li>']\n",
            "TensorFlow Lite ['<ul>', '<li>']\n",
            "TensorFlow.js ['<ul>', '<li>']\n",
            "Tidying up the API ['<ul>', '<li>']\n",
            "The conversion tool ['<ul>', '<li>']\n",
            "Alternative variable scoping ['<ul>', '<li>']\n",
            "Hello World (). Very simple example to learn how to print \"hello world\" using TensorFlow 2.0+. ['<ul>', '<li>']\n",
            "Basic Operations (). A simple example that cover TensorFlow 2.0+ basic operations. ['<ul>', '<li>']\n",
            "Linear Regression (). Implement a Linear Regression with TensorFlow 2.0+. ['<ul>', '<li>']\n",
            "Logistic Regression (). Implement a Logistic Regression with TensorFlow 2.0+. ['<ul>', '<li>']\n",
            "Word2Vec (Word Embedding) (). Build a Word Embedding Model (Word2Vec) from Wikipedia data, with TensorFlow 2.0+. ['<ul>', '<li>']\n",
            "GBDT (Gradient Boosted Decision Trees) (). Implement a Gradient Boosted Decision Trees with TensorFlow 2.0+ to predict house value using Boston Housing dataset. ['<ul>', '<li>']\n",
            "Simple Neural Network (). Use TensorFlow 2.0 'layers' and 'model' API to build a simple neural network to classify MNIST digits dataset. ['<ul>', '<li>']\n",
            "Simple Neural Network (low-level) (). Raw implementation of a simple neural network to classify MNIST digits dataset. ['<ul>', '<li>']\n",
            "Convolutional Neural Network (). Use TensorFlow 2.0+ 'layers' and 'model' API to build a convolutional neural network to classify MNIST digits dataset. ['<ul>', '<li>']\n",
            "Convolutional Neural Network (low-level) (). Raw implementation of a convolutional neural network to classify MNIST digits dataset. ['<ul>', '<li>']\n",
            "Recurrent Neural Network (LSTM) (). Build a recurrent neural network (LSTM) to classify MNIST digits dataset, using TensorFlow 2.0 'layers' and 'model' API. ['<ul>', '<li>']\n",
            "Bi-directional Recurrent Neural Network (LSTM) (). Build a bi-directional recurrent neural network (LSTM) to classify MNIST digits dataset, using TensorFlow 2.0+ 'layers' and 'model' API. ['<ul>', '<li>']\n",
            "Dynamic Recurrent Neural Network (LSTM) (). Build a recurrent neural network (LSTM) that performs dynamic calculation to classify sequences of variable length, using TensorFlow 2.0+ 'layers' and 'model' API. ['<ul>', '<li>']\n",
            "Auto-Encoder (). Build an auto-encoder to encode an image to a lower dimension and re-construct it. ['<ul>', '<li>']\n",
            "DCGAN (Deep Convolutional Generative Adversarial Networks) (). Build a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images from noise. ['<ul>', '<li>']\n",
            "Save and Restore a model (). Save and Restore a model with TensorFlow 2.0+. ['<ul>', '<li>']\n",
            "Build Custom Layers &amp; Modules (). Learn how to build your own layers / modules and integrate them into TensorFlow 2.0+ Models. ['<ul>', '<li>']\n",
            "Tensorboard (). Track and visualize neural network computation graph, metrics, weights and more using TensorFlow 2.0+ tensorboard. ['<ul>', '<li>']\n",
            "Load and Parse data (). Build efficient data pipeline with TensorFlow 2.0 (Numpy arrays, Images, CSV files, custom data, ...). ['<ul>', '<li>']\n",
            "Build and Load TFRecords (). Convert data into TFRecords format, and load them with TensorFlow 2.0+. ['<ul>', '<li>']\n",
            "Image Transformation (i.e. Image Augmentation) (). Apply various image augmentation techniques with TensorFlow 2.0+, to generate distorted images for training. ['<ul>', '<li>']\n",
            "Multi-GPU Training (). Train a convolutional neural network with multiple GPUs on CIFAR-10 dataset. ['<ul>', '<li>']\n",
            "Hello World () (). Very simple example to learn how to print \"hello world\" using TensorFlow. ['<ul>', '<li>']\n",
            "Basic Operations () (). A simple example that cover TensorFlow basic operations. ['<ul>', '<li>']\n",
            "TensorFlow Eager API basics () (). Get started with TensorFlow's Eager API. ['<ul>', '<li>']\n",
            "Linear Regression () (). Implement a Linear Regression with TensorFlow. ['<ul>', '<li>']\n",
            "Linear Regression (eager api) () (). Implement a Linear Regression using TensorFlow's Eager API. ['<ul>', '<li>']\n",
            "Logistic Regression () (). Implement a Logistic Regression with TensorFlow. ['<ul>', '<li>']\n",
            "Logistic Regression (eager api) () (). Implement a Logistic Regression using TensorFlow's Eager API. ['<ul>', '<li>']\n",
            "Nearest Neighbor () (). Implement Nearest Neighbor algorithm with TensorFlow. ['<ul>', '<li>']\n",
            "K-Means () (). Build a K-Means classifier with TensorFlow. ['<ul>', '<li>']\n",
            "Random Forest () (). Build a Random Forest classifier with TensorFlow. ['<ul>', '<li>']\n",
            "Gradient Boosted Decision Tree (GBDT) () (). Build a Gradient Boosted Decision Tree (GBDT) with TensorFlow. ['<ul>', '<li>']\n",
            "Word2Vec (Word Embedding) () (). Build a Word Embedding Model (Word2Vec) from Wikipedia data, with TensorFlow. ['<ul>', '<li>']\n",
            "Simple Neural Network () (). Build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset. Raw TensorFlow implementation. ['<ul>', '<li>']\n",
            "Simple Neural Network (tf.layers/estimator api) () (). Use TensorFlow 'layers' and 'estimator' API to build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset. ['<ul>', '<li>']\n",
            "Simple Neural Network (eager api) () (). Use TensorFlow Eager API to build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset. ['<ul>', '<li>']\n",
            "Convolutional Neural Network () (). Build a convolutional neural network to classify MNIST digits dataset. Raw TensorFlow implementation. ['<ul>', '<li>']\n",
            "Convolutional Neural Network (tf.layers/estimator api) () (). Use TensorFlow 'layers' and 'estimator' API to build a convolutional neural network to classify MNIST digits dataset. ['<ul>', '<li>']\n",
            "Recurrent Neural Network (LSTM) () (). Build a recurrent neural network (LSTM) to classify MNIST digits dataset. ['<ul>', '<li>']\n",
            "Bi-directional Recurrent Neural Network (LSTM) () (). Build a bi-directional recurrent neural network (LSTM) to classify MNIST digits dataset. ['<ul>', '<li>']\n",
            "Dynamic Recurrent Neural Network (LSTM) () (). Build a recurrent neural network (LSTM) that performs dynamic calculation to classify sequences of different length. ['<ul>', '<li>']\n",
            "Auto-Encoder () (). Build an auto-encoder to encode an image to a lower dimension and re-construct it. ['<ul>', '<li>']\n",
            "Variational Auto-Encoder () (). Build a variational auto-encoder (VAE), to encode and generate images from noise. ['<ul>', '<li>']\n",
            "GAN (Generative Adversarial Networks) () (). Build a Generative Adversarial Network (GAN) to generate images from noise. ['<ul>', '<li>']\n",
            "DCGAN (Deep Convolutional Generative Adversarial Networks) () (). Build a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images from noise. ['<ul>', '<li>']\n",
            "Save and Restore a model () (). Save and Restore a model with TensorFlow. ['<ul>', '<li>']\n",
            "Tensorboard - Graph and loss visualization () (). Use Tensorboard to visualize the computation Graph and plot the loss. ['<ul>', '<li>']\n",
            "Tensorboard - Advanced visualization () (). Going deeper into Tensorboard; visualize the variables, gradients, and more... ['<ul>', '<li>']\n",
            "Build an image dataset () (). Build your own images dataset with TensorFlow data queues, from image folders or a dataset file. ['<ul>', '<li>']\n",
            "TensorFlow Dataset API () (). Introducing TensorFlow Dataset API for optimizing the input data pipeline. ['<ul>', '<li>']\n",
            "Load and Parse data (). Build efficient data pipeline (Numpy arrays, Images, CSV files, custom data, ...). ['<ul>', '<li>']\n",
            "Build and Load TFRecords (). Convert data into TFRecords format, and load them. ['<ul>', '<li>']\n",
            "Image Transformation (i.e. Image Augmentation) (). Apply various image augmentation techniques, to generate distorted images for training. ['<ul>', '<li>']\n",
            "Basic Operations on multi-GPU () (). A simple example to introduce multi-GPU in TensorFlow. ['<ul>', '<li>']\n",
            "Train a Neural Network on multi-GPU () (). A clear and simple TensorFlow implementation to train a convolutional neural network on multiple GPUs. ['<ul>', '<li>']\n",
            ". Learn the basics of TFLearn through a concrete machine learning task. Build and train a deep neural network classifier. ['<ul>', '<li>']\n",
            ". A large collection of examples using TFLearn. ['<ul>', '<li>']\n",
            "2015-11-10, 谷歌发布全新人工智能系统TensorFlow并宣布开源, 极客学院Wiki启动协同翻译，创建 GitHub 仓库，制定协同规范 ['<ul>', '<li>']\n",
            "2015-11-18, 所有章节认领完毕，翻译完成18章，校对认领7章，Star数361，fork数100，协同翻译QQ群及技术交流群的TF爱好者将近300人，GitHub搜索TensorFlow排名第二 ['<ul>', '<li>']\n",
            "2015-12-10, Star数超过500 ['<ul>', '<li>']\n",
            "2015-12-15, 项目正式上线 ['<ul>', '<li>']\n",
            "参加翻译的有学生，也有老师；有专门研究AI/ML的，也有对此感兴趣的；有国内的，也有远在纽约的；有工程技术人员也有博士、专家 ['<ul>', '<li>']\n",
            "其中一位，同学，为了翻译一篇文档，在前一天没有睡觉的情况下坚持翻完，20个小时没有合眼 ['<ul>', '<li>']\n",
            "还有一位老师，刚从讲台上讲完课，就立即给我们的翻译提修改意见 ['<ul>', '<li>']\n",
            "很多同学自发的将搭建环境中遇到的问题总结到FAQ里帮助他人 ['<ul>', '<li>']\n",
            "为了一个翻译细节，经常是来回几次，和其他人讨论完善 ['<ul>', '<li>']\n",
            "在GitHub上提Issue或Pull Request，地址为: ['<ul>', '<li>']\n",
            "加入TensorFlow技术交流群，与TensorFlower们一起研究交流技术干货--TensorFlow技术交流群：782484288 ['<ul>', '<li>']\n",
            "对翻译感兴趣？加入协同翻译群：248320884，与翻译大神一道研究TensorFlow的本地化 ['<ul>', '<li>']\n",
            "给我们写邮件： ['<ul>', '<li>']\n",
            "提供图文教程托管服务 ['<ul>', '<li>']\n",
            "🚀 github项目地址: ['<ul>', '<li>']\n",
            "🐳 和鲸专栏地址:  【代码可直接fork后云端运行，无需配置环境】 ['<ul>', '<li>']\n",
            "Model implementation is the most important in the industry. Deployment supporting tensorflow models (not Pytorch) exclusively is the present situation in the majority of the Internet enterprises in China. What's more, the industry prefers the models with higher availability; in most cases, they use well-validated modeling architectures with the minimized requirements of adjustment. ['<ul>', '<li>', '<ol>', '<ol start=\"2\">', '<ol start=\"3\">']\n",
            "Fast iterative development and publication is the most important for the researchers since they need to test a lot of new models. Pytorch has advantages in accessing and debugging comparing with TensorFlow2. Pytorch is most frequently used in academy since 2019 with a large amount of the cutting-edge results. ['<ul>', '<li>', '<ol>', '<ol start=\"2\">', '<ol start=\"3\">']\n",
            "Overall, TensorFlow2 and Pytorch are quite similar in programming nowadays, so mastering one helps learning the other. Mastering both framework provides you a lot more open-sourced models and helps you switching between them. ['<ul>', '<li>', '<ol>', '<ol start=\"2\">', '<ol start=\"3\">']\n",
            "Date ['<p>']\n",
            "Contents ['<li>']\n",
            "Difficulties ['<li>']\n",
            "Est. Time ['<li>']\n",
            "Update Status ['<li>']\n",
            "0hour ['<p>']\n",
            "Day 1 ['<li>']\n",
            "1hour ['<p>']\n",
            "Day 2 ['<li>']\n",
            "2hours ['<li>']\n",
            "Day 3 ['<li>']\n",
            "Day 4 ['<li>']\n",
            "Day 5 ['<li>']\n",
            "Day 6 ['<li>']\n",
            "Day 7 ['<li>']\n",
            "Day 8 ['<li>']\n",
            "Day 9 ['<li>']\n",
            "Day 10 ['<li>']\n",
            "Day 11 ['<li>']\n",
            "Day 12 ['<li>']\n",
            "Day 13 ['<li>']\n",
            "0.5hour ['<p>']\n",
            "Day 14 ['<li>']\n",
            "Day 15 ['<li>']\n",
            "Day 16 ['<li>']\n",
            "Day 17 ['<li>']\n",
            "Day 18 ['<li>']\n",
            "Day 19 ['<li>']\n",
            "Day 20 ['<li>']\n",
            "Day 21 ['<li>']\n",
            "Day 22 ['<li>']\n",
            "Day 23 ['<li>']\n",
            "Day 24 ['<li>']\n",
            "Day 25 ['<li>']\n",
            "Day 26 ['<li>']\n",
            "Day 27 ['<li>']\n",
            "Day 28 ['<li>']\n",
            "Day 29 ['<li>']\n",
            "Day 30 ['<li>']\n",
            "#For the readers in mainland China, using gitee will allow cloning with a faster speed ['<div highlight highlight-source-python\">', '<span pl-c\">']\n",
            "#!git clone https://gitee.com/Python_Ai_Road/eat_tensorflow2_in_30_days ['<div highlight highlight-source-python\">', '<span pl-c\">']\n",
            "#It is suggested to install jupytext that converts and run markdown files as ipynb. ['<div highlight highlight-source-python\">', '<span pl-c\">']\n",
            "#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -U jupytext ['<div highlight highlight-source-python\">', '<span pl-c\">']\n",
            "#It is also suggested to install the latest version of TensorFlow to test the demonstrating code in this book ['<div highlight highlight-source-python\">', '<span pl-c\">']\n",
            "#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple  -U tensorflow ['<div highlight highlight-source-python\">', '<span pl-c\">']\n",
            "import tensorflow as tf ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-en\">']\n",
            "#Note: all the codes are tested under TensorFlow 2.1 ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-s1\">']\n",
            "tf.print(\"tensorflow version:\",tf.__version__) ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-s1\">']\n",
            "a = tf.constant(\"hello\") ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-s1\">']\n",
            "b = tf.constant(\"tensorflow2\") ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-s1\">']\n",
            "c = tf.strings.join([a,b],\" \") ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-s1\">']\n",
            "tf.print(c) ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-s1\">']\n",
            "tensorflow version: 2.1.0 hello tensorflow2 ['<pre>']\n",
            "1，在工业界最重要的是模型落地，目前国内的大部分互联网企业只支持TensorFlow模型的在线部署，不支持Pytorch。 并且工业界更加注重的是模型的高可用性，许多时候使用的都是成熟的模型架构，调试需求并不大。 ['<ul>', '<li>', '<p>']\n",
            "2，研究人员最重要的是快速迭代发表文章，需要尝试一些较新的模型架构。而Pytorch在易用性上相比TensorFlow2有一些优势，更加方便调试。 并且在2019年以来在学术界占领了大半壁江山，能够找到的相应最新研究成果更多。 ['<ul>', '<li>', '<p>']\n",
            "3，TensorFlow2和Pytorch实际上整体风格已经非常相似了，学会了其中一个，学习另外一个将比较容易。两种框架都掌握的话，能够参考的开源模型案例更多，并且可以方便地在两种框架之间切换。 ['<ul>', '<li>', '<p>']\n",
            "日期 ['<p>']\n",
            "学习内容 ['<p>']\n",
            "内容难度 ['<p>']\n",
            "预计学习时间 ['<p>']\n",
            "更新状态 ['<p>']\n",
            "day1 ['<p>']\n",
            "day2 ['<p>']\n",
            "2hour ['<p>']\n",
            "day3 ['<p>']\n",
            "day4 ['<p>']\n",
            "day5 ['<p>']\n",
            "day6 ['<p>']\n",
            "day7 ['<p>']\n",
            "day8 ['<p>']\n",
            "day9 ['<p>']\n",
            "day10 ['<p>']\n",
            "day11 ['<p>']\n",
            "day12 ['<p>']\n",
            "day13 ['<p>']\n",
            "day14 ['<p>']\n",
            "day15 ['<p>']\n",
            "day16 ['<p>']\n",
            "day17 ['<p>']\n",
            "day18 ['<p>']\n",
            "day19 ['<p>']\n",
            "day20 ['<p>']\n",
            "day21 ['<p>']\n",
            "day22 ['<p>']\n",
            "day23 ['<p>']\n",
            "day24 ['<p>']\n",
            "day25 ['<p>']\n",
            "day26 ['<p>']\n",
            "day27 ['<p>']\n",
            "day28 ['<p>']\n",
            "day29 ['<p>']\n",
            "day30 ['<p>']\n",
            "#克隆本书源码到本地,使用码云镜像仓库国内下载速度更快 ['<div highlight highlight-source-python\">', '<span pl-c\">']\n",
            "#建议在jupyter notebook 上安装jupytext，以便能够将本书各章节markdown文件视作ipynb文件运行 ['<div highlight highlight-source-python\">', '<span pl-c\">']\n",
            "#建议在jupyter notebook 上安装最新版本tensorflow 测试本书中的代码 ['<div highlight highlight-source-python\">', '<span pl-c\">']\n",
            "#注：本书全部代码在tensorflow 2.1版本测试通过 ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-s1\">']\n",
            "Tutorials ['<ul>', '<li>']\n",
            "- From the basics to slightly more interesting applications of TensorFlow ['<ul>', '<li>']\n",
            "- Introduction to deep learning based on Google's TensorFlow framework. These tutorials are direct ports of Newmu's Theano ['<ul>', '<li>']\n",
            "- These tutorials are intended for beginners in Deep Learning and TensorFlow with well-documented code and YouTube videos. ['<ul>', '<li>']\n",
            "- TensorFlow tutorials and code examples for beginners ['<blockquote>', '<ul>', '<li>']\n",
            "- TensorFlow tutorials written in Python with Jupyter Notebook ['<blockquote>', '<ul>', '<li>']\n",
            "- Re-create the codes from other TensorFlow examples ['<blockquote>', '<ul>', '<li>']\n",
            "- TensorFlow compiled and running properly on the Raspberry Pi ['<ul>', '<li>']\n",
            "- Recurrent Neural Network classification in TensorFlow with LSTM on cellphone sensor data ['<blockquote>', '<ul>', '<li>']\n",
            "- Build your first TensorFlow Android app ['<ul>', '<li>']\n",
            "- Learn to use a seq2seq model on simple datasets as an introduction to the vast array of possibilities that this architecture offers ['<ul>', '<li>']\n",
            "- SIRDS is a means to present 3D data in a 2D image. It allows for scientific data display of a waterfall type plot with no hidden lines due to perspective. ['<ul>', '<li>']\n",
            "- Stanford Course about Tensorflow from 2017 -  - ['<ul>', '<li>']\n",
            "- Concise and ready-to-use TensorFlow tutorials with detailed documentation are provided. ['<ul>', '<li>']\n",
            "- TensorFlow howtos and best practices. Covers the basics as well as advanced topics. ['<ul>', '<li>']\n",
            "- Modular implementation for TensorFlow's official tutorials. (). ['<ul>', '<li>']\n",
            "A conceptual overview of the Estimator API, when you'd use it and why. ['<ul>', '<li>']\n",
            "- Convolutional Neural Networks in Tensorflow, offered by Coursera ['<ul>', '<li>']\n",
            "Models/Projects ['<li>', '<h2>']\n",
            "- Robotics touch model with TensorFlow DQN example ['<ul>', '<li>']\n",
            "- A simple and well-designed template for your tensorflow project. ['<ul>', '<li>']\n",
            "- Implementation of Unsupervised Cross-Domain Image Generation ['<ul>', '<li>']\n",
            "- Attention Based Image Caption Generator ['<ul>', '<li>']\n",
            "Implementation of Neural Style ['<ul>', '<li>']\n",
            "- Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network ['<ul>', '<li>']\n",
            "- Pretty Tensor provides a high level builder API ['<ul>', '<li>']\n",
            "- An implementation of neural style ['<ul>', '<li>']\n",
            "- An implementations of AlexNet3D. Simple AlexNet model but with 3D convolutional layers (conv3d). ['<ul>', '<li>']\n",
            "- Annotated notes and summaries of the TensorFlow white paper, along with SVG figures and links to documentation ['<ul>', '<li>']\n",
            "- Implementation of A Neural Algorithm of Artistic Style ['<ul>', '<li>']\n",
            "- An attempt to implement the random handwriting generation portion of Alex Graves' paper ['<ul>', '<li>']\n",
            "- implementation of Neural Turing Machine ['<ul>', '<li>']\n",
            "- Search, filter, and describe videos based on objects, places, and other things that appear in them ['<ul>', '<li>']\n",
            "- This performs a monolingual translation, going from modern English to Shakespeare and vice-versa. ['<ul>', '<li>']\n",
            "- Implementation of ['<ul>', '<li>']\n",
            "- Chatbot in 200 lines of code ['<ul>', '<li>']\n",
            "- Deep Convolutional Generative Adversarial Networks ['<ul>', '<li>']\n",
            "-Generative Adversarial Text to Image Synthesis ['<ul>', '<li>']\n",
            "- Unsupervised Image to Image Translation with Generative Adversarial Networks ['<ul>', '<li>']\n",
            "- Unpaired Image to Image Translation ['<ul>', '<li>']\n",
            "- Fast Compressed Sensing MRI Reconstruction ['<ul>', '<li>']\n",
            "- Neural Network to colorize grayscale images ['<ul>', '<li>']\n",
            "- Implementation of viterbi and forward/backward algorithms for HMM ['<ul>', '<li>']\n",
            "- Train TensorFlow neural nets with OpenStreetMap features and satellite imagery. ['<ul>', '<li>']\n",
            "- TensorFlow implementation of DeepMind's 'Human-Level Control through Deep Reinforcement Learning' with OpenAI Gym by Devsisters.com ['<ul>', '<li>']\n",
            "- For Playing Atari Ping Pong ['<ul>', '<li>']\n",
            "- For Playing Frozen Lake Game ['<ul>', '<li>']\n",
            "- Actor Critic for Playing Discrete Action space Game (Cartpole) ['<ul>', '<li>']\n",
            "- Asynchronous Advantage Actor Critic (A3C) for Continuous Action Space (Bipedal Walker) ['<ul>', '<li>']\n",
            "- For Playing ['<ul>', '<li>']\n",
            "- For Continuous and Discrete Action Space by ['<ul>', '<li>']\n",
            "- TensorFlow implementation of  with a ['<ul>', '<li>']\n",
            "- TensorFlow implementation of ['<ul>', '<li>']\n",
            "- TensorFlow implementation of 'YOLO: Real-Time Object Detection', with training and an actual support for real-time running on mobile devices. ['<ul>', '<li>']\n",
            "- This is a TensorFlow implementation of the  for audio generation. ['<ul>', '<li>']\n",
            "- Tensorflow implementation of ['<ul>', '<li>']\n",
            "- Tensorflow implementation for MIT  by Vondrick et al. ['<ul>', '<li>']\n",
            "- Implementation of  in TensorFlow by Torfi et al. ['<ul>', '<li>']\n",
            "- For Brain Tumor Segmentation ['<ul>', '<li>']\n",
            "- Learn the Transformation Function ['<ul>', '<li>']\n",
            "- TensorFlow Implementation of  by Torfi et al. ['<ul>', '<li>']\n",
            "- A simple embedding based text classifier inspired by Facebook's fastText. ['<ul>', '<li>']\n",
            "- Classify music genre from a 10 second sound stream using a Neural Network. ['<ul>', '<li>']\n",
            "- Framework for easily using Tensorflow with Kubernetes. ['<ul>', '<li>']\n",
            "- 40+ Popular Computer Vision Models With Pre-trained Weights. ['<ul>', '<li>']\n",
            "- Implementation of Ladder Network for Semi-Supervised Learning in Keras and Tensorflow ['<ul>', '<li>']\n",
            "* [TF-Unet](https://github.com/juniorxsound/TF-Unet) - General purpose U-Network implemented in Keras for image segmentation * [Sarus TF2 Models](https://github.com/sarus-tech/tf2-published-models) - A long list of recent generative models implemented in clean, easy to reuse, Tensorflow 2 code (Plain Autoencoder, VAE, VQ-VAE, PixelCNN, Gated PixelCNN, PixelCNN++, PixelSNAIL, Conditional Neural Processes). ['<li>', '<h2>']\n",
            "Powered by TensorFlow ['<li>', '<h2>']\n",
            "- Implementation of 'YOLO : Real-Time Object Detection' ['<ul>', '<li>']\n",
            "- Real-time object detection on Android using the YOLO network, powered by TensorFlow. ['<ul>', '<li>']\n",
            "- Research project to advance the state of the art in machine intelligence for music and art generation ['<ul>', '<li>']\n",
            "Libraries ['<li>', '<h2>']\n",
            "- high-level TensorFlow API that greatly simplifies machine learning programming (originally ) ['<ul>', '<li>']\n",
            "- R interface to TensorFlow APIs, including Estimators, Keras, Datasets, etc. ['<ul>', '<li>']\n",
            "- Implementation of Monotonic Calibrated Interpolated Look-Up Tables in TensorFlow ['<ul>', '<li>']\n",
            "- TensorFlow native interface for ruby using SWIG ['<ul>', '<li>']\n",
            "- Deep learning library featuring a higher-level API ['<ul>', '<li>']\n",
            "- Deep learning and reinforcement learning library for researchers and engineers ['<ul>', '<li>']\n",
            "- High-level library for defining models ['<ul>', '<li>']\n",
            "- TensorFlow binding for Apache Spark ['<ul>', '<li>']\n",
            "- TensorForce: A TensorFlow library for applied reinforcement learning ['<ul>', '<li>']\n",
            "- initiative from Yahoo! to enable distributed TensorFlow with Apache Spark. ['<ul>', '<li>']\n",
            "- Convert Caffe models to TensorFlow format ['<ul>', '<li>']\n",
            "- Minimal, modular deep learning library for TensorFlow and Theano ['<ul>', '<li>']\n",
            "- A TensorFlow implementation of the models described in ['<ul>', '<li>']\n",
            "- Run Keras models (tensorflow backend) in the browser, with GPU support ['<ul>', '<li>']\n",
            "- Simple framework allowing to read-in ROOT NTuples by converting them to a Numpy array and then use them in Google Tensorflow. ['<ul>', '<li>']\n",
            "- Sonnet is DeepMind's library built on top of TensorFlow for building complex neural networks. ['<ul>', '<li>']\n",
            "- Neural Network Toolbox on TensorFlow focusing on training speed and on large datasets. ['<ul>', '<li>']\n",
            "- Layer on top of TensorFlow for doing machine learning on encrypted data ['<ul>', '<li>']\n",
            "- Convert PyTorch models to Keras (with TensorFlow backend) format ['<ul>', '<li>']\n",
            "- Convert Gluon models to Keras (with TensorFlow backend) format ['<ul>', '<li>']\n",
            "- Lightweight, cross-platform library for deploying TensorFlow Lite models to mobile devices. ['<ul>', '<li>']\n",
            "- Machine Learning on Graphs, a Python library for machine learning on graph-structured (network-structured) data. ['<ul>', '<li>']\n",
            "- High-Level Keras Complement for implement common architectures stacks, served as easy to use plug-n-play modules ['<ul>', '<li>']\n",
            "Tools/Utilities ['<li>', '<h2>']\n",
            "- Task runner and package manager for TensorFlow ['<ul>', '<li>']\n",
            "- All-in-one web IDE for machine learning and data science. Combines Tensorflow, Jupyter, VS Code, Tensorboard, and many other tools/libraries into one Docker image. ['<ul>', '<li>']\n",
            "Videos ['<li>', '<h2>']\n",
            "- A guide to installation and use ['<ul>', '<li>']\n",
            "- Continuation of first video ['<ul>', '<li>']\n",
            "- A guide going over basic usage ['<ul>', '<li>']\n",
            "- Goes over Deep MNIST ['<ul>', '<li>']\n",
            "- Basic steps to install TensorFlow for free on the Cloud 9 online service with 1Gb of data ['<ul>', '<li>']\n",
            "- CS224d Deep Learning for Natural Language Processing by Richard Socher ['<ul>', '<li>']\n",
            "- Pycon 2016 Portland Oregon,  &amp;  by Julia Ferraioli, Amy Unruh, Eli Bixby ['<ul>', '<li>']\n",
            "- Spark Summit 2016 Keynote by Jeff Dean ['<ul>', '<li>']\n",
            "-  by Martin Görner ['<ul>', '<li>']\n",
            "-  by Alex Pliutau ['<ul>', '<li>']\n",
            "Papers ['<li>', '<h2>']\n",
            "- This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google ['<ul>', '<li>']\n",
            "- The study is performed on several types of deep learning architectures and we evaluate the performance of the above frameworks when employed on a single machine for both (multi-threaded) CPU and GPU (Nvidia Titan X) settings ['<ul>', '<li>']\n",
            "- In this paper, we extend recently proposed Google TensorFlow for execution on large scale clusters using Message Passing Interface (MPI) ['<ul>', '<li>']\n",
            "- This paper describes the models behind . ['<ul>', '<li>']\n",
            "- This paper describes the TensorFlow dataflow model in contrast to existing systems and demonstrate the compelling performance ['<ul>', '<li>']\n",
            "- This paper describes a versatile Python library that aims at helping researchers and engineers efficiently develop deep learning systems. (Winner of The Best Open Source Software Award of ACM MM 2017) ['<ul>', '<li>']\n",
            "Official announcements ['<li>', '<h2>']\n",
            "- An introduction to TensorFlow ['<ul>', '<li>']\n",
            "- Release of SyntaxNet, \"an open-source neural network framework implemented in TensorFlow that provides a foundation for Natural Language Understanding systems. ['<ul>', '<li>']\n",
            "- Goes over the implementation of TensorFlow ['<ul>', '<li>']\n",
            "- Key Features Illustrated ['<ul>', '<li>']\n",
            "- Understanding the Internals of TensorFlow Learn Estimators ['<ul>', '<li>']\n",
            "- A survey of six months rapid evolution (+ tips/hacks and code to fix the ugly stuff), Dan Kuster at Indico, May 9, 2016 ['<ul>', '<li>']\n",
            "- A joke by Joel Grus ['<ul>', '<li>']\n",
            "- Step-by-step guide with full code examples on GitHub. ['<ul>', '<li>']\n",
            "semantic segmentation and handling the TFRecord file format. ['<ul>', '<li>']\n",
            "- Android TensorFlow Machine Learning Example. ['<ul>', '<li>']\n",
            "- Introduces TensorFlow optimizations on Intel® Xeon® and Intel® Xeon Phi™ processor-based platforms based on an Intel/Google collaboration. ['<ul>', '<li>']\n",
            "Coca-Cola's product code image recognizing neural network with user input feedback loop. ['<ul>', '<li>']\n",
            "How Does The Machine Learning Library TensorFlow Work? ['<ul>', '<li>']\n",
            "Community ['<li>', '<h2>']\n",
            "Books ['<li>', '<h2>']\n",
            "by Nishant Shukla, computer vision researcher at UCLA and author of Haskell Data Analysis Cookbook. This book makes the math-heavy topic of ML approachable and practicle to a newcomer. ['<ul>', '<li>']\n",
            "by Jordi Torres, professor at UPC Barcelona Tech and a research manager and senior advisor at Barcelona Supercomputing Center ['<ul>', '<li>']\n",
            "- Develop Deep Learning Models on Theano and TensorFlow Using Keras by Jason Brownlee ['<ul>', '<li>']\n",
            "- Complete guide to use TensorFlow from the basics of graph computing, to deep learning models to using it in production environments - Bleeding Edge Press ['<ul>', '<li>']\n",
            "- Get up and running with the latest numerical computing library by Google and dive deeper into your data, by Giancarlo Zaccone ['<ul>', '<li>']\n",
            "– by Aurélien Geron, former lead of the YouTube video classification team. Covers ML fundamentals, training and deploying deep nets across multiple servers and GPUs using TensorFlow, the latest CNN, RNN and Autoencoder architectures, and Reinforcement Learning (Deep Q). ['<ul>', '<li>']\n",
            "– by Rodolfo Bonnin. This book covers various projects in TensorFlow that expose what can be done with TensorFlow in different scenarios. The book provides projects on training models, machine learning, deep learning, and working with various neural networks. Each project is an engaging and insightful exercise that will teach you how to use TensorFlow and show you how layers of data can be explored by working with Tensors. ['<ul>', '<li>']\n",
            "- by Hao Dong et al. This book covers both deep learning and the implmentation by using TensorFlow and TensorLayer. ['<ul>', '<li>']\n",
            "Contributions ['<li>', '<h2>', '<p>']\n",
            "Your contributions are always welcome! ['<li>', '<h2>', '<p>']\n",
            "Repository's owner explicitly say that \"this library is not maintained\". ['<ul>', '<li>']\n",
            "Not committed for long time (2~3 years). ['<ul>', '<li>']\n",
            "Credits ['<li>', '<h2>']\n",
            "The few go reference I found where pulled from ['<ul>', '<li>']\n",
            "Directory ['<code>', '<br/>']\n",
            "Description ['<div highlight highlight-source-shell\">', '<code>']\n",
            "• A collection of example implementations for SOTA models using the latest TensorFlow 2's high-level APIs• Officially maintained, supported, and kept up to date with the latest TensorFlow 2 APIs by TensorFlow• Reasonably optimized for fast performance while still being easy to read ['<code>', '<br/>']\n",
            "• A collection of research model implementations in TensorFlow 1 or 2 by researchers• Maintained and supported by researchers ['<code>', '<br/>']\n",
            "• A curated list of the GitHub repositories with machine learning models and implementations powered by TensorFlow 2 ['<code>', '<br/>']\n",
            "• A flexible and lightweight library that users can easily use or fork when writing customized training loop code in TensorFlow 2.x. It seamlessly integrates with tf.distribute and supports running on different device types (CPU, GPU, and TPU). ['<code>', '<br/>']\n",
            "News ['<code>']\n",
            "July 10, 2020 ['<code>']\n",
            "TensorFlow 2 meets the  () ['<code>']\n",
            "June 30, 2020 ['<code>']\n",
            "released () ['<code>']\n",
            "June 17, 2020 ['<code>']\n",
            "May 21, 2020 ['<code>']\n",
            "code released ['<code>']\n",
            "May 19, 2020 ['<code>']\n",
            "released ['<code>']\n",
            "May 7, 2020 ['<code>']\n",
            "released for object detection ['<code>']\n",
            "May 1, 2020 ['<code>']\n",
            "updated to support TensorFlow 2.1 ['<code>']\n",
            "March 31, 2020 ['<code>']\n",
            "for the  class on Udacity ['<ul>', '<li>']\n",
            "If you are looking to learn TensorFlow, don't miss the ['<p>']\n",
            "which is largely runnable code. Those notebooks can be opened in Colab from ['<p>']\n",
            "Showcase examples and documentation for our fantastic ['<ul>', '<li>']\n",
            "Provide examples mentioned on TensorFlow.org ['<ul>', '<li>']\n",
            "Publish material supporting official TensorFlow courses ['<ul>', '<li>']\n",
            "Publish supporting material for the  and ['<ul>', '<li>']\n",
            "We welcome community contributions, see  and, for style help, ['<p>']\n",
            "guide. ['<p>']\n",
            "To contribute to the TensorFlow documentation, please read ['<p>']\n",
            ", the ['<p>']\n",
            ", and the . ['<p>']\n",
            "are located in the ['<p>', '<em>']\n",
            "repo. These docs are contributed, reviewed, and maintained by the community as best-effort. To participate as a translator or reviewer, see the site/&lt;lang&gt;/README.md, join the language mailing list, and submit a pull request. ['<p>', '<em>']\n",
            "wrote an excellent  and  based on this repo. ['<ul>', '<li>']\n",
            "To avoid the fast convergence of D (discriminator) network, G (generator) network is updated twice for each D network update, which differs from original paper. ['<ul>', '<li>']\n",
            "Python 2.7 or Python 3.3+ ['<ul>', '<li>']\n",
            "(Optional)  (for visualization) ['<ul>', '<li>']\n",
            "(Optional)  : Large-scale CelebFaces Dataset ['<ul>', '<li>']\n",
            "$ python main.py --dataset mnist --input_height=28 --output_height=28 --train $ python main.py --dataset celebA --input_height=108 --train --crop ['<pre>']\n",
            "$ python main.py --dataset mnist --input_height=28 --output_height=28 $ python main.py --dataset celebA --input_height=108 --crop ['<pre>']\n",
            "$ mkdir data/DATASET_NAME ... add images to data/DATASET_NAME ... $ python main.py --dataset DATASET_NAME --train $ python main.py --dataset DATASET_NAME $ # example $ python main.py --dataset=eyes --input_fname_pattern=\"*_cropped.png\" --train ['<pre>']\n",
            "$ python main.py --dataset DATASET_NAME --data_dir DATASET_ROOT_DIR --train $ python main.py --dataset DATASET_NAME --data_dir DATASET_ROOT_DIR $ # example $ python main.py --dataset=eyes --data_dir ../datasets/ --input_fname_pattern=\"*_cropped.png\" --train ['<pre>']\n",
            "This repository contains code examples for the course CS 20: TensorFlow for Deep Learning Research.  It will be updated as the class progresses.  Detailed syllabus and lecture notes can be found . For this course, I use python3.6 and TensorFlow 1.4.1. ['<p>']\n",
            "1、基础 ['<p>', '<br/>']\n",
            "2、自然语言相关 ['<p>', '<br/>']\n",
            "3、强化学习相关 ['<p>', '<br/>']\n",
            "4、推荐系统 ['<p>', '<br/>']\n",
            "5、GAN ['<p>', '<br/>']\n",
            "Here we introduce TensorFlow and the general outline of how most TensorFlow algorithms work. ['<ul>', '<li>']\n",
            "How to create and initialize tensors in TensorFlow.  We also depict how these operations appear in Tensorboard. ['<ul>', '<li>']\n",
            "How to create and use variables and placeholders in TensorFlow.  We also depict how these operations appear in Tensorboard. ['<ul>', '<li>']\n",
            "Understanding how TensorFlow can work with matrices is crucial to understanding how the algorithms work. ['<ul>', '<li>']\n",
            "How to use various mathematical operations in TensorFlow. ['<ul>', '<li>']\n",
            "Activation functions are unique functions that TensorFlow has built in for your use in algorithms. ['<ul>', '<li>']\n",
            "Here we show how to access all the various required data sources in the book.  There are also links describing the data sources and where they come from. ['<ul>', '<li>']\n",
            "Mostly official resources and papers.  The papers are TensorFlow papers or Deep Learning resources. ['<ul>', '<li>']\n",
            "We show how to create an operation on a computational graph and how to visualize it using Tensorboard. ['<ul>', '<li>']\n",
            "We show how to create multiple operations on a computational graph and how to visualize them using Tensorboard. ['<ul>', '<li>']\n",
            "Here we extend the usage of the computational graph to create multiple layers and show how they appear in Tensorboard. ['<ul>', '<li>']\n",
            "In order to train a model, we must be able to evaluate how well it is doing. This is given by loss functions. We plot various loss functions and talk about the benefits and limitations of some. ['<ul>', '<li>']\n",
            "Here we show how to use loss functions to iterate through data and back propagate errors for regression and classification. ['<ul>', '<li>']\n",
            "TensorFlow makes it easy to use both batch and stochastic training. We show how to implement both and talk about the benefits and limitations of each. ['<ul>', '<li>']\n",
            "We now combine everything together that we have learned and create a simple classifier. ['<ul>', '<li>']\n",
            "Any model is only as good as it's evaluation.  Here we show two examples of (1) evaluating a regression algorithm and (2) a classification algorithm. ['<ul>', '<li>']\n",
            "How to solve a 2D regression with a matrix inverse in TensorFlow. ['<ul>', '<li>']\n",
            "Solving a 2D linear regression with Cholesky decomposition. ['<ul>', '<li>']\n",
            "Linear regression iterating through a computational graph with L2 Loss. ['<ul>', '<li>']\n",
            "L2 vs L1 loss in linear regression.  We talk about the benefits and limitations of both. ['<ul>', '<li>']\n",
            "Deming (total) regression implemented in TensorFlow by changing the loss function. ['<ul>', '<li>']\n",
            "Lasso and Ridge regression are ways of regularizing the coefficients. We implement both of these in TensorFlow via changing the loss functions. ['<ul>', '<li>']\n",
            "Elastic net is a regularization technique that combines the L2 and L1 loss for coefficients.  We show how to implement this in TensorFlow. ['<ul>', '<li>']\n",
            "We implement logistic regression by the use of an activation function in our computational graph. ['<ul>', '<li>']\n",
            "We introduce the concept of SVMs and how we will go about implementing them in the TensorFlow framework. ['<ul>', '<li>']\n",
            "We create a linear SVM to separate I. setosa based on sepal length and pedal width in the Iris data set. ['<ul>', '<li>']\n",
            "The heart of SVMs is separating classes with a line.  We change tweek the algorithm slightly to perform SVM regression. ['<ul>', '<li>']\n",
            "In order to extend SVMs into non-linear data, we explain and show how to implement different kernels in TensorFlow. ['<ul>', '<li>']\n",
            "We use the Gaussian kernel (RBF) to separate non-linear classes. ['<ul>', '<li>']\n",
            "SVMs are inherently binary predictors.  We show how to extend them in a one-vs-all strategy in TensorFlow. ['<ul>', '<li>']\n",
            "We introduce the concepts and methods needed for performing k-Nearest Neighbors in TensorFlow. ['<ul>', '<li>']\n",
            "We create a nearest neighbor algorithm that tries to predict housing worth (regression). ['<ul>', '<li>']\n",
            "In order to use a distance function on text, we show how to use edit distances in TensorFlow. ['<ul>', '<li>']\n",
            "Here we implement scaling of the distance function by the standard deviation of the input feature for k-Nearest Neighbors. ['<ul>', '<li>']\n",
            "We use a mixed distance function to match addresses. We use numerical distance for zip codes, and string edit distance for street names. The street names are allowed to have typos. ['<ul>', '<li>']\n",
            "The MNIST digit image collection is a great data set for illustration of how to perform k-Nearest Neighbors for an image classification task. ['<ul>', '<li>']\n",
            "We introduce the concept of neural networks and how TensorFlow is built to easily handle these algorithms. ['<ul>', '<li>']\n",
            "We implement an operational gate with one operation. Then we show how to extend this to multiple nested operations. ['<ul>', '<li>']\n",
            "Now we have to introduce activation functions on the gates.  We show how different activation functions operate. ['<ul>', '<li>']\n",
            "We have all the pieces to start implementing our first neural network.  We do so here with regression on the Iris data set. ['<ul>', '<li>']\n",
            "This section introduces the convolution layer and the max-pool layer.  We show how to chain these together in a 1D and 2D example with fully connected layers as well. ['<ul>', '<li>']\n",
            "Here we show how to functionalize different layers and variables for a cleaner multi-layer neural network. ['<ul>', '<li>']\n",
            "We show how we can improve the convergence of our prior logistic regression with a set of hidden layers. ['<ul>', '<li>']\n",
            "Given a set of tic-tac-toe boards and corresponding optimal moves, we train a neural network classification model to play.  At the end of the script, you can attempt to play against the trained model. ['<ul>', '<li>']\n",
            "We introduce methods for turning text into numerical vectors. We introduce the TensorFlow 'embedding' feature as well. ['<ul>', '<li>']\n",
            "Here we use TensorFlow to do a one-hot-encoding of words called bag-of-words.  We use this method and logistic regression to predict if a text message is spam or ham. ['<ul>', '<li>']\n",
            "We implement Text Frequency - Inverse Document Frequency (TFIDF) with a combination of Sci-kit Learn and TensorFlow. We perform logistic regression on TFIDF vectors to improve on our spam/ham text-message predictions. ['<ul>', '<li>']\n",
            "Our first implementation of Word2Vec called, \"skip-gram\" on a movie review database. ['<ul>', '<li>']\n",
            "Next, we implement a form of Word2Vec called, \"CBOW\" (Continuous Bag of Words) on a movie review database.  We also introduce method to saving and loading word embeddings. ['<ul>', '<li>']\n",
            "In this example, we use the prior saved CBOW word embeddings to improve on our TF-IDF logistic regression of movie review sentiment. ['<ul>', '<li>']\n",
            "Here, we introduce a Doc2Vec method (concatenation of doc and word embeddings) to improve out logistic model of movie review sentiment. ['<ul>', '<li>']\n",
            "We introduce convolutional neural networks (CNN), and how we can use them in TensorFlow. ['<ul>', '<li>']\n",
            "Here, we show how to create a CNN architecture that performs well on the MNIST digit recognition task. ['<ul>', '<li>']\n",
            "In this example, we show how to replicate an architecture for the CIFAR-10 image recognition task. ['<ul>', '<li>']\n",
            "We show how to download and setup the CIFAR-10 data for the TensorFlow retraining/fine-tuning tutorial. ['<ul>', '<li>']\n",
            "In this recipe, we show a basic implementation of using Stylenet or Neuralstyle. ['<ul>', '<li>']\n",
            "This script shows a line-by-line explanation of TensorFlow's deepdream tutorial. Taken from . Note that the code here is converted to Python 3. ['<ul>', '<li>']\n",
            "We introduce Recurrent Neural Networks and how they are able to feed in a sequence and predict either a fixed target (categorical/numerical) or another sequence (sequence to sequence). ['<ul>', '<li>']\n",
            "In this example, we create an RNN model to improve on our spam/ham SMS text predictions. ['<ul>', '<li>']\n",
            "We show how to implement a LSTM (Long Short Term Memory) RNN for Shakespeare language generation. (Word level vocabulary) ['<ul>', '<li>']\n",
            "We stack multiple LSTM layers to improve on our Shakespeare language generation. (Character level vocabulary) ['<ul>', '<li>']\n",
            "Here, we use TensorFlow's sequence-to-sequence models to train an English-German translation model. ['<ul>', '<li>']\n",
            "Here, we implement a Siamese RNN to predict the similarity of addresses and use it for record matching.  Using RNNs for record matching is very versatile, as we do not have a fixed set of target categories and can use the trained model to predict similarities across new addresses. ['<ul>', '<li>']\n",
            "We show how to implement different types of unit tests on tensors (placeholders and variables). ['<ul>', '<li>']\n",
            "How to use a machine with multiple devices.  E.g., a machine with a CPU, and one or more GPUs. ['<ul>', '<li>']\n",
            "How to setup and use TensorFlow distributed on multiple machines. ['<ul>', '<li>']\n",
            "Various tips for developing with TensorFlow ['<ul>', '<li>']\n",
            "We show how to do take the RNN model for predicting ham/spam (from Chapter 9, recipe #2) and put it in two production level files: training and evaluation. ['<ul>', '<li>']\n",
            "An example of using histograms, scalar summaries, and creating images in Tensorboard. ['<ul>', '<li>']\n",
            "We create a genetic algorithm to optimize an individual (array of 50 numbers) toward the ground truth function. ['<ul>', '<li>']\n",
            "How to use TensorFlow to do k-means clustering.  We use the Iris data set, set k=3, and use k-means to make predictions. ['<ul>', '<li>']\n",
            "Here, we show how to use TensorFlow to solve a system of ODEs.  The system of concern is the Lotka-Volterra predator-prey system. ['<ul>', '<li>']\n",
            "We illustrate how to use TensorFlow's gradient boosted regression and classification trees. ['<ul>', '<li>']\n",
            "Here we show how to use the Keras sequential model building for a fully connected neural network and a CNN model with callbacks. ['<ul>', '<li>']\n",
            "These tutorials are intended for beginners in Deep Learning and TensorFlow. ['<ul>', '<li>']\n",
            "Each tutorial covers a single topic. ['<ul>', '<li>']\n",
            "The source-code is well-documented. ['<ul>', '<li>']\n",
            "There is a  for each tutorial. ['<ul>', '<li>']\n",
            "Simple Linear Model () () ['<ol>', '<li>', '<p>']\n",
            "Convolutional Neural Network () () ['<ol>', '<li>', '<p>']\n",
            "3-C. Keras API () () ['<p>']\n",
            "Fine-Tuning () () ['<ol start=\"10\">', '<li>']\n",
            "13-B. Visual Analysis for MNIST () () ['<p>']\n",
            "Reinforcement Learning () () ['<ol start=\"16\">', '<li>', '<p>']\n",
            "Hyper-Parameter Optimization () () ['<ol start=\"16\">', '<li>', '<p>']\n",
            "Natural Language Processing () () ['<ol start=\"16\">', '<li>', '<p>']\n",
            "Machine Translation () () ['<ol start=\"16\">', '<li>', '<p>']\n",
            "Image Captioning () () ['<ol start=\"16\">', '<li>', '<p>']\n",
            "Time-Series Prediction () () ['<ol start=\"16\">', '<li>', '<p>']\n",
            "The following tutorials only work with the older TensorFlow 1 API, so you would need to install an older version of TensorFlow to run these. It would take too much time and effort to convert these tutorials to TensorFlow 2. ['<p>']\n",
            "Pretty Tensor () () ['<ol start=\"3\">', '<li>']\n",
            "3-B. Layers API () () ['<p>']\n",
            "Save &amp; Restore () () ['<ol start=\"4\">', '<li>', '<p>']\n",
            "Ensemble Learning () () ['<ol start=\"4\">', '<li>', '<p>']\n",
            "CIFAR-10 () () ['<ol start=\"4\">', '<li>', '<p>']\n",
            "Inception Model () () ['<ol start=\"4\">', '<li>', '<p>']\n",
            "Transfer Learning () () ['<ol start=\"4\">', '<li>', '<p>']\n",
            "Video Data () () ['<ol start=\"4\">', '<li>', '<p>']\n",
            "Adversarial Examples () () ['<ol start=\"4\">', '<li>', '<p>']\n",
            "Adversarial Noise for MNIST () () ['<ol start=\"4\">', '<li>', '<p>']\n",
            "Visual Analysis () () ['<ol start=\"4\">', '<li>', '<p>']\n",
            "DeepDream () () ['<ol start=\"4\">', '<li>', '<p>']\n",
            "Style Transfer () () ['<ol start=\"4\">', '<li>', '<p>']\n",
            "Estimator API () () ['<ol start=\"4\">', '<li>', '<p>']\n",
            "TFRecords &amp; Dataset API () () ['<ol start=\"4\">', '<li>', '<p>']\n",
            "If you are new to using Python and Linux then this may be challenging to get working and you may need to do internet searches for error-messages, etc. It will get easier with practice. You can also run the tutorials without installing anything by using Google Colab, see further below. ['<p>']\n",
            "Some of the Python Notebooks use source-code located in different files to allow for easy re-use across multiple tutorials. It is therefore recommended that you download the whole repository from GitHub, instead of just downloading the individual Python Notebooks. ['<p>']\n",
            "I use  because it comes with many Python packages already installed and it is easy to work with. After installing Anaconda, you should create a  so you do not destroy your main installation in case you make a mistake somewhere: ['<p>']\n",
            "When Python gets updated to a new version, it takes a while before TensorFlow also uses the new Python version. So if the TensorFlow installation fails, then you may have to specify an older Python version for your new environment, such as: ['<p>']\n",
            "To install the required Python packages and dependencies you first have to activate the conda-environment as described above, and then you run the following command in a terminal: ['<p>']\n",
            "Starting with TensorFlow 2.1 it includes both the CPU and GPU versions and will automatically switch if you have a GPU. But this requires the installation of various NVIDIA drivers, which is a bit complicated and is not described here. ['<p>']\n",
            "cd ~/development/TensorFlow-Tutorials/  # Your installation directory. jupyter notebook ['<pre>']\n",
            "If you do not want to install anything on your own computer, then the Notebooks can be viewed, edited and run entirely on the internet by using ['<p>']\n",
            ". There is a ['<p>']\n",
            "explaining how to do this. You click the \"Google Colab\"-link next to each tutorial listed above. You can view the Notebook on Colab but in order to run it you need to login using your Google account. Then you need to execute the following commands at the top of the Notebook, which clones the contents of this repository to your work-directory on Colab. ['<p>']\n",
            "# Clone the repository from GitHub to Google Colab's temporary drive. import os work_dir = \"/content/TensorFlow-Tutorials/\" if not os.path.exists(work_dir):     !git clone https://github.com/Hvass-Labs/TensorFlow-Tutorials.git os.chdir(work_dir) ['<pre>']\n",
            "If you want to see the exact versions of the source-code that were used in the YouTube videos, then you can  of commits to the GitHub repository. ['<p>']\n",
            "First of all, what's the point of putting effort into something that most of the people won't stop by and take a look? What's the point of creating something that does not help anyone in the developers and researchers community? Why spend time for something that can easily be forgotten? But how we try to do it? Even up to this very moment there are countless tutorials on TensorFlow whether on the model design or TensorFlow workflow. ['<p>']\n",
            "topic ['<em>']\n",
            "Run ['<div align=\"left\">', '<em>']\n",
            "Source Code ['<em>']\n",
            "Media ['<div align=\"left\">', '<em>']\n",
            "1 ['<p>', '<code>']\n",
            "Start-up ['<li>']\n",
            "Tensors ['<div align=\"left\">']\n",
            "2 ['<p>', '<code>']\n",
            "Automatic Differentiation ['<div align=\"left\">']\n",
            "Introduction to Graphs ['<div align=\"left\">']\n",
            "4 ['<em>']\n",
            "TensorFlow Models ['<div align=\"left\">']\n",
            "More ['<div align=\"left\">']\n",
            "Linear Regression ['<ul>', '<li>']\n",
            "Data Augmentation ['<div align=\"left\">']\n",
            "Multi Layer Perceptron ['<em>']\n",
            "Convolutional Neural Networks ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Custom Training ['<div align=\"left\">', '<em>']\n",
            "Dataset Generator ['<div align=\"left\">', '<em>']\n",
            "Create TFRecords ['<div align=\"left\">', '<em>']\n",
            "When contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change. For typos, please do not create a pull request. Instead, declare them in issues or email the repository owner. ['<p>']\n",
            "The pull request is mainly expected to be a code script suggestion or improvement. ['<blockquote>', '<ul>', '<li>']\n",
            "Please do NOT change the ipython files. Instead, change the corresponsing PYTHON files. ['<blockquote>', '<ul>', '<li>']\n",
            "A pull request related to non-code-script sections is expected to make a significant difference in the documentation. Otherwise, it is expected to be announced in the issues section. ['<blockquote>', '<ul>', '<li>']\n",
            "Ensure any install or build dependencies are removed before the end of the layer when doing a build and creating a pull request. ['<blockquote>', '<ul>', '<li>']\n",
            "Add comments with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters. ['<blockquote>', '<ul>', '<li>']\n",
            "You may merge the Pull Request in once you have the sign-off of at least one other developer, or if you do not have permission to do that, you may request the owner to merge it for you if you believe all checks are passed. ['<blockquote>', '<ul>', '<li>']\n",
            "We are looking forward to your kind feedback. Please help us to improve this open source project and make our work better. For contribution, please create a pull request and we will investigate it promptly. Once again, we appreciate your kind feedback and elaborate code inspections. ['<p>']\n",
            "Update ['<p>']\n",
            "2018-04-10 ['<p>']\n",
            "Added new models trained on Casia-WebFace and VGGFace2 (see below). Note that the models uses fixed image standardization (see ). ['<p>']\n",
            "2018-03-31 ['<p>']\n",
            "Added a new, more flexible input pipeline as well as a bunch of minor updates. ['<p>']\n",
            "2017-05-13 ['<p>']\n",
            "Removed a bunch of older non-slim models. Moved the last bottleneck layer into the respective models. Corrected normalization of Center Loss. ['<p>']\n",
            "2017-05-06 ['<p>']\n",
            "Added code to . Renamed facenet_train.py to train_tripletloss.py and facenet_train_classifier.py to train_softmax.py. ['<p>']\n",
            "2017-03-02 ['<p>']\n",
            "Added pretrained models that generate 128-dimensional embeddings. ['<p>']\n",
            "2017-02-22 ['<p>']\n",
            "Updated to Tensorflow r1.0. Added Continuous Integration using Travis-CI. ['<p>']\n",
            "2017-02-03 ['<p>']\n",
            "Added models where only trainable variables has been stored in the checkpoint. These are therefore significantly smaller. ['<p>']\n",
            "2017-01-27 ['<p>']\n",
            "Added a model trained on a subset of the MS-Celeb-1M dataset. The LFW accuracy of this model is around 0.994. ['<p>']\n",
            "2017‑01‑02 ['<p>']\n",
            "Updated to run with Tensorflow r0.12. Not sure if it runs with older versions of Tensorflow though. ['<p>']\n",
            "Model name ['<p>']\n",
            "LFW accuracy ['<p>']\n",
            "Training dataset ['<p>']\n",
            "Architecture ['<p>']\n",
            "0.9905 ['<p>']\n",
            "CASIA-WebFace ['<p>']\n",
            "0.9965 ['<p>']\n",
            "VGGFace2 ['<p>']\n",
            "One problem with the above approach seems to be that the Dlib face detector misses some of the hard examples (partial occlusion, silhouettes, etc). This makes the training set too \"easy\" which causes the model to perform worse on other benchmarks. To solve this, other face landmark detectors has been tested. One face landmark detector that has proven to work very well in this setting is the ['<p>']\n",
            ". A Matlab/Caffe implementation can be found  and this has been used for face alignment with very good results. A Python/Tensorflow implementation of MTCNN can be found . This implementation does not give identical results to the Matlab/Caffe implementation but the performance is very similar. ['<p>']\n",
            "datasets: interface to popular datasets (Pascal VOC, COCO, ...) and scripts to convert the former to TF-Records; ['<ul>', '<li>']\n",
            "networks: definition of SSD networks, and common encoding and decoding methods (we refer to the paper on this precise topic); ['<ul>', '<li>']\n",
            "pre-processing: pre-processing and data augmentation routines, inspired by original VGG and Inception implementations. ['<ul>', '<li>']\n",
            "Here are two examples of successful detection outputs: ['<p>']\n",
            "DATASET_DIR=./VOC2007/test/ OUTPUT_DIR=./tfrecords python tf_convert_data.py \\     --dataset_name=pascalvoc \\     --dataset_dir=${DATASET_DIR} \\     --output_name=voc_2007_train \\     --output_dir=${OUTPUT_DIR} ['<div highlight highlight-source-shell\">']\n",
            "Model ['<p>']\n",
            "Training data ['<div highlight highlight-source-shell\">']\n",
            "Testing data ['<div highlight highlight-source-shell\">']\n",
            "mAP ['<div highlight highlight-source-shell\">']\n",
            "FPS ['<div highlight highlight-source-shell\">']\n",
            "VOC07+12 trainval ['<div highlight highlight-source-shell\">']\n",
            "VOC07 test ['<div highlight highlight-source-shell\">']\n",
            "0.778 ['<div highlight highlight-source-shell\">']\n",
            "VOC07+12+COCO trainval ['<div highlight highlight-source-shell\">']\n",
            "0.817 ['<div highlight highlight-source-shell\">']\n",
            "0.837 ['<div highlight highlight-source-shell\">']\n",
            "EVAL_DIR=./logs/ CHECKPOINT_PATH=./checkpoints/VGG_VOC0712_SSD_300x300_ft_iter_120000.ckpt python eval_ssd_network.py \\     --eval_dir=${EVAL_DIR} \\     --dataset_dir=${DATASET_DIR} \\     --dataset_name=pascalvoc_2007 \\     --dataset_split_name=test \\     --model_name=ssd_300_vgg \\     --checkpoint_path=${CHECKPOINT_PATH} \\     --batch_size=1 ['<div highlight highlight-source-shell\">']\n",
            "CAFFE_MODEL=./ckpts/SSD_300x300_ft_VOC0712/VGG_VOC0712_SSD_300x300_ft_iter_120000.caffemodel python caffe_to_tensorflow.py \\     --model_name=ssd_300_vgg \\     --num_classes=21 \\     --caffemodel_path=${CAFFE_MODEL} ['<div highlight highlight-source-shell\">']\n",
            "DATASET_DIR=./tfrecords TRAIN_DIR=./logs/ CHECKPOINT_PATH=./checkpoints/ssd_300_vgg.ckpt python train_ssd_network.py \\     --train_dir=${TRAIN_DIR} \\     --dataset_dir=${DATASET_DIR} \\     --dataset_name=pascalvoc_2012 \\     --dataset_split_name=train \\     --model_name=ssd_300_vgg \\     --checkpoint_path=${CHECKPOINT_PATH} \\     --save_summaries_secs=60 \\     --save_interval_secs=600 \\     --weight_decay=0.0005 \\     --optimizer=adam \\     --learning_rate=0.001 \\     --batch_size=32 ['<div highlight highlight-source-shell\">']\n",
            "EVAL_DIR=${TRAIN_DIR}/eval python eval_ssd_network.py \\     --eval_dir=${EVAL_DIR} \\     --dataset_dir=${DATASET_DIR} \\     --dataset_name=pascalvoc_2007 \\     --dataset_split_name=test \\     --model_name=ssd_300_vgg \\     --checkpoint_path=${TRAIN_DIR} \\     --wait_for_checkpoints=True \\     --batch_size=1 \\     --max_num_batches=500 ['<div highlight highlight-source-shell\">']\n",
            "DATASET_DIR=./tfrecords TRAIN_DIR=./log/ CHECKPOINT_PATH=./checkpoints/vgg_16.ckpt python train_ssd_network.py \\     --train_dir=${TRAIN_DIR} \\     --dataset_dir=${DATASET_DIR} \\     --dataset_name=pascalvoc_2007 \\     --dataset_split_name=train \\     --model_name=ssd_300_vgg \\     --checkpoint_path=${CHECKPOINT_PATH} \\     --checkpoint_model_scope=vgg_16 \\     --checkpoint_exclude_scopes=ssd_300_vgg/conv6,ssd_300_vgg/conv7,ssd_300_vgg/block8,ssd_300_vgg/block9,ssd_300_vgg/block10,ssd_300_vgg/block11,ssd_300_vgg/block4_box,ssd_300_vgg/block7_box,ssd_300_vgg/block8_box,ssd_300_vgg/block9_box,ssd_300_vgg/block10_box,ssd_300_vgg/block11_box \\     --trainable_scopes=ssd_300_vgg/conv6,ssd_300_vgg/conv7,ssd_300_vgg/block8,ssd_300_vgg/block9,ssd_300_vgg/block10,ssd_300_vgg/block11,ssd_300_vgg/block4_box,ssd_300_vgg/block7_box,ssd_300_vgg/block8_box,ssd_300_vgg/block9_box,ssd_300_vgg/block10_box,ssd_300_vgg/block11_box \\     --save_summaries_secs=60 \\     --save_interval_secs=600 \\     --weight_decay=0.0005 \\     --optimizer=adam \\     --learning_rate=0.001 \\     --learning_rate_decay_factor=0.94 \\     --batch_size=32 ['<div highlight highlight-source-shell\">']\n",
            "DATASET_DIR=./tfrecords TRAIN_DIR=./log_finetune/ CHECKPOINT_PATH=./log/model.ckpt-N python train_ssd_network.py \\     --train_dir=${TRAIN_DIR} \\     --dataset_dir=${DATASET_DIR} \\     --dataset_name=pascalvoc_2007 \\     --dataset_split_name=train \\     --model_name=ssd_300_vgg \\     --checkpoint_path=${CHECKPOINT_PATH} \\     --checkpoint_model_scope=vgg_16 \\     --save_summaries_secs=60 \\     --save_interval_secs=600 \\     --weight_decay=0.0005 \\     --optimizer=adam \\     --learning_rate=0.00001 \\     --learning_rate_decay_factor=0.94 \\     --batch_size=32 ['<div highlight highlight-source-shell\">']\n",
            "Tensorflow basic ['<ul>', '<li>']\n",
            "Build your first network ['<ul>', '<li>']\n",
            "Advanced neural network ['<ul>', '<li>']\n",
            "Others (WIP) ['<ul>', '<li>']\n",
            "Slide: ['<ul>', '<li>']\n",
            "YouTube: ['<ul>', '<li>']\n",
            "Lec 01: 기본적인 Machine Learning의 용어와 개념 설명 ['<ul>', '<li>']\n",
            "Lab 01: (추가 예정) ['<ul>', '<li>']\n",
            "Lec 02: Simple Linear Regression ['<ul>', '<li>']\n",
            "Lab 02: Simple Linear Regression를 TensorFlow로 구현하기 ['<ul>', '<li>']\n",
            "Lec 03: Linear Regression and How to minimize cost ['<ul>', '<li>']\n",
            "Lab 03: Linear Regression and How to minimize cost를 TensorFlow로 구현하기 ['<ul>', '<li>']\n",
            "Lec 04: Multi-variable Linear Regression ['<ul>', '<li>']\n",
            "Lab 04: Multi-variable Linear Regression를 TensorFlow로 구현하기 ['<ul>', '<li>']\n",
            "Lec 05-1: Logistic Regression/Classification의 소개 ['<ul>', '<li>']\n",
            "Lec 05-2: Logistic Regression/Classification의 cost 함수, 최소화 ['<ul>', '<li>']\n",
            "Lab 05-3: Logistic Regression/Classification를 TensorFlow로 구현하기 ['<ul>', '<li>']\n",
            "Lec 06-1: Softmax Regression: 기본 개념 소개 ['<ul>', '<li>']\n",
            "Lec 06-2: Softmax Classifier의 cost 함수 ['<ul>', '<li>']\n",
            "Lab 06-1: Softmax classifier를 TensorFlow로 구현하기 ['<ul>', '<li>']\n",
            "Lab 06-2: Fancy Softmax classifier를 TensorFlow로 구현하기 ['<ul>', '<li>']\n",
            "Lab 07-1: Application &amp; Tips: 학습률(Learning Rate)과 데이터 전처리(Data Preprocessing) ['<ul>', '<li>']\n",
            "Lab 07-2-1: Application &amp; Tips: 오버피팅(Overfitting) &amp; Solutions ['<ul>', '<li>']\n",
            "Lab 07-2-2: Application &amp; Tips: 학습률, 전처리, 오버피팅을 TensorFlow로 실습 ['<ul>', '<li>']\n",
            "Lab 07-3-1: Application &amp; Tips: Data &amp; Learning ['<ul>', '<li>']\n",
            "Lab 07-3-2: Application &amp; Tips: 다양한 Dataset으로 실습 ['<ul>', '<li>']\n",
            "Lec 08-1: 딥러닝의 기본 개념: 시작과 XOR 문제 ['<ul>', '<li>']\n",
            "Lec 08-2: 딥러닝의 기본 개념 2: Back-propagation 과 2006/2007 '딥'의 출현 ['<ul>', '<li>']\n",
            "Lec 09-1: XOR 문제 딥러닝으로 풀기 ['<ul>', '<li>']\n",
            "Lec 09-2: 딥넷트웍 학습 시키기 (backpropagation) ['<ul>', '<li>']\n",
            "Lab 09-1: Neural Net for XOR ['<ul>', '<li>']\n",
            "Lab 09-2: Tensorboard (Neural Net for XOR) ['<ul>', '<li>']\n",
            "Lab 10-1: Sigmoid 보다 ReLU가 더 좋아 ['<ul>', '<li>']\n",
            "Lab 10-2: Weight 초기화 잘해보자 ['<ul>', '<li>']\n",
            "Lab 10-3: Dropout ['<ul>', '<li>']\n",
            "Lab 10-4: Batch Normalization ['<ul>', '<li>']\n",
            "Lec 11-1: ConvNet의 Conv 레이어 만들기 ['<ul>', '<li>']\n",
            "Lec 11-2: ConvNet Max pooling 과 Full Network ['<ul>', '<li>']\n",
            "Lec 11-3: ConvNet의 활용 예 ['<ul>', '<li>']\n",
            "Lab 11-0-1: CNN Basic: Convolution ['<ul>', '<li>']\n",
            "Lab 11-0-2: CNN Basic: Pooling ['<ul>', '<li>']\n",
            "Lab 11-1: mnist cnn keras sequential eager ['<ul>', '<li>']\n",
            "Lab 11-2: mnist cnn keras functional eager ['<ul>', '<li>']\n",
            "Lab-11-3: mnist cnn keras subclassing eager ['<ul>', '<li>']\n",
            "Lab-11-4: mnist cnn keras ensemble eager ['<ul>', '<li>']\n",
            "Lab-11-5: mnist cnn best keras eager ['<ul>', '<li>']\n",
            "Lec 12: NN의 꽃 RNN 이야기 ['<ul>', '<li>']\n",
            "Lab 12-0: rnn basics ['<ul>', '<li>']\n",
            "Lab 12-1: many to one (word sentiment classification) ['<ul>', '<li>']\n",
            "Lab 12-2: many to one stacked (sentence classification, stacked) ['<ul>', '<li>']\n",
            "Lab 12-3: many to many (simple pos-tagger training) ['<ul>', '<li>']\n",
            "Lab 12-4: many to many bidirectional (simpled pos-tagger training, bidirectional) ['<ul>', '<li>']\n",
            "Lab 12-5: seq to seq (simple neural machine translation) ['<ul>', '<li>']\n",
            "Lab 12-6: seq to seq with attention (simple neural machine translation, attention) ['<ul>', '<li>']\n",
            "Prof. Kim () ['<ul>', '<li>']\n",
            "김보섭 () ['<ul>', '<li>']\n",
            "김수상 () ['<ul>', '<li>']\n",
            "김준호 () ['<ul>', '<li>']\n",
            "신성진 () ['<ul>', '<li>']\n",
            "이승준 () ['<ul>', '<li>']\n",
            "이진원 () ['<ul>', '<li>']\n",
            "오상준 () ['<ul>', '<li>']\n",
            "네이버 커넥트재단 : 이효은, 장지수, 임우담 ['<ul>', '<li>']\n",
            "The WaveNet neural network architecture directly generates a raw audio waveform, showing excellent results in text-to-speech and general audio generation (see the DeepMind blog post and paper for details). ['<li>', '<p>']\n",
            "The network models the conditional probability to generate the next sample in the audio waveform, given all previous samples and possibly additional parameters. ['<li>', '<p>']\n",
            "After an audio preprocessing step, the input waveform is quantized to a fixed integer range. The integer amplitudes are then one-hot encoded to produce a tensor of shape (num_samples, num_channels). ['<li>', '<p>']\n",
            "A convolutional layer that only accesses the current and previous inputs then reduces the channel dimension. ['<li>', '<p>']\n",
            "The core of the network is constructed as a stack of causal dilated layers, each of which is a dilated convolution (convolution with holes), which only accesses the current and past audio samples. ['<li>', '<p>']\n",
            "The outputs of all layers are combined and extended back to the original number of channels by a series of dense postprocessing layers, followed by a softmax function to transform the outputs into a categorical distribution. ['<li>', '<p>']\n",
            "The loss function is the cross-entropy between the output for each timestep and the input at the next timestep. ['<li>', '<p>']\n",
            "In this repository, the network implementation can be found in . ['<li>', '<p>']\n",
            "Global conditioning refers to modifying the model such that the id of a set of mutually-exclusive categories is specified during training and generation of .wav file. In the case of the VCTK, this id is the integer id of the speaker, of which there are over a hundred. This allows (indeed requires) that a speaker id be specified at time of generation to select which of the speakers it should mimic. For more details see the paper or source code. ['<p>']\n",
            "It tells the train.py script that it should build a model that includes global conditioning. ['<ul>', '<li>']\n",
            "It specifies the size of the embedding vector that is looked up based on the id of the speaker. ['<ul>', '<li>']\n",
            "Fast generation is enabled by default. It uses the implementation from the  repository. You can follow the link for an explanation of how it works. This reduces the time needed to generate samples to a few minutes. ['<p>']\n",
            "--gc_cardinality=377 is required as 376 is the largest id of a speaker in the VCTK corpus. If some other corpus is used, then this number should match what is automatically determined and printed out by the train.py script at training time. ['<p>']\n",
            ", a WaveNet for text generation. ['<ul>', '<li>']\n",
            ", a WaveNet for image generation. ['<ul>', '<li>']\n",
            "Source code ['<li>', '<strong>']\n",
            "Setup with tensorflow and graph computation. ['<li>', '<strong>']\n",
            "Performing regression with a single factor and bias. ['<li>', '<strong>']\n",
            "Performing regression using polynomial factors. ['<li>', '<strong>']\n",
            "Performing logistic regression using a single layer neural network. ['<li>', '<strong>']\n",
            "5 ['<em>']\n",
            "Building a deep convolutional neural network. ['<li>', '<strong>']\n",
            "6 ['<p>', '<code>']\n",
            "Building a deep convolutional neural network with batch normalization and leaky rectifiers. ['<li>', '<strong>']\n",
            "7 ['<em>']\n",
            "Building a deep autoencoder with tied weights. ['<li>', '<strong>']\n",
            "8 ['<em>']\n",
            "Building a deep denoising autoencoder which corrupts the input. ['<li>', '<strong>']\n",
            "9 ['<em>']\n",
            "Building a deep convolutional autoencoder. ['<li>', '<strong>']\n",
            "10 ['<em>']\n",
            "Building a deep residual network. ['<li>', '<strong>']\n",
            "11 ['<em>']\n",
            "Building an autoencoder with a variational encoding. ['<li>', '<strong>']\n",
            "import numpy as np ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "x = np.random.normal(size=[10, 10]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "y = np.random.normal(size=[10, 10]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "z = np.dot(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "print(z) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "x = tf.random.normal([10, 10]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "y = tf.random.normal([10, 10]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "z = tf.matmul(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "# Assuming we know that the desired function is a polynomial of 2nd degree, we ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "# allocate a vector of size 3 to hold the coefficients and initialize it with ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "# random noise. ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "w = tf.Variable(tf.random.normal([3, 1])) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "# We use the Adam optimizer with learning rate set to 0.1 to minimize the loss. ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "opt = tf.optimizers.Adam(0.1) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "def model(x):     # We define yhat to be our estimate of y.     f = tf.stack([tf.square(x), x, tf.ones_like(x)], 1)     yhat = tf.squeeze(tf.matmul(f, w), 1)     return yhat ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "def compute_loss(y, yhat):     # The loss is defined to be the l2 distance between our estimate of y and its     # true value. We also added a shrinkage term, to ensure the resulting weights     # would be small.     loss = tf.nn.l2_loss(yhat - y) + 0.1 * tf.nn.l2_loss(w)     return loss ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "def generate_data():     # Generate some training data based on the true function     x = np.random.uniform(-10.0, 10.0, size=100).astype(np.float32)     y = 5 * np.square(x) + 3     return x, y ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "def train_step():     x, y = generate_data() ['<div highlight highlight-source-python\">', '<span pl-k\">']\n",
            "def _loss_fn():         yhat = model(x)         loss = compute_loss(y, yhat)         return loss          opt.minimize(_loss_fn, [w]) ['<div highlight highlight-source-python\">', '<span pl-k\">']\n",
            "for _ in range(1000):     train_step() ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "print(w.numpy()) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "@tf.function ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "a = tf.constant([[1., 2.], [3., 4.]]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "b = tf.constant([[1.], [2.]]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "# c = a + tf.tile(b, [1, 2]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "c = a + b ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "print(c) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "a = tf.random.uniform([5, 3, 5]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "b = tf.random.uniform([5, 1, 6]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "# concat a and b and apply nonlinearity ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "tiled_b = tf.tile(b, [1, 3, 1]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "c = tf.concat([a, tiled_b], 2) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "d = tf.keras.layers.Dense(10, activation=tf.nn.relu).apply(c) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "print(d) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "pa = tf.keras.layers.Dense(10).apply(a) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "pb = tf.keras.layers.Dense(10).apply(b) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "d = tf.nn.relu(pa + pb) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "def merge(a, b, units, activation=None):     pa = tf.keras.layers.Dense(units).apply(a)     pb = tf.keras.layers.Dense(units).apply(b)     c = pa + pb     if activation is not None:         c = activation(c)     return c ['<div highlight highlight-source-python\">']\n",
            "a = tf.constant([[1.], [2.]]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "b = tf.constant([1., 2.]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "c = tf.reduce_sum(a + b) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "c = tf.reduce_sum(a + b, 0) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "import time ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "x = tf.random.uniform([500, 10]) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "z = tf.zeros([10]) ['<div highlight highlight-source-python\">', '<span pl-k\">']\n",
            "start = time.time() ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "for i in range(500):     z += x[i] ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "print(\"Took %f seconds.\" % (time.time() - start)) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "for x_i in tf.unstack(x):     z += x_i ['<div highlight highlight-source-python\">', '<span pl-k\">']\n",
            "z = -x  # z = tf.negative(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x + y  # z = tf.add(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x - y  # z = tf.subtract(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x * y  # z = tf.mul(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x / y  # z = tf.div(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x // y  # z = tf.floordiv(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x % y  # z = tf.mod(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x ** y  # z = tf.pow(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x @ y  # z = tf.matmul(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x &gt; y  # z = tf.greater(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x &gt;= y  # z = tf.greater_equal(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x &lt; y  # z = tf.less(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x &lt;= y  # z = tf.less_equal(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = abs(x)  # z = tf.abs(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x &amp; y  # z = tf.logical_and(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x | y  # z = tf.logical_or(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = x ^ y  # z = tf.logical_xor(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "z = ~x  # z = tf.logical_not(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "a = tf.constant(1) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "b = tf.constant(2) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "p = tf.constant(True) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "# Alternatively: ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "# x = tf.cond(p, lambda: a + b, lambda: a * b) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "x = a + b if p else a * b ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-en\">']\n",
            "print(x.numpy()) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "a = tf.constant([1, 1]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "b = tf.constant([2, 2]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "p = tf.constant([True, False]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "x = tf.where(p, a + b, a * b) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "def fibonacci(n):     a = tf.constant(1)     b = tf.constant(1) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "for i in range(2, n):         a, b = b, a + b          return b ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "n = tf.constant(5) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "b = fibonacci(n) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "print(b.numpy()) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "def fibonacci(n):     a = tf.constant(1)     b = tf.constant(1)     c = tf.constant([1, 1]) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "for i in range(2, n):         a, b = b, a + b         c = tf.concat([c, [b]], 0)          return c ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "def cond(i, a, b, c):     return i &lt; n ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "def body(i, a, b, c):     a, b = b, a + b     c = tf.concat([c, [b]], 0)     return i + 1, a, b, c ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "i, a, b, c = tf.while_loop(     cond, body, (2, 1, 1, tf.constant([1, 1])),     shape_invariants=(tf.TensorShape([]),                       tf.TensorShape([]),                       tf.TensorShape([]),                       tf.TensorShape([None]))) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "print(c.numpy()) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "c = tf.TensorArray(tf.int32, n)     c = c.write(0, a)     c = c.write(1, b) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "for i in range(2, n):         a, b = b, a + b         c = c.write(i, b)          return c.stack() ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "c = fibonacci(n) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "import uuid ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">']\n",
            "def relu(inputs):     # Define the op in python     def _py_relu(x):         return np.maximum(x, 0.) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">']\n",
            "# Define the op's gradient in python     def _py_relu_grad(x):         return np.float32(x &gt; 0)          @tf.custom_gradient     def _relu(x):         y = tf.py_function(_py_relu, [x], tf.float32)                  def _relu_grad(dy):             return dy * tf.py_function(_py_relu_grad, [x], tf.float32) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">']\n",
            "return y, _relu_grad ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">']\n",
            "return _relu(inputs) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-c\">']\n",
            "# Compute analytical gradient ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "x = tf.random.normal([10], dtype=np.float32) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "with tf.GradientTape() as tape:     tape.watch(x)     y = relu(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "g = tape.gradient(y, x) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "print(g) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "# Compute numerical gradient ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "dx_n = 1e-5 ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "dy_n = relu(x + dx_n) - relu(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "g_n = dy_n / dx_n ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "print(g_n) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "def visualize_labeled_images(images, labels, max_outputs=3, name=\"image\"):     def _visualize_image(image, label):         # Do the actual drawing in python         fig = plt.figure(figsize=(3, 3), dpi=80)         ax = fig.add_subplot(111)         ax.imshow(image[::-1,...])         ax.text(0, 0, str(label),           horizontalalignment=\"left\",           verticalalignment=\"top\")         fig.canvas.draw() ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# Write the plot as a memory file.         buf = io.BytesIO()         data = fig.savefig(buf, format=\"png\")         buf.seek(0) ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# Read the image and convert to numpy array         img = PIL.Image.open(buf)         return np.array(img.getdata()).reshape(img.size[0], img.size[1], -1) ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-k\">']\n",
            "def _visualize_images(images, labels):         # Only display the given number of examples in the batch         outputs = []         for i in range(max_outputs):             output = _visualize_image(images[i], labels[i])             outputs.append(output)         return np.array(outputs, dtype=np.uint8) ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# Run the python op.     figs = tf.py_function(_visualize_images, [images, labels], tf.uint8)     return tf.summary.image(name, figs) ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-k\">']\n",
            "x = np.float32(1) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "y = np.float32(1e-50)  # y would be stored as zero ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "z = x * y / y ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "print(z)  # prints nan ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "y = np.float32(1e39)  # y would be stored as inf ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "def unstable_softmax(logits):     exp = tf.exp(logits)     return exp / tf.reduce_sum(exp) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-en\">']\n",
            "print(unstable_softmax([1000., 0.]).numpy())  # prints [ nan, 0.] ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-en\">']\n",
            "def softmax(logits):     exp = tf.exp(logits - tf.reduce_max(logits))     return exp / tf.reduce_sum(exp) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-en\">']\n",
            "print(softmax([1000., 0.]).numpy())  # prints [ 1., 0.] ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-en\">']\n",
            "def unstable_softmax_cross_entropy(labels, logits):     logits = tf.math.log(softmax(logits))     return -tf.reduce_sum(labels * logits) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "labels = tf.constant([0.5, 0.5]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "logits = tf.constant([1000., 0.]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "xe = unstable_softmax_cross_entropy(labels, logits) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "print(xe.numpy())  # prints inf ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "def softmax_cross_entropy(labels, logits):     scaled_logits = logits - tf.reduce_max(logits)     normalized_logits = scaled_logits - tf.reduce_logsumexp(scaled_logits)     return -tf.reduce_sum(labels * normalized_logits) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "xe = softmax_cross_entropy(labels, logits) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "print(xe.numpy())  # prints 500.0 ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "with tf.GradientTape() as tape:     tape.watch(logits)     xe = softmax_cross_entropy(labels, logits) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "g = tape.gradient(xe, logits) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "print(g.numpy())  # prints [0.5, -0.5] ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-en\">']\n",
            "A Style-Based Generator Architecture for Generative Adversarial Networks Tero Karras (NVIDIA), Samuli Laine (NVIDIA), Timo Aila (NVIDIA) ['<blockquote>', '<p>']\n",
            "Abstract: We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces. ['<blockquote>', '<p>']\n",
            "Paper: ['<ul>', '<li>']\n",
            "Video: ['<ul>', '<li>']\n",
            "Code: ['<ul>', '<li>']\n",
            "FFHQ: ['<ul>', '<li>']\n",
            "Path ['<li>']\n",
            "Main folder. ['<li>']\n",
            "High-quality version of the paper PDF. ['<li>']\n",
            "High-quality version of the result video. ['<li>']\n",
            "Example images produced using our generator. ['<li>']\n",
            "High-quality images to be used in articles, blog posts, etc. ['<li>']\n",
            "100,000 generated images for different amounts of truncation. ['<li>']\n",
            "Generated using Flickr-Faces-HQ dataset at 1024×1024. ['<li>']\n",
            "Generated using LSUN Bedroom dataset at 256×256. ['<li>']\n",
            "Generated using LSUN Car dataset at 512×384. ['<li>']\n",
            "Generated using LSUN Cat dataset at 256×256. ['<li>']\n",
            "Example videos produced using our generator. ['<li>']\n",
            "Individual segments of the result video as high-quality MP4. ['<li>']\n",
            "Raw data for the . ['<li>']\n",
            "Pre-trained networks as pickled instances of . ['<li>']\n",
            "StyleGAN trained with Flickr-Faces-HQ dataset at 1024×1024. ['<li>']\n",
            "StyleGAN trained with CelebA-HQ dataset at 1024×1024. ['<li>']\n",
            "StyleGAN trained with LSUN Bedroom dataset at 256×256. ['<li>']\n",
            "StyleGAN trained with LSUN Car dataset at 512×384. ['<li>']\n",
            "StyleGAN trained with LSUN Cat dataset at 256×256. ['<li>']\n",
            "Auxiliary networks for the quality and disentanglement metrics. ['<li>']\n",
            "Standard  classifier that outputs a raw feature vector. ['<li>']\n",
            "Standard  metric to estimate perceptual similarity. ['<li>']\n",
            "Binary classifier trained to detect a single attribute of CelebA-HQ. ['<li>']\n",
            "Please see the file listing for remaining networks. ['<li>']\n",
            "Both Linux and Windows are supported, but we strongly recommend Linux for performance and compatibility reasons. ['<ul>', '<li>']\n",
            "64-bit Python 3.6 installation. We recommend Anaconda3 with numpy 1.14.3 or newer. ['<ul>', '<li>']\n",
            "TensorFlow 1.10.0 or newer with GPU support. ['<ul>', '<li>']\n",
            "One or more high-end NVIDIA GPUs with at least 11GB of DRAM. We recommend NVIDIA DGX-1 with 8 Tesla V100 GPUs. ['<ul>', '<li>']\n",
            "NVIDIA driver 391.35 or newer, CUDA toolkit 9.0 or newer, cuDNN 7.3.1 or newer. ['<ul>', '<li>']\n",
            "&gt; python pretrained_example.py Downloading https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ .... done ['<pre>']\n",
            "Gs                              Params    OutputShape          WeightShape ---                             ---       ---                  --- latents_in                      -         (?, 512)             - ... images_out                      -         (?, 3, 1024, 1024)   - ---                             ---       ---                  --- Total                           26219627 ['<pre>']\n",
            "&gt; ls results example.png # https://drive.google.com/uc?id=1UDLT_zb-rof9kKH0GwiJW_bS9MoZi8oP ['<pre>']\n",
            "&gt; python generate_figures.py results/figure02-uncurated-ffhq.png     # https://drive.google.com/uc?id=1U3r1xgcD7o-Fd0SBRpq8PXYajm7_30cu results/figure03-style-mixing.png       # https://drive.google.com/uc?id=1U-nlMDtpnf1RcYkaFQtbh5oxnhA97hy6 results/figure04-noise-detail.png       # https://drive.google.com/uc?id=1UX3m39u_DTU6eLnEW6MqGzbwPFt2R9cG results/figure05-noise-components.png   # https://drive.google.com/uc?id=1UQKPcvYVeWMRccGMbs2pPD9PVv1QDyp_ results/figure08-truncation-trick.png   # https://drive.google.com/uc?id=1ULea0C12zGlxdDQFNLXOWZCHi3QNfk_v results/figure10-uncurated-bedrooms.png # https://drive.google.com/uc?id=1UEBnms1XMfj78OHj3_cx80mUf_m9DUJr results/figure11-uncurated-cars.png     # https://drive.google.com/uc?id=1UO-4JtAs64Kun5vIj10UXqAJ1d5Ir1Ke results/figure12-uncurated-cats.png     # https://drive.google.com/uc?id=1USnJc14prlu3QAYxstrtlfXC9sDWPA-W ['<pre>']\n",
            "# Load pre-trained network. url = 'https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ' # karras2019stylegan-ffhq-1024x1024.pkl with dnnlib.util.open_url(url, cache_dir=config.cache_dir) as f:     _G, _D, Gs = pickle.load(f)     # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.     # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.     # Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot. ['<pre>']\n",
            "Use Gs.run() for immediate-mode operation where the inputs and outputs are numpy arrays: ['<ol>', '<li>', '<p>', '<pre>']\n",
            "# Pick latent vector. rnd = np.random.RandomState(5) latents = rnd.randn(1, Gs.input_shape[1]) ['<ol>', '<li>', '<p>', '<pre>']\n",
            "# Generate image. fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True) images = Gs.run(latents, None, truncation_psi=0.7, randomize_noise=True, output_transform=fmt) ['<ol>', '<li>', '<p>', '<pre>']\n",
            "The first argument is a batch of latent vectors of shape [num, 512]. The second argument is reserved for class labels (not used by StyleGAN). The remaining keyword arguments are optional and can be used to further modify the operation (see below). The output is a batch of images, whose format is dictated by the output_transform argument. ['<ol>', '<li>', '<p>', '<pre>']\n",
            "Use Gs.get_output_for() to incorporate the generator as a part of a larger TensorFlow expression: ['<ol>', '<li>', '<p>', '<pre>']\n",
            "latents = tf.random_normal([self.minibatch_per_gpu] + Gs_clone.input_shape[1:]) images = Gs_clone.get_output_for(latents, None, is_validation=True, randomize_noise=True) images = tflib.convert_images_to_uint8(images) result_expr.append(inception_clone.get_output_for(images)) ['<ol>', '<li>', '<p>', '<pre>']\n",
            "The above code is from . It generates a batch of random images and feeds them directly to the  network without having to convert the data to numpy arrays in between. ['<ol>', '<li>', '<p>', '<pre>']\n",
            "Look up Gs.components.mapping and Gs.components.synthesis to access individual sub-networks of the generator. Similar to Gs, the sub-networks are represented as independent instances of : ['<ol>', '<li>', '<p>', '<pre>']\n",
            "src_latents = np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in src_seeds) src_dlatents = Gs.components.mapping.run(src_latents, None) # [seed, layer, component] src_images = Gs.components.synthesis.run(src_dlatents, randomize_noise=False, **synthesis_kwargs) ['<ol>', '<li>', '<p>', '<pre>']\n",
            "The above code is from . It first transforms a batch of latent vectors into the intermediate W space using the mapping network and then turns these vectors into a batch of images using the synthesis network. The dlatents array stores a separate copy of the same w vector for each layer of the synthesis network to facilitate style mixing. ['<ol>', '<li>', '<p>', '<pre>']\n",
            "truncation_psi and truncation_cutoff control the truncation trick that that is performed by default when using Gs (ψ=0.7, cutoff=8). It can be disabled by setting truncation_psi=1 or is_validation=True, and the image quality can be further improved at the cost of variation by setting e.g. truncation_psi=0.5. Note that truncation is always disabled when using the sub-networks directly. The average w needed to manually perform the truncation trick can be looked up using Gs.get_var('dlatent_avg'). ['<ul>', '<li>', '<p>']\n",
            "randomize_noise determines whether to use re-randomize the noise inputs for each generated image (True, default) or whether to use specific noise values for the entire minibatch (False). The specific values can be accessed via the tf.Variable instances that are found using [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]. ['<ul>', '<li>', '<p>']\n",
            "When using the mapping network directly, you can specify dlatent_broadcast=None to disable the automatic duplication of dlatents over the layers of the synthesis network. ['<ul>', '<li>', '<p>']\n",
            "Runtime performance can be fine-tuned via structure='fixed' and dtype='float16'. The former disables support for progressive growing, which is not needed for a fully-trained generator, and the latter performs all computation using half-precision floating point arithmetic. ['<ul>', '<li>', '<p>']\n",
            "result_dir = 'results' data_dir = 'datasets' cache_dir = 'cache' ['<pre>']\n",
            "&gt; python dataset_tool.py create_lsun datasets/lsun-bedroom-full ~/lsun/bedroom_lmdb --resolution 256 &gt; python dataset_tool.py create_lsun_wide datasets/lsun-car-512x384 ~/lsun/car_lmdb --width 512 --height 384 &gt; python dataset_tool.py create_lsun datasets/lsun-cat-full ~/lsun/cat_lmdb --resolution 256 &gt; python dataset_tool.py create_cifar10 datasets/cifar10 ~/cifar10 &gt; python dataset_tool.py create_from_images datasets/custom-dataset ~/custom-images ['<pre>']\n",
            "Edit  to specify the dataset and training configuration by uncommenting or editing specific lines. ['<ol>', '<li>']\n",
            "Run the training script with python train.py. ['<ol>', '<li>']\n",
            "The results are written to a newly created directory results/&lt;ID&gt;-&lt;DESCRIPTION&gt;. ['<ol>', '<li>']\n",
            "The training may take several days (or weeks) to complete, depending on the configuration. ['<ol>', '<li>']\n",
            "GPUs ['<li>']\n",
            "1024×1024 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "512×512 ['<li>']\n",
            "256×256 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "41 days 4 hours ['<li>']\n",
            "24 days 21 hours ['<li>']\n",
            "14 days 22 hours ['<li>']\n",
            "21 days 22 hours ['<li>']\n",
            "13 days 7 hours ['<li>']\n",
            "9 days 5 hours ['<li>']\n",
            "11 days 8 hours ['<li>']\n",
            "7 days 0 hours ['<li>']\n",
            "4 days 21 hours ['<li>']\n",
            "6 days 14 hours ['<li>']\n",
            "4 days 10 hours ['<li>']\n",
            "3 days 8 hours ['<li>']\n",
            "Metric ['<div highlight highlight-source-shell\">', '<code>']\n",
            "Time ['<li>', '<em>', '<br/>']\n",
            "Result ['<li>', '<em>', '<br/>']\n",
            "fid50k ['<div highlight highlight-source-shell\">', '<code>']\n",
            "16 min ['<li>', '<em>', '<br/>']\n",
            "4.4159 ['<li>', '<em>', '<br/>']\n",
            "Fréchet Inception Distance using 50,000 images. ['<li>', '<em>', '<br/>']\n",
            "ppl_zfull ['<div highlight highlight-source-shell\">', '<code>']\n",
            "55 min ['<li>', '<em>', '<br/>']\n",
            "664.8854 ['<li>', '<em>', '<br/>']\n",
            "Perceptual Path Length for full paths in Z. ['<li>', '<em>', '<br/>']\n",
            "ppl_wfull ['<div highlight highlight-source-shell\">', '<code>']\n",
            "233.3059 ['<li>', '<em>', '<br/>']\n",
            "Perceptual Path Length for full paths in W. ['<li>', '<em>', '<br/>']\n",
            "ppl_zend ['<div highlight highlight-source-shell\">', '<code>']\n",
            "666.1057 ['<li>', '<em>', '<br/>']\n",
            "Perceptual Path Length for path endpoints in Z. ['<li>', '<em>', '<br/>']\n",
            "ppl_wend ['<div highlight highlight-source-shell\">', '<code>']\n",
            "197.2266 ['<li>', '<em>', '<br/>']\n",
            "Perceptual Path Length for path endpoints in W. ['<li>', '<em>', '<br/>']\n",
            "ls ['<div highlight highlight-source-shell\">', '<code>']\n",
            "10 hours ['<li>', '<em>', '<br/>']\n",
            "z: 165.0106w: 3.7447 ['<li>', '<em>', '<br/>']\n",
            "Linear Separability in Z and W. ['<li>', '<em>', '<br/>']\n",
            "TensorFlow 1.0 alpha ['<ul>', '<li>']\n",
            "Numpy ['<ul>', '<li>']\n",
            "matplotlib ['<ul>', '<li>']\n",
            "More TF (1.0) style: use more recent and decent TF APIs. ['<ul>', '<li>']\n",
            "More Pythonic: fully leverage the power of python ['<ul>', '<li>']\n",
            "Readability (over efficiency): Since it's for instruction purposes, we prefer readability over others. ['<ul>', '<li>']\n",
            "Understandability (over everything): Understanding TF key concepts is the main goal of this code. ['<ul>', '<li>']\n",
            "KISS: Keep It Simple Stupid! ['<ul>', '<li>']\n",
            "klab-XX-X-[name].py: Keras labs code ['<ul>', '<li>']\n",
            "lab-XX-X-[name].py: TensorFlow lab code ['<ul>', '<li>']\n",
            "mxlab-XX-X-[name].py: MXNet lab code ['<ul>', '<li>']\n",
            "python -m unittest discover -s tests; ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# http://stackoverflow.com/questions/14328406/ pip install autopep8 # if you haven't install autopep8 . --recursive --in-place --pep8-passes 2000 --verbose ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "pip install pipreqs ['<div highlight highlight-source-shell\">']\n",
            "pipreqs /path/to/project ['<div highlight highlight-source-shell\">']\n",
            "A data file (in NumPy's native format) containing the model's learned parameters. ['<ol>', '<li>']\n",
            "A Python class that constructs the model's graph. ['<ol>', '<li>']\n",
            "Top 5 Accuracy ['<li>']\n",
            "92.92% ['<li>']\n",
            "92.63% ['<li>']\n",
            "92.02% ['<li>']\n",
            "89.88% ['<li>']\n",
            "89.06% ['<li>']\n",
            "81.21% ['<li>']\n",
            "79.93% ['<li>']\n",
            "79.84% ['<li>']\n",
            "Only the new Caffe model format is supported. If you have an old model, use the upgrade_net_proto_text and upgrade_net_proto_binary tools that ship with Caffe to upgrade them first. Also make sure you're using a fairly recent version of Caffe. ['<ul>', '<li>', '<p>']\n",
            "It appears that Caffe and TensorFlow cannot be concurrently invoked (CUDA conflicts - even with set_mode_cpu). This makes it a two-stage process: first extract the parameters with convert.py, then import it into TensorFlow. ['<ul>', '<li>', '<p>']\n",
            "Caffe is not strictly required. If PyCaffe is found in your PYTHONPATH, and the USE_PYCAFFE environment variable is set, it will be used. Otherwise, a fallback will be used. However, the fallback uses the pure Python-based implementation of protobuf, which is astoundingly slow (~1.5 minutes to parse the VGG16 parameters). The experimental CPP protobuf backend doesn't particularly help here, since it runs into the file size limit (Caffe gets around this by overriding this limit in C++). A cleaner solution here would be to implement the loader as a C++ module. ['<ul>', '<li>', '<p>']\n",
            "Only a subset of Caffe layers and accompanying parameters are currently supported. ['<ul>', '<li>', '<p>']\n",
            "Not all Caffe models can be converted to TensorFlow. For instance, Caffe supports arbitrary padding whereas TensorFlow's support is currently restricted to SAME and VALID. ['<ul>', '<li>', '<p>']\n",
            "The border values are handled differently by Caffe and TensorFlow. However, these don't appear to affect things too much. ['<ul>', '<li>', '<p>']\n",
            "Image rescaling can affect the ILSVRC2012 top 5 accuracy listed above slightly. VGG16 expects isotropic rescaling (anisotropic reduces accuracy to 88.45%) whereas BVLC's implementation of GoogLeNet expects anisotropic (isotropic reduces accuracy to 87.7%). ['<ul>', '<li>', '<p>']\n",
            "The support class kaffe.tensorflow.Network has no internal dependencies. It can be safely extracted and deployed without the rest of this library. ['<ul>', '<li>', '<p>']\n",
            "The ResNet model uses 1x1 convolutions with a stride of 2. This is currently only supported in the master branch of TensorFlow (the latest release at time of writing being v0.8.0, which does not support it). ['<ul>', '<li>', '<p>']\n",
            "An ai powered automatically generats poems in Chinese. ['<blockquote>', '<p>']\n",
            "很久以来，我们都想让机器自己创作诗歌，当无数作家、编辑还没有抬起笔时，AI已经完成了数千篇文章。现在，这里是第一步.... ['<blockquote>', '<p>']\n",
            "龙舆迎池里，控列守龙猱。 几岁芳篁落，来和晚月中。 殊乘暮心处，麦光属激羁。 铁门通眼峡，高桂露沙连。 倘子门中望，何妨嶮锦楼。 择闻洛臣识，椒苑根觞吼。 柳翰天河酒，光方入胶明。 ['<pre>']\n",
            "这诗做的很有感觉啊，这都是勤奋的结果啊，基本上学习了全唐诗的所有精华才有了这么牛逼的能力，这一般人能做到？ 本博客讲讲解一些里面实现的技术细节，如果有未尽之处，大家可以通过微信找到我，那个头像很神奇的男人。闲话不多说，先把 github 链接放上来，这个作诗机器人我会一直维护，如果大家因为时间太紧没有时间看，可以给这个项目 star 一下或者 fork， 我一推送更新你就能看到，主要是为了修复一些 api 问题，tensorflow 虽然到了1.0，但是 api 还是会变化。 把星星加起来，让更多人可以看到我们创造这个作诗机器人，后期会加入更多牛逼掉渣天的功能，比如说押韵等等。 ['<p>']\n",
            "# train on poems 训练 python3 train.py ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# compose poems 作诗 python3 compose_poem.py ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "人工智能从入门到逆天杀神(知乎专栏)： ['<ul>', '<li>']\n",
            "每周一项目黑科技-TrackTech(知乎专栏):   If you want talk about AI, visit our website (for now):   (strangeai.pro availiable soon) , subscribe our WeChat channel: 奇异人工智能学院 ['<ul>', '<li>']\n",
            "对数据预处理脚本进行了前所未有的简化，现在连小学生都能了解了 ['<ul>', '<li>']\n",
            "训练只需要运行train.py，数据和预训练模型都已经备好 ['<ul>', '<li>']\n",
            "可以直接compose_poem.py 作诗，这次不会出现死循环的情况了。 ['<ul>', '<li>']\n",
            "训练完成作诗时出现一直不出现的情况，实际上是陷入了一直作诗的死循环，已修复 ['<ul>', '<li>']\n",
            "新增pretty print功能，打印出的古诗标准，接入第三方APP或者其他平台可以直接获取到标准格式的诗词 ['<ul>', '<li>']\n",
            "Ternimal disable了tensorflow默认的debug信息 最后最后最重要的是： 我们的作诗机器人（暂且叫李白）已经可以根据你的指定的字作诗了哦！！ 欢迎大家继续来踩，没有star的快star！！保持更新！！永远开源！！！ 让我们来看看李白做的藏头诗吧： ['<ul>', '<li>']\n",
            "# 最近一直下雨，就作一首雨字开头的吧 雨霁开门中，山听淮水流。 落花遍霜霰，金壶横河湟。 年年忽息世，径远谁论吟。 惊舟望秋月，应柳待晨围。 人处山霜月，萧萧广野虚。 ['<pre>']\n",
            "# 李白人工智能作诗机器人的作者长得比较帅，以帅开头做一首吧 帅主何幸化，自日兼春连。 命钱犯夕兴，职馀玄赏圣。 君有不知益，浮于但神衍。 （浓浓的怀才不遇之风...） ['<pre>']\n",
            "我的你的她 蛾眉脚的泪花 乱飞从慌乱 笛卡尔的悲伤 迟早在是石板上 荒废了晚上 夜你的她不是她 .... ['<pre>']\n",
            "怎么说，目前由于缺乏训练文本，导致我们的AI做的歌词有点....额，还好啦，有那么一点忧郁之风，这个周杰伦完全不是一种风格呀。 然而没有关系，目前它训练的文本还太少，只有112首歌，在这里我来呼吁大家一起来整理 中国歌手的语料文本！！！ 如果你喜欢周杰伦的歌，可以把他的歌一首一行，每首歌句子空格分开保存到txt中，大家可以集中发到我的： 相信如果不断的加入训练文本我们的歌词创作机器人会越来越牛逼！当然我会及时把数据集更新到github上，大家可以 star 一下跟进本项目的更新。 ['<p>']\n",
            "Website: ['<ul>', '<li>']\n",
            "GitHub: ['<ul>', '<li>']\n",
            "Twitter: ['<ul>', '<li>']\n",
            "To use the VGG networks, the npy files for  or  has to be downloaded. ['<blockquote>', '<p>']\n",
            "vgg = vgg19.Vgg19() vgg.build(images) ['<pre>']\n",
            "vgg = vgg16.Vgg16() vgg.build(images) ['<pre>']\n",
            "Trick: the tensor can be a placeholder, a variable or even a constant. ['<blockquote>', '<p>']\n",
            "Additional material ['<pre>', '<span>']\n",
            "Main Google Drive folder ['<pre>', '<span>']\n",
            "High-quality version of the paper ['<pre>', '<span>']\n",
            "High-quality version of the video ['<pre>', '<span>']\n",
            "Example images produced using our method ['<pre>', '<span>']\n",
            "Hand-picked images showcasing our results ['<pre>', '<span>']\n",
            "Random images with and without truncation ['<pre>', '<span>']\n",
            "Individual clips of the video as high-quality MP4 ['<pre>', '<span>']\n",
            "Pre-trained networks ['<pre>', '<span>']\n",
            "├   stylegan2-ffhq-config-f.pkl ['<pre>', '<span>']\n",
            "StyleGAN2 for FFHQ dataset at 1024×1024 ['<pre>', '<span>']\n",
            "├   stylegan2-car-config-f.pkl ['<pre>', '<span>']\n",
            "StyleGAN2 for LSUN Car dataset at 512×384 ['<pre>', '<span>']\n",
            "├   stylegan2-cat-config-f.pkl ['<pre>', '<span>']\n",
            "StyleGAN2 for LSUN Cat dataset at 256×256 ['<pre>', '<span>']\n",
            "├   stylegan2-church-config-f.pkl ['<pre>', '<span>']\n",
            "StyleGAN2 for LSUN Church dataset at 256×256 ['<pre>', '<span>']\n",
            "├   stylegan2-horse-config-f.pkl ['<pre>', '<span>']\n",
            "StyleGAN2 for LSUN Horse dataset at 256×256 ['<pre>', '<span>']\n",
            "Other training configurations used in the paper ['<pre>', '<span>']\n",
            "Both Linux and Windows are supported. Linux is recommended for performance and compatibility reasons. ['<ul>', '<li>']\n",
            "We recommend TensorFlow 1.14, which we used for all experiments in the paper, but TensorFlow 1.15 is also supported on Linux. TensorFlow 2.x is not supported. ['<ul>', '<li>']\n",
            "On Windows you need to use TensorFlow 1.14, as the standard 1.15 installation does not include necessary C++ headers. ['<ul>', '<li>']\n",
            "One or more high-end NVIDIA GPUs, NVIDIA drivers, CUDA 10.0 toolkit and cuDNN 7.5. To reproduce the results reported in the paper, you need an NVIDIA GPU with at least 16 GB of DRAM. ['<ul>', '<li>']\n",
            "Docker users: use the  to build an image with the required library dependencies. ['<ul>', '<li>']\n",
            "nvcc test_nvcc.cu -o test_nvcc -run ['<div highlight highlight-source-shell\">', '<span pl-k\">']\n",
            "| CPU says hello. ['<div highlight highlight-source-shell\">', '<span pl-k\">']\n",
            "| GPU says hello. ['<div highlight highlight-source-shell\">', '<span pl-k\">']\n",
            "# Generate uncurated ffhq images (matches paper Figure 12) python run_generator.py generate-images --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\   --seeds=6600-6625 --truncation-psi=0.5 ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# Generate curated ffhq images (matches paper Figure 11) python run_generator.py generate-images --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\   --seeds=66,230,389,1518 --truncation-psi=1.0 ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# Generate uncurated car images python run_generator.py generate-images --network=gdrive:networks/stylegan2-car-config-f.pkl \\   --seeds=6000-6025 --truncation-psi=0.5 ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# Example of style mixing (matches the corresponding video clip) python run_generator.py style-mixing-example --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\   --row-seeds=85,100,75,458,1500 --col-seeds=55,821,1789,293 --truncation-psi=1.0 ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "pushd ~ git clone https://github.com/NVlabs/ffhq-dataset.git ['<div highlight highlight-source-shell\">', '<span pl-c1\">']\n",
            "cd ffhq-dataset python download_ffhq.py --tfrecords ['<div highlight highlight-source-shell\">', '<span pl-c1\">']\n",
            "popd python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq ['<div highlight highlight-source-shell\">', '<span pl-c1\">']\n",
            "python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384 python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256 python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256 ['<div highlight highlight-source-shell\">']\n",
            "# Project generated images python run_projector.py project-generated-images --network=gdrive:networks/stylegan2-car-config-f.pkl \\   --seeds=0,1,5 ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# Project real images python run_projector.py project-real-images --network=gdrive:networks/stylegan2-car-config-f.pkl \\   --dataset=car --data-dir=~/datasets ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "python run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\   --dataset=ffhq --mirror-augment=true python run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\   --dataset=car --total-kimg=57000 python run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\   --dataset=cat --total-kimg=88000 python run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\   --dataset=church --total-kimg 88000 --gamma=100 python run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\   --dataset=horse --total-kimg 100000 --gamma=100 ['<div highlight highlight-source-shell\">']\n",
            "Configuration ['<div highlight highlight-source-shell\">', '<code>']\n",
            "Resolution ['<div highlight highlight-source-shell\">', '<code>']\n",
            "Total kimg ['<div highlight highlight-source-shell\">', '<code>']\n",
            "1 GPU ['<div highlight highlight-source-shell\">', '<code>']\n",
            "2 GPUs ['<div highlight highlight-source-shell\">', '<code>']\n",
            "4 GPUs ['<div highlight highlight-source-shell\">', '<code>']\n",
            "8 GPUs ['<div highlight highlight-source-shell\">', '<code>']\n",
            "GPU mem ['<div highlight highlight-source-shell\">', '<code>']\n",
            "config-f ['<div highlight highlight-source-shell\">', '<code>']\n",
            "25000 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "69d 23h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "36d 4h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "18d 14h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "9d 18h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "13.3 GB ['<div highlight highlight-source-shell\">', '<code>']\n",
            "10000 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "27d 23h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "14d 11h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "7d 10h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "3d 22h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "config-e ['<div highlight highlight-source-shell\">', '<code>']\n",
            "35d 11h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "18d 15h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "9d 15h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "5d 6h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "8.6 GB ['<div highlight highlight-source-shell\">', '<code>']\n",
            "14d 4h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "7d 11h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "3d 20h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "2d 3h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "32d 13h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "16d 23h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "8d 21h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "4d 18h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "6.4 GB ['<div highlight highlight-source-shell\">', '<code>']\n",
            "13d 0h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "6d 19h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "3d 13h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "1d 22h ['<div highlight highlight-source-shell\">', '<code>']\n",
            "# Generate 1000 random images without truncation python run_generator.py generate-images --seeds=0-999 --truncation-psi=1.0 \\   --network=results/00006-stylegan2-ffhq-8gpu-config-f/networks-final.pkl ['<div highlight highlight-source-shell\">']\n",
            "python run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\   --metrics=fid50k,ppl_wend --dataset=ffhq --mirror-augment=true python run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-car-config-f.pkl \\   --metrics=fid50k,ppl2_wend --dataset=car python run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-cat-config-f.pkl \\   --metrics=fid50k,ppl2_wend --dataset=cat python run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-church-config-f.pkl \\   --metrics=fid50k,ppl2_wend --dataset=church python run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-horse-config-f.pkl \\   --metrics=fid50k,ppl2_wend --dataset=horse ['<div highlight highlight-source-shell\">']\n",
            "FFHQ config F ['<div highlight highlight-source-shell\">', '<code>']\n",
            "2.84 ± 0.03 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "22 min ['<div highlight highlight-source-shell\">', '<code>']\n",
            "14 min ['<div highlight highlight-source-shell\">', '<code>']\n",
            "10 min ['<div highlight highlight-source-shell\">', '<code>']\n",
            "is50k ['<div highlight highlight-source-shell\">', '<code>']\n",
            "5.13 ± 0.02 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "23 min ['<div highlight highlight-source-shell\">', '<code>']\n",
            "8 min ['<div highlight highlight-source-shell\">', '<code>']\n",
            "348.0 ± 3.8 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "41 min ['<div highlight highlight-source-shell\">', '<code>']\n",
            "in Z, full paths ['<div highlight highlight-source-shell\">', '<code>']\n",
            "126.9 ± 0.2 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "42 min ['<div highlight highlight-source-shell\">', '<code>']\n",
            "13 min ['<div highlight highlight-source-shell\">', '<code>']\n",
            "in W, full paths ['<div highlight highlight-source-shell\">', '<code>']\n",
            "348.6 ± 3.0 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "in Z, path endpoints ['<div highlight highlight-source-shell\">', '<code>']\n",
            "129.4 ± 0.8 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "40 min ['<div highlight highlight-source-shell\">', '<code>']\n",
            "in W, path endpoints ['<div highlight highlight-source-shell\">', '<code>']\n",
            "ppl2_wend ['<div highlight highlight-source-shell\">', '<code>']\n",
            "145.0 ± 0.5 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "without center crop ['<div highlight highlight-source-shell\">', '<code>']\n",
            "154.2 / 4.27 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "10 hrs ['<div highlight highlight-source-shell\">', '<code>']\n",
            "6 hrs ['<div highlight highlight-source-shell\">', '<code>']\n",
            "4 hrs ['<div highlight highlight-source-shell\">', '<code>']\n",
            "pr50k3 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "0.689 / 0.492 ['<div highlight highlight-source-shell\">', '<code>']\n",
            "26 min ['<div highlight highlight-source-shell\">', '<code>']\n",
            "17 min ['<div highlight highlight-source-shell\">', '<code>']\n",
            "12 min ['<div highlight highlight-source-shell\">', '<code>']\n",
            "@inproceedings{Karras2019stylegan2,   title     = {Analyzing and Improving the Image Quality of {StyleGAN}},   author    = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},   booktitle = {Proc. CVPR},   year      = {2020} } ['<pre>']\n",
            "本书被“机器之心”，“量子位”等权威媒体报导！ ['<ul>', '<li>', '<p>']\n",
            "本库在Github趋势日榜单连续多天全球排名第1！ ['<ul>', '<li>', '<p>']\n",
            "提交错误或者修改等反馈意见，请在Github 页面提交 ['<ul>', '<li>', '<p>']\n",
            "联系邮箱(一般问题建议Github issues交流)：liangqu.long AT gmail.com ['<ul>', '<li>', '<p>']\n",
            "高校老师索取PPT原素材等教案，请邮箱联系，并详注院校课程等信息，一般3天内发送邮件回复 ['<ul>', '<li>', '<p>']\n",
            "使用本书本的任何内容时(仅限非商业用途)，请注明作者和Github链接 ['<ul>', '<li>', '<p>']\n",
            "更多TensorFlow 2实战案例在 ['<ul>', '<li>', '<p>']\n",
            "深度学习与TensorFlow入门实战 ['<ul>', '<li>']\n",
            "深度学习与PyTorch入门实战 ['<ul>', '<li>']\n",
            "Tensorflow 1.4.1 ['<ul>', '<li>']\n",
            "Linux with Tensorflow GPU edition + cuDNN ['<ul>', '<li>']\n",
            "# clone this repo git clone https://github.com/affinelayer/pix2pix-tensorflow.git ['<div highlight highlight-source-shell\">', '<span pl-c1\">', '<span pl-c\">']\n",
            "cd pix2pix-tensorflow ['<div highlight highlight-source-shell\">', '<span pl-c1\">', '<span pl-c\">']\n",
            "# download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/) python tools/download-dataset.py facades ['<div highlight highlight-source-shell\">', '<span pl-c1\">', '<span pl-c\">']\n",
            "# train the model (this may take 1-8 hours depending on GPU, on CPU you will be waiting for a bit) python pix2pix.py \\   --mode train \\   --output_dir facades_train \\   --max_epochs 200 \\   --input_dir facades/train \\   --which_direction BtoA ['<div highlight highlight-source-shell\">', '<span pl-c1\">', '<span pl-c\">']\n",
            "# test the model python pix2pix.py \\   --mode test \\   --output_dir facades_test \\   --input_dir facades/val \\   --checkpoint facades_train ['<div highlight highlight-source-shell\">', '<span pl-c1\">', '<span pl-c\">']\n",
            "# train the model python tools/dockrun.py python pix2pix.py \\       --mode train \\       --output_dir facades_train \\       --max_epochs 200 \\       --input_dir facades/train \\       --which_direction BtoA ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# test the model python tools/dockrun.py python pix2pix.py \\       --mode test \\       --output_dir facades_test \\       --input_dir facades/val \\       --checkpoint facades_train ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "dataset ['<span pl-c\">', '<code>']\n",
            "example ['<span pl-c\">', '<code>']\n",
            "python tools/download-dataset.py facades  400 images from . (31MB)  Pre-trained: ['<span pl-c\">', '<code>']\n",
            "python tools/download-dataset.py cityscapes  2975 images from the . (113M)  Pre-trained: ['<span pl-c\">', '<code>']\n",
            "python tools/download-dataset.py maps  1096 training images scraped from Google Maps (246M)  Pre-trained: ['<span pl-c\">', '<code>']\n",
            "python tools/download-dataset.py edges2shoes  50k training images from . Edges are computed by  edge detector + post-processing. (2.2GB)  Pre-trained: ['<span pl-c\">', '<code>']\n",
            "python tools/download-dataset.py edges2handbags  137K Amazon Handbag images from . Edges are computed by  edge detector + post-processing. (8.6GB)  Pre-trained: ['<span pl-c\">', '<code>']\n",
            "# Resize source images python tools/process.py \\   --input_dir photos/original \\   --operation resize \\   --output_dir photos/resized ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# Create images with blank centers python tools/process.py \\   --input_dir photos/resized \\   --operation blank \\   --output_dir photos/blank ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# Combine resized images with blanked images python tools/process.py \\   --input_dir photos/resized \\   --b_dir photos/blank \\   --operation combine \\   --output_dir photos/combined ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# Split into train/val set python tools/split.py \\   --dir photos/combined ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "python tools/process.py \\   --input_dir a \\   --b_dir b \\   --operation combine \\   --output_dir c ['<div highlight highlight-source-shell\">']\n",
            "python tools/process.py \\   --input_dir photos/original \\   --operation resize \\   --output_dir photos/resized ['<div highlight highlight-source-shell\">']\n",
            "python pix2pix.py \\   --mode train \\   --output_dir facades_train \\   --max_epochs 200 \\   --input_dir facades/train \\   --which_direction BtoA ['<div highlight highlight-source-shell\">']\n",
            "python pix2pix.py \\   --mode train \\   --output_dir photos_train \\   --max_epochs 200 \\   --input_dir photos/train \\   --lab_colorization ['<div highlight highlight-source-shell\">']\n",
            "python pix2pix.py \\   --mode test \\   --output_dir facades_test \\   --input_dir facades/val \\   --checkpoint facades_train ['<div highlight highlight-source-shell\">']\n",
            "git clone https://github.com/affinelayer/pix2pix-tensorflow.git ['<div highlight highlight-source-shell\">', '<span pl-c1\">']\n",
            "cd pix2pix-tensorflow python tools/download-dataset.py facades sudo nvidia-docker run \\   --volume $PWD:/prj \\   --workdir /prj \\   --env PYTHONUNBUFFERED=x \\   affinelayer/pix2pix-tensorflow \\     python pix2pix.py \\       --mode train \\       --output_dir facades_train \\       --max_epochs 200 \\       --input_dir facades/train \\       --which_direction BtoA sudo nvidia-docker run \\   --volume $PWD:/prj \\   --workdir /prj \\   --env PYTHONUNBUFFERED=x \\   affinelayer/pix2pix-tensorflow \\     python pix2pix.py \\       --mode test \\       --output_dir facades_test \\       --input_dir facades/val \\       --checkpoint facades_train ['<div highlight highlight-source-shell\">', '<span pl-c1\">']\n",
            "Input ['<span pl-c1\">']\n",
            "Tensorflow ['<ul>', '<li>']\n",
            "Torch ['<span pl-c1\">']\n",
            "Target ['<span pl-c1\">']\n",
            "defineG_encoder_decoder ['<ul>', '<li>']\n",
            "defineG_unet_128 ['<ul>', '<li>']\n",
            "defineD_pixelGAN ['<ul>', '<li>']\n",
            "@article{pix2pix2016,   title={Image-to-Image Translation with Conditional Adversarial Networks},   author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},   journal={arxiv},   year={2016} } ['<pre>']\n",
            "Oct. 1, 2019: TensorFlow 2.0 Stable! ['<ul>', '<li>']\n",
            "Aug. 24, 2019: ['<ul>', '<li>']\n",
            "Jun. 8, 2019: ['<ul>', '<li>']\n",
            "Mar. 7, 2019: ['<ul>', '<li>']\n",
            "Jan. 11, 2019: ['<ul>', '<li>']\n",
            "Aug. 14, 2018: ['<ul>', '<li>']\n",
            "CPU install ['<ul>', '<li>']\n",
            "GPU install ['<ul>', '<li>']\n",
            "In [2]: import tensorflow  as tf ['<div highlight highlight-source-python\">', '<span pl-v\">', '<span pl-s1\">']\n",
            "In [3]: tf.__version__ ['<div highlight highlight-source-python\">', '<span pl-v\">', '<span pl-s1\">']\n",
            "Out[3]: '2.0.0' ['<div highlight highlight-source-python\">', '<span pl-v\">', '<span pl-s1\">']\n",
            "In [4]: tf.test.is_gpu_available() ... ['<div highlight highlight-source-python\">', '<span pl-v\">', '<span pl-s1\">']\n",
            "totalMemory: 3.95GiB freeMemory: 3.00GiB ... ['<div highlight highlight-source-python\">', '<span pl-v\">', '<span pl-s1\">']\n",
            "Out[4]: True ['<div highlight highlight-source-python\">', '<span pl-v\">', '<span pl-s1\">']\n",
            "爱可可-爱生活 友情推荐 ['<ul>', '<li>']\n",
            "TensorFlow 2.0 Overview ['<ul>', '<li>']\n",
            "TensorFlow 2.0 Basic Usage ['<ul>', '<li>']\n",
            "MNIST, FashionMNIST ['<ul>', '<li>']\n",
            "CIFAR10 ['<ul>', '<li>']\n",
            "Fully Connected Layer ['<ul>', '<li>']\n",
            "VGG16 ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Inception Network ['<ul>', '<li>']\n",
            "ResNet18 ['<ul>', '<li>']\n",
            "Naive RNN ['<ul>', '<li>']\n",
            "LSTM ['<ul>', '<li>']\n",
            "ColorBot ['<ul>', '<li>']\n",
            "Auto-Encoders ['<ul>', '<li>']\n",
            "Variational Auto-Encoders ['<ul>', '<li>']\n",
            "DCGAN ['<ul>', '<li>']\n",
            "CycleGAN ['<ul>', '<li>']\n",
            "WGAN ['<em>']\n",
            "Pixel2Pixel ['<ul>', '<li>']\n",
            "Faster RCNN ['<ul>', '<li>']\n",
            "A2C ['<ul>', '<li>']\n",
            "GPT ['<ul>', '<li>']\n",
            "BERT ['<strong>']\n",
            "GCN ['<ul>', '<li>']\n",
            "Concept 1: Defining tensors ['<ul>', '<li>']\n",
            "Concept 2: Evaluating ops ['<ul>', '<li>']\n",
            "Concept 3: Interactive session ['<ul>', '<li>']\n",
            "Concept 4: Session loggings ['<ul>', '<li>']\n",
            "Concept 5: Variables ['<ul>', '<li>']\n",
            "Concept 6: Saving variables ['<ul>', '<li>']\n",
            "Concept 7: Loading variables ['<ul>', '<li>']\n",
            "Concept 8: TensorBoard ['<ul>', '<li>']\n",
            "Concept 1: Linear regression ['<ul>', '<li>']\n",
            "Concept 2: Polynomial regression ['<ul>', '<li>']\n",
            "Concept 3: Regularization ['<ul>', '<li>']\n",
            "Concept 1: Linear regression for classification ['<ul>', '<li>']\n",
            "Concept 2: Logistic regression ['<ul>', '<li>']\n",
            "Concept 3: 2D Logistic regression ['<ul>', '<li>']\n",
            "Concept 4: Softmax classification ['<ul>', '<li>']\n",
            "Concept 1: Clustering ['<ul>', '<li>']\n",
            "Concept 2: Segmentation ['<ul>', '<li>']\n",
            "Concept 3: Self-organizing map ['<ul>', '<li>']\n",
            "Concept 1: Forward algorithm ['<ul>', '<li>']\n",
            "Concept 2: Viterbi decode ['<ul>', '<li>']\n",
            "Concept 1: Autoencoder ['<ul>', '<li>']\n",
            "Concept 2: Applying an autoencoder to images ['<ul>', '<li>']\n",
            "Concept 3: Denoising autoencoder ['<ul>', '<li>']\n",
            "Concept 1: Reinforcement learning ['<ul>', '<li>']\n",
            "Concept 1: Using CIFAR-10 dataset ['<ul>', '<li>']\n",
            "Concept 2: Convolutions ['<ul>', '<li>']\n",
            "Concept 3: Convolutional neural network ['<ul>', '<li>']\n",
            "Concept 1: Loading timeseries data ['<ul>', '<li>']\n",
            "Concept 2: Recurrent neural networks ['<ul>', '<li>']\n",
            "Concept 3: Applying RNN to real-world data for timeseries prediction ['<ul>', '<li>']\n",
            "Concept 1: Multi-cell RNN ['<ul>', '<li>']\n",
            "Concept 2: Embedding lookup ['<ul>', '<li>']\n",
            "Concept 3: Seq2seq model ['<ul>', '<li>']\n",
            "Concept 1: RankNet ['<ul>', '<li>']\n",
            "Concept 2: Image embedding ['<ul>', '<li>']\n",
            "Concept 3: Image ranking ['<ul>', '<li>']\n",
            "Python 3.8 support ['<ul>', '<li>', '<p>']\n",
            "64 bit Windows support ['<ul>', '<li>', '<p>']\n",
            "Legacy &amp; low-end CPU (without AVX) support ['<ul>', '<li>', '<p>']\n",
            "If your CPU didn't support AVX instructions, you will get ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed. (Win 10) or ImportError: DLL load failed with error code -1073741795 (Win 7) when using tensorflow official release 1.6.0 and up (pip install tensorflow) ['<ul>', '<li>', '<p>']\n",
            "You can use pip install &lt;filename.whl&gt; which file download from sse2 folder instead of using official AVX binary. ['<ul>', '<li>', '<p>']\n",
            "Compiler ['<li>']\n",
            "CUDA/cuDNN ['<li>']\n",
            "SIMD ['<li>']\n",
            "Notes ['<li>']\n",
            "2.4.0\\py38\\CPU+GPU\\cuda111cudnn8sse2 ['<li>']\n",
            "VS2019 16.8 ['<li>']\n",
            "11.1.1_456.81/8.0.5.39 ['<li>']\n",
            "x86_64 ['<li>']\n",
            "Python 3.8/compute_35 ['<li>']\n",
            "2.4.0\\py38\\CPU+GPU\\cuda111cudnn8avx2 ['<li>']\n",
            "AVX2 ['<li>']\n",
            "Python 3.8/compute_35,sm_50,sm_52,sm_61,sm_70,sm_75,compute_86 ['<li>']\n",
            "2.3.0\\py38\\CPU+GPU\\cuda110cudnn8sse2 ['<li>']\n",
            "VS2019 16.6 ['<li>']\n",
            "11.0.2_451.48/8.0.2.39 ['<li>']\n",
            "2.3.0\\py38\\CPU+GPU\\cuda110cudnn8avx2 ['<li>']\n",
            "Python 3.8/compute_35,sm_50,sm_52,sm_61,sm_70,compute_75 ['<li>']\n",
            "2.2.0\\py37\\CPU+GPU\\cuda102cudnn76sse2 ['<li>']\n",
            "VS2019 16.5 ['<li>']\n",
            "10.2.89_441.22/7.6.5.32 ['<li>']\n",
            "Python 3.7/Compute 3.0 ['<li>']\n",
            "2.2.0\\py37\\CPU+GPU\\cuda102cudnn76avx2 ['<li>']\n",
            "Python 3.7/Compute 3.0,3.5,5.0,5.2,6.1,7.0,7.5 ['<li>']\n",
            "2.1.0\\py37\\CPU+GPU\\cuda102cudnn76sse2 ['<li>']\n",
            "VS2019 16.4 ['<li>']\n",
            "2.1.0\\py37\\CPU+GPU\\cuda102cudnn76avx2 ['<li>']\n",
            "2.0.0\\py37\\CPU\\sse2 ['<li>']\n",
            "VS2019 16.3 ['<li>']\n",
            "No ['<li>']\n",
            "Python 3.7 ['<li>']\n",
            "2.0.0\\py37\\CPU\\avx2 ['<li>']\n",
            "2.0.0\\py37\\GPU\\cuda101cudnn76sse2 ['<li>']\n",
            "10.1.243_426.00/7.6.4.38 ['<li>']\n",
            "2.0.0\\py37\\GPU\\cuda101cudnn76avx2 ['<li>']\n",
            "1.15.0\\py37\\CPU+GPU\\cuda101cudnn76sse2 ['<li>']\n",
            "1.15.0\\py37\\CPU+GPU\\cuda101cudnn76avx2 ['<li>']\n",
            "1.14.0\\py37\\CPU\\sse2 ['<li>']\n",
            "VS2019 16.1 ['<li>']\n",
            "1.14.0\\py37\\CPU\\avx2 ['<li>']\n",
            "1.14.0\\py37\\GPU\\cuda101cudnn76sse2 ['<li>']\n",
            "10.1.168_425.25/7.6.0.64 ['<li>']\n",
            "1.14.0\\py37\\GPU\\cuda101cudnn76avx2 ['<li>']\n",
            "1.13.1\\py37\\CPU\\sse2 ['<li>']\n",
            "VS2017 15.9 ['<li>']\n",
            "1.13.1\\py37\\CPU\\avx2 ['<li>']\n",
            "1.13.1\\py37\\GPU\\cuda101cudnn75sse2 ['<li>']\n",
            "10.1.105_418.96/7.5.0.56 ['<li>']\n",
            "1.13.1\\py37\\GPU\\cuda101cudnn75avx2 ['<li>']\n",
            "1.12.0\\py36\\CPU\\sse2 ['<li>']\n",
            "VS2017 15.8 ['<li>']\n",
            "Python 3.6 ['<li>']\n",
            "1.12.0\\py36\\CPU\\avx2 ['<li>']\n",
            "1.12.0\\py36\\GPU\\cuda100cudnn73sse2 ['<li>']\n",
            "10.0.130_411.31/7.3.1.20 ['<li>']\n",
            "Python 3.6/Compute 3.0 ['<li>']\n",
            "1.12.0\\py36\\GPU\\cuda100cudnn73avx2 ['<li>']\n",
            "Python 3.6/Compute 3.0,3.5,5.0,5.2,6.1,7.0,7.5 ['<li>']\n",
            "1.12.0\\py37\\CPU\\sse2 ['<li>']\n",
            "1.12.0\\py37\\CPU\\avx2 ['<li>']\n",
            "1.12.0\\py37\\GPU\\cuda100cudnn73sse2 ['<li>']\n",
            "1.12.0\\py37\\GPU\\cuda100cudnn73avx2 ['<li>']\n",
            "1.11.0\\py36\\CPU\\sse2 ['<li>']\n",
            "1.11.0\\py36\\CPU\\avx2 ['<li>']\n",
            "1.11.0\\py36\\GPU\\cuda100cudnn73sse2 ['<li>']\n",
            "10.0.130_411.31/7.3.0.29 ['<li>']\n",
            "1.11.0\\py36\\GPU\\cuda100cudnn73avx2 ['<li>']\n",
            "1.11.0\\py37\\CPU\\sse2 ['<li>']\n",
            "1.11.0\\py37\\CPU\\avx2 ['<li>']\n",
            "1.11.0\\py37\\GPU\\cuda100cudnn73sse2 ['<li>']\n",
            "1.11.0\\py37\\GPU\\cuda100cudnn73avx2 ['<li>']\n",
            "1.10.0\\py36\\CPU\\sse2 ['<li>']\n",
            "1.10.0\\py36\\CPU\\avx2 ['<li>']\n",
            "1.10.0\\py36\\GPU\\cuda92cudnn72sse2 ['<li>']\n",
            "9.2.148.1/7.2.1.38 ['<li>']\n",
            "1.10.0\\py36\\GPU\\cuda92cudnn72avx2 ['<li>']\n",
            "Python 3.6/Compute 3.0,3.5,5.0,5.2,6.1,7.0 ['<li>']\n",
            "1.10.0\\py27\\CPU\\sse2 ['<li>']\n",
            "Python 2.7 ['<ul>', '<li>']\n",
            "1.10.0\\py27\\CPU\\avx2 ['<li>']\n",
            "1.10.0\\py27\\GPU\\cuda92cudnn72sse2 ['<li>']\n",
            "Python 2.7/Compute 3.0 ['<li>']\n",
            "1.10.0\\py27\\GPU\\cuda92cudnn72avx2 ['<li>']\n",
            "Python 2.7/Compute 3.0,3.5,5.0,5.2,6.1,7.0 ['<li>']\n",
            "1.9.0\\py36\\CPU\\sse2 ['<li>']\n",
            "VS2017 15.7 ['<li>']\n",
            "1.9.0\\py36\\CPU\\avx2 ['<li>']\n",
            "1.9.0\\py36\\GPU\\cuda92cudnn71sse2 ['<li>']\n",
            "9.2.148/7.1.4 ['<li>']\n",
            "1.9.0\\py36\\GPU\\cuda92cudnn71avx2 ['<li>']\n",
            "1.9.0\\py27\\CPU\\sse2 ['<li>']\n",
            "1.9.0\\py27\\CPU\\avx2 ['<li>']\n",
            "1.9.0\\py27\\GPU\\cuda92cudnn71sse2 ['<li>']\n",
            "1.9.0\\py27\\GPU\\cuda92cudnn71avx2 ['<li>']\n",
            "1.8.0\\py36\\CPU\\sse2 ['<li>']\n",
            "VS2017 15.4 ['<li>']\n",
            "1.8.0\\py36\\CPU\\avx2 ['<li>']\n",
            "1.8.0\\py36\\GPU\\cuda91cudnn71sse2 ['<li>']\n",
            "9.1.85.3/7.1.3 ['<li>']\n",
            "1.8.0\\py36\\GPU\\cuda91cudnn71avx2 ['<li>']\n",
            "1.8.0\\py27\\CPU\\sse2 ['<li>']\n",
            "1.8.0\\py27\\CPU\\avx2 ['<li>']\n",
            "1.8.0\\py27\\GPU\\cuda91cudnn71sse2 ['<li>']\n",
            "1.8.0\\py27\\GPU\\cuda91cudnn71avx2 ['<li>']\n",
            "1.7.0\\py36\\CPU\\sse2 ['<li>']\n",
            "1.7.0\\py36\\CPU\\avx2 ['<li>']\n",
            "1.7.0\\py36\\GPU\\cuda91cudnn71sse2 ['<li>']\n",
            "9.1.85.3/7.1.2 ['<li>']\n",
            "1.7.0\\py36\\GPU\\cuda91cudnn71avx2 ['<li>']\n",
            "1.7.0\\py27\\CPU\\sse2 ['<li>']\n",
            "1.7.0\\py27\\CPU\\avx2 ['<li>']\n",
            "1.7.0\\py27\\GPU\\cuda91cudnn71sse2 ['<li>']\n",
            "1.7.0\\py27\\GPU\\cuda91cudnn71avx2 ['<li>']\n",
            "1.6.0\\py36\\CPU\\sse2 ['<li>']\n",
            "1.6.0\\py36\\CPU\\avx2 ['<li>']\n",
            "1.6.0\\py36\\GPU\\cuda91cudnn71sse2 ['<li>']\n",
            "9.1.85.3/7.1.1 ['<li>']\n",
            "1.6.0\\py36\\GPU\\cuda91cudnn71avx2 ['<li>']\n",
            "1.6.0\\py27\\CPU\\sse2 ['<li>']\n",
            "1.6.0\\py27\\CPU\\avx2 ['<li>']\n",
            "1.6.0\\py27\\GPU\\cuda91cudnn71sse2 ['<li>']\n",
            "9.1.85.2/7.1.1 ['<li>']\n",
            "1.6.0\\py27\\GPU\\cuda91cudnn71avx2 ['<li>']\n",
            "1.5.0\\py36\\CPU\\avx ['<li>']\n",
            "AVX ['<li>']\n",
            "1.5.0\\py36\\CPU\\avx2 ['<li>']\n",
            "1.5.0\\py36\\GPU\\cuda91cudnn7avx2 ['<li>']\n",
            "9.1.85/7.0.5 ['<li>']\n",
            "1.5.0\\py27\\CPU\\sse2 ['<li>']\n",
            "1.5.0\\py27\\CPU\\avx ['<li>']\n",
            "1.5.0\\py27\\CPU\\avx2 ['<li>']\n",
            "1.5.0\\py27\\GPU\\cuda91cudnn7sse2 ['<li>']\n",
            "1.5.0\\py27\\GPU\\cuda91cudnn7avx2 ['<li>']\n",
            "1.4.0\\py36\\CPU\\avx ['<li>']\n",
            "1.4.0\\py36\\CPU\\avx2 ['<li>']\n",
            "1.4.0\\py36\\GPU\\cuda91cudnn7avx2 ['<li>']\n",
            "1.3.0\\py36\\CPU\\avx ['<li>']\n",
            "VS2015 Update 3 ['<li>']\n",
            "1.3.0\\py36\\CPU\\avx2 ['<li>']\n",
            "1.3.0\\py36\\GPU\\cuda8cudnn6avx2 ['<li>']\n",
            "8.0.61.2/6.0.21 ['<li>']\n",
            "Python 3.6/Compute 3.0,3.5,5.0,5.2,6.1 ['<li>']\n",
            "1.2.1\\py36\\CPU\\avx ['<li>']\n",
            "1.2.1\\py36\\CPU\\avx2 ['<li>']\n",
            "1.2.1\\py36\\GPU\\cuda8cudnn6avx2 ['<li>']\n",
            "1.1.0\\py36\\CPU\\avx ['<li>']\n",
            "1.1.0\\py36\\CPU\\avx2 ['<li>']\n",
            "1.1.0\\py36\\GPU\\cuda8cudnn6avx2 ['<li>']\n",
            "1.0.0\\py36\\CPU\\sse2 ['<li>']\n",
            "1.0.0\\py36\\CPU\\avx ['<li>']\n",
            "1.0.0\\py36\\CPU\\avx2 ['<li>']\n",
            "1.0.0\\py36\\GPU\\cuda8cudnn51sse2 ['<li>']\n",
            "8.0.61.2/5.1.10 ['<li>']\n",
            "1.0.0\\py36\\GPU\\cuda8cudnn51avx2 ['<li>']\n",
            "0.12.0\\py35\\CPU\\avx ['<li>']\n",
            "Python 3.5 ['<li>']\n",
            "0.12.0\\py35\\CPU\\avx2 ['<li>']\n",
            "0.12.0\\py35\\GPU\\cuda8cudnn51avx2 ['<li>']\n",
            "Python 3.5/Compute 3.0,3.5,5.0,5.2,6.1 ['<li>']\n",
            "If you hate the fucking tensorflow1.x very much, no worries! I have implemented a new YOLOv3 repo with TF2.0, and also made a chinese blog on how to implement YOLOv3 object detector from scratch. ['<blockquote>', '<p>']\n",
            "Clone this file ['<ol>', '<li>']\n",
            "You are supposed  to install some dependencies before getting out hands with these codes. ['<ol start=\"2\">', '<li>']\n",
            "$ cd tensorflow-yolov3 $ pip install -r ./docs/requirements.txt ['<pre lang=\"bashrc\">']\n",
            "Exporting loaded COCO weights as TF checkpoint(yolov3_coco.ckpt)【】 ['<ol start=\"3\">', '<li>']\n",
            "$ cd checkpoint $ wget https://github.com/YunYang1994/tensorflow-yolov3/releases/download/v1.0/yolov3_coco.tar.gz $ tar -xvf yolov3_coco.tar.gz $ cd .. $ python convert_weight.py $ python freeze_graph.py ['<pre lang=\"bashrc\">']\n",
            "Then you will get some .pb files in the root path.,  and run the demo script ['<ol start=\"4\">', '<li>']\n",
            "$ python image_demo.py $ python video_demo.py # if use camera, set video_path = 0 ['<pre lang=\"bashrc\">']\n",
            "xxx/xxx.jpg 18.19,6.32,424.13,421.83,20 323.86,2.65,640.0,421.94,20  xxx/xxx.jpg 48,240,195,371,11 8,12,352,498,14 # image_path x_min, y_min, x_max, y_max, class_id  x_min, y_min ,..., class_id  # make sure that x_max &lt; width and y_max &lt; height ['<pre>']\n",
            "person bicycle car ... toothbrush ['<pre>']\n",
            "$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar $ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar $ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar ['<pre lang=\"bashrc\">']\n",
            "VOC           # path:  /home/yang/dataset/VOC ├── test |    └──VOCdevkit |        └──VOC2007 (from VOCtest_06-Nov-2007.tar) └── train      └──VOCdevkit          └──VOC2007 (from VOCtrainval_06-Nov-2007.tar)          └──VOC2012 (from VOCtrainval_11-May-2012.tar)                       $ python scripts/voc_annotation.py --data_path /home/yang/test/VOC ['<pre lang=\"bashrc\">']\n",
            "__C.YOLO.CLASSES                = \"./data/classes/voc.names\" __C.TRAIN.ANNOT_PATH            = \"./data/dataset/voc_train.txt\" __C.TEST.ANNOT_PATH             = \"./data/dataset/voc_test.txt\" ['<pre lang=\"bashrc\">']\n",
            "$ python train.py $ tensorboard --logdir ./data ['<pre lang=\"bashrc\">']\n",
            "$ cd checkpoint $ wget https://github.com/YunYang1994/tensorflow-yolov3/releases/download/v1.0/yolov3_coco.tar.gz $ tar -xvf yolov3_coco.tar.gz $ cd .. $ python convert_weight.py --train_from_coco $ python train.py ['<pre lang=\"bashrc\">']\n",
            "$ python evaluate.py $ cd mAP $ python main.py -na ['<pre>']\n",
            "(中文版 | Chinese): ['<ul>', '<li>']\n",
            "(英文版 | English): ['<ul>', '<li>']\n",
            "(中文 | Chinese): ['<ul>', '<li>']\n",
            "(英文 | English): ['<ul>', '<li>']\n",
            "run make install for run all the follow commands. ['<blockquote>', '<p>']\n",
            "# https://www.ibm.com/developerworks/cn/opensource/os-sphinx-documentation/index.html pip install sphinx ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# for theme pip install sphinx_rtd_theme ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# for auto build pip install sphinx_autobuild ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "TensorFlow doesn't support  (which is what the original authors used), so we use . This may require a little bit more hyperparameter tuning to get nice results. ['<p>']\n",
            "client-side in your web browser without installing any software (using ['<p>']\n",
            "Running it for 500-2000 iterations seems to produce nice results. With certain images or output sizes, you might need some hyperparameter tuning (especially ['<p>', '<code>']\n",
            "--content-weight, --style-weight, and --learning-rate). ['<p>', '<code>']\n",
            "The following example demonstrates style blending, and was run for 1000 iterations to produce the result (with style blend weight parameters 0.8 and 0.2): ['<p>']\n",
            "The style input images were Picasso's \"Dora Maar\" and Starry Night, with the Picasso image having a style blend weight of 0.8 and Starry Night having a style blend weight of 0.2: ['<p>']\n",
            "--style-layer-weight-exp command line argument could be used to tweak how \"abstract\" the style transfer should be. Lower values mean that style transfer of a finer features will be favored over style transfer of a more coarse features, and vice versa. Default value is 1.0 - all layers treated equally. Somewhat extreme examples of what you can achieve: ['<p>']\n",
            "--content-weight-blend specifies the coefficient of content transfer layers. Default value - 1.0, style transfer tries to preserve finer grain content details. The value should be in range [0.0; 1.0]. ['<p>']\n",
            "--pooling allows to select which pooling layers to use (specify either max or avg). Original VGG topology uses max pooling, but the  suggests replacing it with average pooling. The outputs are perceptually different, max pool in general tends to have finer detail style transfer, but could have troubles at lower-freqency detail level: ['<p>']\n",
            "--preserve-colors boolean command line argument adds post-processing step, which combines colors from the original image and luma from the stylized image (YCbCr color space), thus producing color-preserving style transfer: ['<p>']\n",
            "(MD5 106118b7cf60435e6d8e04f6a6dc3657) - put it in the top level of this repository, or specify its location using the --network option. ['<ul>', '<li>']\n",
            "You can install Python dependencies using pip install -r requirements.txt, and it should just work. If you want to install the packages manually, here's a list: ['<p>']\n",
            "@misc{athalye2015neuralstyle,   author = {Anish Athalye},   title = {Neural Style},   year = {2015},   howpublished = {\\url{https://github.com/anishathalye/neural-style}},   note = {commit xxxxxxx} } ['<pre>']\n",
            "Notes: ['<blockquote>', '<p>', '<ol>', '<li>']\n",
            "The current version supports  and  datasets. The current test accuracy for MNIST is 99.64%, and Fashion-MNIST 90.60%, see details in the  section ['<blockquote>', '<p>', '<ol>', '<li>']\n",
            "See  for multi-GPU support ['<blockquote>', '<p>', '<ol>', '<li>']\n",
            "is an article explaining my understanding of the paper. It may be helpful in understanding the code. ['<blockquote>', '<p>', '<ol>', '<li>']\n",
            "Important: ['<blockquote>', '<p>']\n",
            "If you need to apply CapsNet model to your own datasets or build up a new model with the basic block of CapsNet, please follow my new project , which is an advanced library for capsule theory, aiming to integrate capsule-relevant technologies, provide relevant analysis tools, develop related application examples, and promote the development of capsule theory. For example, you can use capsule layer block in your code easily with the API capsLayer.layers.fully_connected and capsLayer.layers.conv2d ['<blockquote>', '<p>']\n",
            "Python ['<ul>', '<li>', '<p>', '<pre>']\n",
            "NumPy ['<ul>', '<li>']\n",
            "&gt;=1.3 ['<ul>', '<li>']\n",
            "tqdm (for displaying training progress info) ['<ul>', '<li>']\n",
            "scipy (for saving images) ['<ul>', '<li>']\n",
            "$ git clone https://github.com/naturomics/CapsNet-Tensorflow.git $ cd CapsNet-Tensorflow ['<pre>']\n",
            "a) Automatic downloading with download_data.py script ['<ul>', '<li>']\n",
            "$ python download_data.py   (for mnist dataset) $ python download_data.py --dataset fashion-mnist --save_to data/fashion-mnist (for fashion-mnist dataset) ['<pre>']\n",
            "b) Manual downloading with wget or other tools, move and extract dataset into data/mnist or data/fashion-mnist directory, for example: ['<ul>', '<li>']\n",
            "$ mkdir -p data/mnist $ wget -c -P data/mnist http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz $ wget -c -P data/mnist http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz $ wget -c -P data/mnist http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz $ wget -c -P data/mnist http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz $ gunzip data/mnist/*.gz ['<pre>']\n",
            "$ python main.py $ # or training for fashion-mnist dataset $ python main.py --dataset fashion-mnist $ # If you need to monitor the training process, open tensorboard with this command $ tensorboard --logdir=logdir $ # or use `tail` command on linux system $ tail -f results/val_acc.csv ['<pre>']\n",
            "$ python main.py --is_training=False $ # for fashion-mnist dataset $ python main.py --dataset fashion-mnist --is_training=False ['<pre>']\n",
            "Note: The default parameters of batch size is 128, and epoch 50. You may need to modify the config.py file or use command line parameters to suit your case, e.g. set batch size to 64 and do once test summary every 200 steps: python main.py  --test_sum_freq=200 --batch_size=48 ['<blockquote>', '<p>']\n",
            "training loss ['<ul>', '<li>']\n",
            "The best val error(using reconstruction) ['<ul>', '<li>']\n",
            "Routing iteration ['<li>', '<em>']\n",
            "val error ['<li>', '<em>']\n",
            "0.36 ['<li>', '<em>']\n",
            "0.41 ['<li>', '<em>']\n",
            "Paper ['<li>', '<em>']\n",
            "0.29 ['<li>', '<em>']\n",
            "0.25 ['<li>', '<em>']\n",
            "My simple comments for capsule ['<blockquote>', '<p>', '<ol>', '<li>']\n",
            "A new version neural unit(vector in vector out, not scalar in scalar out) ['<blockquote>', '<p>', '<ol>', '<li>']\n",
            "The routing algorithm is similar to attention mechanism ['<blockquote>', '<p>', '<ol>', '<li>']\n",
            "Anyway, a great potential work, a lot to be built upon ['<blockquote>', '<p>', '<ol>', '<li>']\n",
            ": referred for some code optimizations ['<ul>', '<li>']\n",
            "Basics of  /  /   /  / ['<ol>', '<li>']\n",
            "Machine Learing Basics with TensorFlow:  /  / ['<ol>', '<li>']\n",
            "Multi-Layer Perceptron (MLP):  /  /  / ['<ol>', '<li>']\n",
            "Convolutional Neural Network (CNN):  /  /  / ['<ol>', '<li>']\n",
            "Using Pre-trained Model (VGG):  / ['<ol>', '<li>']\n",
            "Recurrent Neural Network (RNN):  /  /  /  / ['<ol>', '<li>']\n",
            "Word Embedding (Word2Vec):  / ['<ol>', '<li>']\n",
            "Auto-Encoder Model:  /  / ['<ol>', '<li>']\n",
            "Class Activation Map (CAM): ['<ol>', '<li>']\n",
            "TensorBoard Usage:  /  / ['<ol>', '<li>']\n",
            "TensorFlow ['<ul>', '<li>']\n",
            "SciPy ['<ul>', '<li>']\n",
            "Pillow ['<ul>', '<li>']\n",
            "BeautifulSoup ['<ul>', '<li>']\n",
            ": inside 'data/' folder ['<ul>', '<li>']\n",
            "To find out about APIs for models, look at the README in each of the respective directories. In general, we try to hide tensors so the API can be used by non-machine learning experts. ['<p>']\n",
            "For those interested in contributing a model, please file a  to gauge interest. We are trying to add models that complement the existing set of models and can be used as building blocks in other apps. ['<p>']\n",
            "Type ['<p>', '<b>', '<code>']\n",
            "Demo ['<p>', '<b>', '<code>']\n",
            "Details ['<p>', '<b>', '<code>']\n",
            "Install ['<p>', '<b>', '<code>']\n",
            "Images ['<p>', '<b>', '<code>']\n",
            "Classify images with labels from the . ['<p>', '<b>', '<code>']\n",
            "npm i @tensorflow-models/mobilenet ['<p>', '<b>', '<code>']\n",
            "Real-time hand pose detection in the browser using TensorFlow.js. ['<p>', '<b>', '<code>']\n",
            "npm i @tensorflow-models/handpose ['<p>', '<b>', '<code>']\n",
            "A machine learning model which allows for real-time human pose estimation in the browser. See a detailed description . ['<p>', '<b>', '<code>']\n",
            "npm i @tensorflow-models/posenet ['<p>', '<b>', '<code>']\n",
            "Object detection model that aims to localize and identify multiple objects in a single image. Based on the . ['<p>', '<b>', '<code>']\n",
            "npm i @tensorflow-models/coco-ssd ['<p>', '<b>', '<code>']\n",
            "Real-time person and body part segmentation in the browser using TensorFlow.js. ['<p>', '<b>', '<code>']\n",
            "npm i @tensorflow-models/body-pix ['<p>', '<b>', '<code>']\n",
            "Real-time rapid Face detection in the browser using TensorFlow.js. ['<p>', '<b>', '<code>']\n",
            "npm i @tensorflow-models/blazeface ['<p>', '<b>', '<code>']\n",
            "Semantic segmentation ['<p>', '<b>', '<code>']\n",
            "npm i @tensorflow-models/deeplab ['<p>', '<b>', '<code>']\n",
            "Audio ['<p>', '<b>', '<code>']\n",
            "Classify 1 second audio snippets from the . ['<p>', '<b>', '<code>']\n",
            "npm i @tensorflow-models/speech-commands ['<p>', '<b>', '<code>']\n",
            "Text ['<p>']\n",
            "Encode text into a 512-dimensional embedding to be used as inputs to natural language processing tasks such as sentiment classification and textual similarity. ['<p>', '<b>', '<code>']\n",
            "npm i @tensorflow-models/universal-sentence-encoder ['<p>', '<b>', '<code>']\n",
            "Score the perceived impact a comment might have on a conversation, from \"Very toxic\" to \"Very healthy\". ['<p>', '<b>', '<code>']\n",
            "npm i @tensorflow-models/toxicity ['<p>', '<b>', '<code>']\n",
            "General Utilities ['<p>', '<b>', '<code>']\n",
            "This package provides a utility for creating a classifier using the K-Nearest Neighbors algorithm. Can be used for transfer learning. ['<p>', '<b>', '<code>']\n",
            "npm i @tensorflow-models/knn-classifier ['<p>', '<b>', '<code>']\n",
            "Deep Learning with Keras and Tensorflow ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Author: Valerio Maggio ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Contacts: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "valeriomaggio_at_gmail ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "git clone https://github.com/leriomaggio/deep-learning-keras-tensorflow.git ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Table of Contents ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Part I: Introduction ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Intro to Artificial Neural Networks ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Perceptron and MLP ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "naive pure-Python implementation ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "fast forward, sgd, backprop ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Introduction to Deep Learning Frameworks ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Intro to Theano ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Intro to Tensorflow ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Intro to Keras ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Overview and main features ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Overview of the core layers ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Multi-Layer Perceptron and Fully Connected ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Examples with keras.models.Sequential and Dense ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Keras Backend ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Part II: Supervised Learning ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Fully Connected Networks and Embeddings ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Intro to MNIST Dataset ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Hidden Leayer Representation and Embeddings ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "meaning of convolutional filters ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "examples from ImageNet ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Visualising ConvNets ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Advanced CNN ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Dropout ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "MaxPooling ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Batch Normalisation ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "HandsOn: MNIST Dataset ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "FC and MNIST ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "CNN and MNIST ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Deep Convolutiona Neural Networks with Keras (ref: keras.applications) ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "VGG19 ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "ResNet50 ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Transfer Learning and FineTuning ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Hyperparameters Optimisation ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Part III: Unsupervised Learning ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "AutoEncoders and Embeddings ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "AutoEncoders and MNIST ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "word2vec and doc2vec (gensim) with keras.datasets ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "word2vec and CNN ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Part IV: Recurrent Neural Networks ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Recurrent Neural Network in Keras ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "SimpleRNN, LSTM, GRU ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "LSTM for Sentence Generation ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "PartV: Additional Materials: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Custom Layers in Keras ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Multi modal Network Topologies with Keras ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Requirements ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "This tutorial requires the following packages: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Python version 3.5 ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Python 3.4 should be fine as well ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "likely Python 2.7 would be also fine, but who knows? :P ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "numpy version 1.10 or later: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "scipy version 0.16 or later: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "matplotlib version 1.4 or later: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "pandas version 0.16 or later: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "scikit-learn version 0.15 or later: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "keras version 2.0 or later: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "tensorflow version 1.0 or later: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "ipython/jupyter version 4.0 or later, with notebook support ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "(Optional but recommended): ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "pyyaml ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "hdf5 and h5py (required if you use model saving/loading functions in keras) ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "NVIDIA cuDNN if you have NVIDIA GPUs on your machines. ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "The easiest way to get (most) these is to use an all-in-one installer such as  from Continuum. These are available for multiple architectures. ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Python Version ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "I'm currently running this tutorial with Python 3 on Anaconda ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "!python --version ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Python 3.5.2 ['<ol>', '<li>', '<p>', '<ul>']\n",
            "Setting the Environment ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "In this repository, files to re-create virtual env with conda are provided for Linux and OSX systems, namely deep-learning.yml and deep-learning-osx.yml, respectively. ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "To re-create the virtual environments (on Linux, for example): ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "conda env create -f deep-learning.yml ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "For OSX, just change the filename, accordingly. ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Notes about Installing Theano with GPU support ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "NOTE: Read this section only if after pip installing theano, it raises error in enabling the GPU support! ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Since version 0.9 Theano introduced the  in the stable release (it was previously only available in the development version). ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "The goal of libgpuarray is (from the documentation) make a common GPU ndarray (n dimensions array) that can be reused by all projects that is as future proof as possible, while keeping it easy to use for simple need/quick test. ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Here are some useful tips (hopefully) I came up with to properly install and configure theano on (Ubuntu) Linux with GPU support: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "[If you're using Anaconda] conda install theano pygpu should be just fine! ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Sometimes it is suggested to install pygpu using the conda-forge channel: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "conda install -c conda-forge pygpu ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "[Works with both Anaconda Python or Official CPython] ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Install libgpuarray from source: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Then, install pygpu from source: (in the same source folder) ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "python setup.py build &amp;&amp; python setup.py install ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "pip install theano. ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "After Theano is installed: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "echo \"[global] device = cuda floatX = float32 ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "[lib] cnmem = 1.0\" &gt; ~/.theanorc ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Installing Tensorflow ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "To date tensorflow comes in two different packages, namely tensorflow and tensorflow-gpu, whether you want to install the framework with CPU-only or GPU support, respectively. ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "For this reason, tensorflow has not been included in the conda envs and has to be installed separately. ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Tensorflow for CPU only: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "pip install tensorflow ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Tensorflow with GPU support: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "pip install tensorflow-gpu ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Note: NVIDIA Drivers and CuDNN must be installed and configured before hand. Please refer to the official ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "for further details. ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Important Note: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "All the code provided+ in this tutorial can run even if tensorflow is not installed, and so using theano as the (default) backend! ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "This is exactly the power of Keras! ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Therefore, installing tensorflow is not stricly required! ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "+: Apart from the 1.2 Introduction to Tensorflow tutorial, of course. ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Configure Keras with tensorflow ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "By default, Keras is configured with theano as backend. ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "If you want to use tensorflow instead, these are the simple steps to follow: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Create the keras.json (if it does not exist): ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "touch $HOME/.keras/keras.json ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Copy the following content into the file: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "{     \"epsilon\": 1e-07,     \"backend\": \"tensorflow\",     \"floatx\": \"float32\",     \"image_data_format\": \"channels_last\" } ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Verify it is properly configured: ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "!cat ~/.keras/keras.json ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "{ \t\"epsilon\": 1e-07, \t\"backend\": \"tensorflow\", \t\"floatx\": \"float32\", \t\"image_data_format\": \"channels_last\" } ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Test if everything is up&amp;running ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "1. Check import ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "import scipy as sp ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "import pandas as pd ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "import matplotlib.pyplot as plt ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "import sklearn ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "import keras ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Using TensorFlow backend. ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "2. Check installed Versions ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "import numpy ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "print('numpy:', numpy.__version__) ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "import scipy ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "print('scipy:', scipy.__version__) ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "import matplotlib ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "print('matplotlib:', matplotlib.__version__) ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "import IPython ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "print('iPython:', IPython.__version__) ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "print('scikit-learn:', sklearn.__version__) ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "numpy: 1.11.1 scipy: 0.18.0 matplotlib: 1.5.2 iPython: 5.1.0 scikit-learn: 0.18 ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "print('keras: ', keras.__version__) ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "# optional ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "import theano ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "print('Theano: ', theano.__version__) ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "print('Tensorflow: ', tf.__version__) ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "keras:  2.0.2 Theano:  0.9.0 Tensorflow:  1.0.1 ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "If everything worked till down here, you're ready to start! ['<div>', '<h1>', '<br/>', '<h3>', '<h4>', '<div highlight highlight-source-shell\">', '<hr/>', '<h2>', '<ul>', '<li>', '<p>', '<div highlight highlight-source-python\">', '<pre>', '<ol>', '<ol start=\"2\">', '<code>', '<ol start=\"3\">', '<span pl-k\">', '<span pl-en\">', '<span pl-c\">']\n",
            "Deep Q-network and Q-learning ['<ol>', '<li>', '<ul>']\n",
            "Experience replay memory ['<ol>', '<li>', '<ul>']\n",
            "to reduce the correlations between consecutive updates ['<ol>', '<li>', '<ul>']\n",
            "Network for Q-learning targets are fixed for intervals ['<ol>', '<li>', '<ul>']\n",
            "to reduce the correlations between target and predicted Q-values ['<ol>', '<li>', '<ul>']\n",
            "or ['<p>', '<em>', '<code>']\n",
            "$ python main.py --env_name=Breakout-v0 --is_train=True $ python main.py --env_name=Breakout-v0 --is_train=True --display=True ['<pre>']\n",
            "$ python main.py --is_train=False $ python main.py --is_train=False --display=True ['<pre>']\n",
            "This README gives an overview of key concepts in TensorBoard, as well as how to interpret the visualizations TensorBoard provides. For an in-depth example of using TensorBoard, see the tutorial: . Documentation on how to use TensorBoard to work with images, graphs, hyper parameters, and more are linked from there, along with tutorial walk-throughs in Colab. ['<p>']\n",
            "You may also be interested in the hosted TensorBoard solution at ['<p>']\n",
            ". You can use TensorBoard.dev to easily host, track, and share your ML experiments for free. For example,  shows a working example featuring the scalars, graphs, histograms, distributions, and hparams dashboards. ['<p>']\n",
            "TensorBoard is designed to run entirely offline, without requiring any access to the Internet. For instance, this may be on your local machine, behind a corporate firewall, or in a datacenter. ['<p>']\n",
            "# sess.graph contains the graph definition; that enables the Graph Visualizer. ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "file_writer = tf.summary.FileWriter('/path/to/logs', sess.graph) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "For more details, see ['<p>']\n",
            ". Once you have event files, run TensorBoard and provide the log directory. If you're using a precompiled TensorFlow package (e.g. you installed via pip), run: ['<p>']\n",
            "bazel build tensorboard:tensorboard ./bazel-bin/tensorboard/tensorboard --logdir path/to/logs ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# or even more succinctly bazel run tensorboard -- --logdir path/to/logs ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "The first step in using TensorBoard is acquiring data from your TensorFlow run. For this, you need ['<p>']\n",
            ". Summary ops are ops, just like ['<p>']\n",
            ", which means they take in tensors, produce tensors, and are evaluated from within a TensorFlow graph. However, summary ops have a twist: the Tensors they produce contain serialized protobufs, which are written to disk and sent to TensorBoard. To visualize the summary data in TensorBoard, you should evaluate the summary op, retrieve the result, and then write that result to disk using a summary.FileWriter. A full explanation, with examples, is in . ['<p>']\n",
            "When you make a summary op, you will also give it a tag. The tag is basically a name for the data recorded by that op, and will be used to organize the data in the frontend. The scalar and histogram dashboards organize data by tag, and group the tags into folders according to a directory/like/hierarchy. If you have a lot of tags, we recommend grouping them with slashes. ['<p>']\n",
            "summary.FileWriters take summary data from TensorFlow, and then write them to a specified directory, known as the logdir. Specifically, the data is written to an append-only record dump that will have \"tfevents\" in the filename. TensorBoard reads data from a full directory, and organizes it into the history of a single TensorFlow execution. ['<p>']\n",
            "Why does it read the whole directory, rather than an individual file? You might have been using ['<p>']\n",
            "to run your model, in which case if TensorFlow crashes, the supervisor will restart it from a checkpoint. When it restarts, it will start writing to a new events file, and TensorBoard will stitch the various event files together to produce a consistent history of what happened. ['<p>']\n",
            "You may want to visually compare multiple executions of your model; for example, suppose you've changed the hyperparameters and want to see if it's converging faster. TensorBoard enables this through different \"runs\". When TensorBoard is passed a logdir at startup, it recursively walks the directory tree rooted at ['<p>', '<code>']\n",
            "logdir looking for subdirectories that contain tfevents data. Every time it encounters such a subdirectory, it loads it as a new run, and the frontend will organize the data accordingly. ['<p>', '<code>']\n",
            "/some/path/mnist_experiments/ /some/path/mnist_experiments/run1/ /some/path/mnist_experiments/run1/events.out.tfevents.1456525581.name /some/path/mnist_experiments/run1/events.out.tfevents.1456525585.name /some/path/mnist_experiments/run2/ /some/path/mnist_experiments/run2/events.out.tfevents.1456525385.name /tensorboard --logdir /some/path/mnist_experiments ['<pre>']\n",
            "You may also pass a comma separated list of log directories, and TensorBoard will watch each directory. You can also assign names to individual log directories by putting a colon between the name and the path, as in ['<p>']\n",
            "TensorBoard's Scalar Dashboard visualizes scalar statistics that vary over time; for example, you might want to track the model's loss or learning rate. As described in Key Concepts, you can compare multiple runs, and the data is organized by tag. The line charts have the following interactions: ['<p>']\n",
            "Clicking on the small blue icon in the lower-left corner of each chart will expand the chart ['<ul>', '<li>', '<p>']\n",
            "Dragging a rectangular region on the chart will zoom in ['<ul>', '<li>', '<p>']\n",
            "Double clicking on the chart will zoom out ['<ul>', '<li>', '<p>']\n",
            "Mousing over the chart will produce crosshairs, with data values recorded in the run-selector on the left. ['<ul>', '<li>', '<p>']\n",
            "The HistogramDashboard displays how the statistical distribution of a Tensor has varied over time. It visualizes data recorded via tf.summary.histogram. Each chart shows temporal \"slices\" of data, where each slice is a histogram of the tensor at a given step. It's organized with the oldest timestep in the back, and the most recent timestep in front. By changing the Histogram Mode from \"offset\" to \"overlay\", the perspective will rotate so that every histogram slice is rendered as a line and overlaid with one another. ['<p>']\n",
            "The Distribution Dashboard is another way of visualizing histogram data from ['<p>', '<code>']\n",
            "tf.summary.histogram. It shows some high-level statistics on a distribution. Each line on the chart represents a percentile in the distribution over the data: for example, the bottom line shows how the minimum value has changed over time, and the line in the middle shows how the median has changed. Reading from top to bottom, the lines have the following meaning: [maximum, 93%, 84%, 69%, 50%, 31%, 16%, 7%, minimum] ['<p>', '<code>']\n",
            "These percentiles can also be viewed as standard deviation boundaries on a normal distribution: [maximum, μ+1.5σ, μ+σ, μ+0.5σ, μ, μ-0.5σ, μ-σ, μ-1.5σ, minimum] so that the colored regions, read from inside to outside, have widths ['<p>', '<code>']\n",
            "[σ, 2σ, 3σ] respectively. ['<p>', '<code>']\n",
            "The Image Dashboard can display pngs that were saved via a tf.summary.image. The dashboard is set up so that each row corresponds to a different tag, and each column corresponds to a run. Since the image dashboard supports arbitrary pngs, you can use this to embed custom visualizations (e.g. matplotlib scatterplots) into TensorBoard. This dashboard always shows you the latest image for each tag. ['<p>']\n",
            "The Audio Dashboard can embed playable audio widgets for audio saved via a ['<p>', '<code>']\n",
            "tf.summary.audio. The dashboard is set up so that each row corresponds to a different tag, and each column corresponds to a run. This dashboard always embeds the latest audio for each tag. ['<p>', '<code>']\n",
            "The Graph Explorer can visualize a TensorBoard graph, enabling inspection of the TensorFlow model. To get best use of the graph visualizer, you should use name scopes to hierarchically group the ops in your graph - otherwise, the graph may be difficult to decipher. For more information, including examples, see . ['<p>']\n",
            "The Embedding Projector allows you to visualize high-dimensional data; for example, you may view your input data after it has been embedded in a high- dimensional space by your model. The embedding projector reads data from your model checkpoint file, and may be configured with additional metadata, like a vocabulary file or sprite images. For more details, see . ['<p>']\n",
            "First, check that the directory passed to --logdir is correct. You can also verify this by navigating to the Scalars dashboard (under the \"Inactive\" menu) and looking for the log directory path at the bottom of the left sidebar. ['<p>']\n",
            "If you're loading from the proper path, make sure that event files are present. TensorBoard will recursively walk its logdir, it's fine if the data is nested under a subdirectory. Ensure the following shows at least one result: ['<p>']\n",
            "Update: After , TensorBoard no longer auto reloads every 30 seconds. To re-enable the behavior, please open the settings by clicking the gear icon in the top-right of the TensorBoard web interface, and enable \"Reload data\". ['<blockquote>', '<p>']\n",
            "Update: the  can now be used to poll all \"active\" files in a directory for new data, rather than the most recent one as described below. A file is \"active\" as long as it received new data within --reload_multifile_inactive_secs seconds ago, defaulting to 4000. ['<blockquote>', '<p>']\n",
            "This issue usually comes about because of how TensorBoard iterates through the ['<p>', '<code>']\n",
            "tfevents files: it progresses through the events file in timestamp order, and only reads one file at a time. Let's suppose we have files with timestamps a and b, where a&lt;b. Once TensorBoard has read all the events in a, it will never return to it, because it assumes any new events are being written in the more recent file. This could cause an issue if, for example, you have two ['<p>', '<code>']\n",
            "FileWriters simultaneously writing to the same directory. If you have multiple summary writers, each one should be writing to a separate directory. ['<p>', '<code>']\n",
            "Update: the  can now be used to poll all \"active\" files in a directory for new data, defined as any file that received new data within --reload_multifile_inactive_secs seconds ago, defaulting to 4000. ['<ul>', '<li>', '<p>', '<blockquote>']\n",
            "No. TensorBoard expects that only one events file will be written to at a time, and multiple summary writers means multiple events files. If you are running a distributed TensorFlow instance, we encourage you to designate a single worker as the \"chief\" that is responsible for all summary processing. See ['<p>']\n",
            "for an example. ['<p>']\n",
            "You may have multiple execution of TensorFlow that all wrote to the same log directory. Please have each TensorFlow run write to its own logdir. ['<ul>', '<li>', '<p>', '<blockquote>']\n",
            "You may have a bug in your code where the global_step variable (passed to FileWriter.add_summary) is being maintained incorrectly. ['<ul>', '<li>', '<p>', '<blockquote>']\n",
            "It may be that your TensorFlow job crashed, and was restarted from an earlier checkpoint. See How to handle TensorFlow restarts, below. ['<ul>', '<li>', '<p>', '<blockquote>']\n",
            "TensorFlow is designed with a mechanism for graceful recovery if a job crashes or is killed: TensorFlow can periodically write model checkpoint files, which enable you to restart TensorFlow without losing all your training progress. ['<p>']\n",
            "However, this can complicate things for TensorBoard; imagine that TensorFlow wrote a checkpoint at step a, and then continued running until step b, and then crashed and restarted at timestamp a. All of the events written between ['<p>', '<code>']\n",
            "a and b were \"orphaned\" by the restart event and should be removed. ['<p>', '<code>']\n",
            "To facilitate this, we have a SessionLog message in ['<p>', '<code>']\n",
            "tensorflow/core/util/event.proto which can record SessionStatus.START as an event; like all events, it may have a step associated with it. If TensorBoard detects a SessionStatus.START event with step a, it will assume that every event with a step greater than a was orphaned, and it will discard those events. This behavior may be disabled with the flag ['<p>', '<code>']\n",
            "--purge_orphaned_data false (in versions after 0.7). ['<p>', '<code>']\n",
            "The Scalar Dashboard supports exporting data; you can click the \"enable download links\" option in the left-hand bar. Then, each plot will provide download links for the data it contains. ['<p>']\n",
            "If you need access to the full dataset, you can read the event files that TensorBoard consumes by using the  method. ['<p>']\n",
            "Yes! You can clone and tinker with one of the  and make your own, amazing visualizations. More documentation on the plugin system is described in the  guide. Feel free to file feature requests or questions about plugin functionality. ['<p>']\n",
            "Once satisfied with your own groundbreaking new plugin, see the ['<p>']\n",
            "on how to publish to PyPI and share it with the community. ['<p>']\n",
            "Using the , you can create scalar plots with lines for custom run-tag pairs. However, within the original scalars dashboard, each scalar plot corresponds to data for a specific tag and contains lines for each run that includes that tag. ['<p>']\n",
            "Margin plots (that visualize lower and upper bounds) may be created with the ['<p>']\n",
            ". The original scalars plugin does not support visualizing margins. ['<p>']\n",
            "This isn't yet possible. As a workaround, you could create your custom plot in your own code (e.g. matplotlib) and then write it into an SummaryProto (core/framework/summary.proto) and add it to your FileWriter. Then, your custom plot will appear in the TensorBoard image tab. ['<p>']\n",
            "TensorBoard uses  to downsample your data so that it can be loaded into RAM. You can modify the number of elements it will keep per tag by using the --samples_per_plugin command line argument (ex: ['<p>', '<code>']\n",
            "--samples_per_plugin=scalars=500,images=20). See this  for some more information. ['<p>', '<code>']\n",
            "Versions of TensorBoard prior to TensorBoard 2.0 would by default serve on host ['<p>', '<code>']\n",
            "0.0.0.0, which is publicly accessible. For those versions of TensorBoard, you can stop the popups by specifying --host localhost at startup. ['<p>', '<code>']\n",
            "In TensorBoard 2.0 and up, --host localhost is the default. Use --bind_all to restore the old behavior of serving to the public network on both IPv4 and IPv6. ['<p>']\n",
            "TensorBoard 1.14+ can be run with a reduced feature set if you do not have TensorFlow installed. The primary limitation is that as of 1.14, only the following plugins are supported: scalars, custom scalars, image, audio, graph, projector (partial), distributions, histograms, text, PR curves, mesh. In addition, there is no support for log directories on Google Cloud Storage. ['<p>']\n",
            "First, try searching our  and ['<p>']\n",
            ". It may be that someone else has already had the same issue or question. ['<p>']\n",
            "If you have found a bug in TensorBoard, please  with as much supporting information as you can provide (e.g. attaching events files, including the output of tensorboard --inspect, etc.). ['<p>']\n",
            "numpy ['<ul>', '<li>']\n",
            "scipy ['<ul>', '<li>']\n",
            "six ['<ul>', '<li>']\n",
            "tensorflow () ['<ul>', '<li>']\n",
            "TensorFlowOnSpark brings scalable deep learning to Apache Hadoop and Apache Spark clusters. ['<blockquote>', '<p>']\n",
            "It enables both distributed TensorFlow training and inferencing on Spark clusters, with a goal to minimize the amount of code changes required to run existing TensorFlow programs on a shared grid.  Its Spark-compatible API helps manage the TensorFlow cluster with the following steps: ['<p>']\n",
            "Startup - launches the Tensorflow main function on the executors, along with listeners for data/control messages. ['<ol>', '<li>', '<ul>']\n",
            "Data ingestion ['<ol>', '<li>', '<ul>']\n",
            "InputMode.TENSORFLOW - leverages TensorFlow's built-in APIs to read data files directly from HDFS. ['<ol>', '<li>', '<ul>']\n",
            "InputMode.SPARK - sends Spark RDD data to the TensorFlow nodes via a TFNode.DataFeed class.  Note that we leverage the  to access TFRecords on HDFS. ['<ol>', '<li>', '<ul>']\n",
            "Shutdown - shuts down the Tensorflow workers and PS nodes on the executors. ['<ol>', '<li>', '<ul>']\n",
            "TensorFlowOnSpark provides some important benefits (see ) over alternative deep learning solutions. ['<p>']\n",
            "Easily migrate existing TensorFlow programs with &lt;10 lines of code change. ['<ul>', '<li>']\n",
            "Support all TensorFlow functionalities: synchronous/asynchronous training, model/data parallelism, inferencing and TensorBoard. ['<ul>', '<li>']\n",
            "Server-to-server direct communication achieves faster learning when available. ['<ul>', '<li>']\n",
            "Allow datasets on HDFS and other sources pushed by Spark or pulled by TensorFlow. ['<ul>', '<li>']\n",
            "Easily integrate with your existing Spark data processing pipelines. ['<ul>', '<li>']\n",
            "Easily deployed on cloud or on-premise and on CPUs or GPUs. ['<ul>', '<li>']\n",
            "# for tensorflow&gt;=2.0.0 pip install tensorflowonspark ['<pre>']\n",
            "# for tensorflow&lt;2.0.0 pip install tensorflowonspark==1.4.4 ['<pre>']\n",
            "python YOLO_(small or tiny)_tf.py argvs ['<pre>']\n",
            "where argvs are ['<pre>']\n",
            "-fromfile (input image filename) : input image file -disp_console (0 or 1) : whether display results on terminal or not -imshow (0 or 1) : whether display result image or not -tofile_img (output image filename) : output image file -tofile_txt (output txt filename) : output text file (contains class, x, y, w, h, probability) ['<pre>']\n",
            "import YOLO_(small or tiny)_tf yolo = YOLO_(small or tiny)_tf.YOLO_TF() ['<pre>']\n",
            "yolo.disp_console = (True or False, default = True) yolo.imshow = (True or False, default = True) yolo.tofile_img = (output image filename) yolo.tofile_txt = (output txt filename) yolo.filewrite_img = (True or False, default = False) yolo.filewrite_txt = (True of False, default = False) ['<pre>']\n",
            "yolo.detect_from_file(filename) yolo.detect_from_cvmat(cvmat) ['<pre>']\n",
            "Opencv2 ['<ul>', '<li>']\n",
            "Me and original author hold no liability for any damages ['<ul>', '<li>']\n",
            "Do not use this on commercial! ['<ul>', '<li>']\n",
            "是由实时维护的 TensorFlow 官方文档中文版，维护者为全球各大公司开发人员和各著名高校研究者及学生。欢迎大家加入维护团队，欢迎提 Issue 和 PR，参与之前请阅读。 ['<blockquote>', '<p>', '<ul>', '<li>']\n",
            "阅读文档请到 👉 ['<blockquote>', '<p>', '<ul>', '<li>']\n",
            "推荐学习顺序等更多内容详见： ['<blockquote>', '<p>', '<ul>', '<li>']\n",
            "相关术语表：， ['<blockquote>', '<p>', '<ul>', '<li>']\n",
            "掘金翻译计划欢迎大家的加入，详见 👉 ['<blockquote>', '<p>', '<ul>', '<li>']\n",
            "我们非常高兴发布 TensorFlow 的开发版，现在 pypi 提供开发版的 pip 包  和 ['<ul>', '<li>', '<code>']\n",
            "项目。在干净的环境中简单运行 pip install tf-nightly 或 pip install tf-nightly-gpu 即可安装 TensorFlow 开发版。 我们为 Linux、Mac 和 Windows 提供  CPU 和 GPU 支持。 ['<ul>', '<li>', '<code>']\n",
            "&gt;&gt;&gt; tf.enable_eager_execution() ['<div highlight highlight-source-python\">', '<span pl-c1\">', '<span pl-s\">']\n",
            "&gt;&gt;&gt; tf.add(1, 2) ['<div highlight highlight-source-python\">', '<span pl-c1\">', '<span pl-s\">']\n",
            "'Hello, TensorFlow!' ['<div highlight highlight-source-python\">', '<span pl-c1\">', '<span pl-s\">']\n",
            "MacOS ['<ul>', '<li>']\n",
            "IBM s390x ['<strong>', '<br/>']\n",
            "IBM ppc64le CPU ['<strong>', '<br/>']\n",
            "IBM ppc64le GPU ['<strong>', '<br/>']\n",
            "Linux CPU with Intel® MKL-DNN Nightly ['<strong>', '<br/>']\n",
            "Linux CPU with Intel® MKL-DNN Python 2.7 Linux CPU with Intel® MKL-DNN Python 3.5 Linux CPU with Intel® MKL-DNN Python 3.6 ['<strong>', '<br/>']\n",
            "You need CUDA-compatible GPUs to train the model. ['<ol>', '<li>']\n",
            "You should first download  and .WIDER Face for face detection and Celeba for landmark detection(This is required by original paper.But I found some labels were wrong in Celeba. So I use  for landmark detection). ['<ol>', '<li>']\n",
            "Tensorflow 1.2.1 ['<ul>', '<li>']\n",
            "TF-Slim ['<ul>', '<li>']\n",
            "Ubuntu 16.04 ['<ul>', '<li>']\n",
            "Cuda 8.0 ['<ul>', '<li>']\n",
            "Download Wider Face Training part only from Official Website , unzip to replace WIDER_train and put it into prepare_data folder. ['<ol>', '<li>']\n",
            "Download landmark training data from here,unzip and put them into prepare_data folder. ['<ol>', '<li>']\n",
            "Run prepare_data/gen_12net_data.py to generate training data(Face Detection Part) for PNet. ['<ol>', '<li>']\n",
            "Run gen_landmark_aug_12.py to generate training data(Face Landmark Detection Part) for PNet. ['<ol>', '<li>']\n",
            "Run gen_imglist_pnet.py to merge two parts of training data. ['<ol>', '<li>']\n",
            "Run gen_PNet_tfrecords.py to generate tfrecord for PNet. ['<ol>', '<li>']\n",
            "After training PNet, run gen_hard_example to generate training data(Face Detection Part) for RNet. ['<ol>', '<li>']\n",
            "Run gen_landmark_aug_24.py to generate training data(Face Landmark Detection Part) for RNet. ['<ol>', '<li>']\n",
            "Run gen_imglist_rnet.py to merge two parts of training data. ['<ol>', '<li>']\n",
            "Run gen_RNet_tfrecords.py to generate tfrecords for RNet.(you should run this script four times to generate tfrecords of neg,pos,part and landmark respectively) ['<ol>', '<li>']\n",
            "After training RNet, run gen_hard_example to generate training data(Face Detection Part) for ONet. ['<ol>', '<li>']\n",
            "Run gen_landmark_aug_48.py to generate training data(Face Landmark Detection Part) for ONet. ['<ol>', '<li>']\n",
            "Run gen_imglist_onet.py to merge two parts of training data. ['<ol>', '<li>']\n",
            "Run gen_ONet_tfrecords.py to generate tfrecords for ONet.(you should run this script four times to generate tfrecords of neg,pos,part and landmark respectively) ['<ol>', '<li>']\n",
            "When training PNet,I merge four parts of data(pos,part,landmark,neg) into one tfrecord,since their total number radio is almost 1:1:1:3.But when training RNet and ONet,I generate four tfrecords,since their total number is not balanced.During training,I read 64 samples from pos,part and landmark tfrecord and read 192 samples from neg tfrecord to construct mini-batch. ['<ul>', '<li>', '<p>']\n",
            "It's important for PNet and RNet to keep high recall radio.When using well-trained PNet to generate training data for RNet,I can get 14w+ pos samples.When using well-trained RNet to generate training data for ONet,I can get 19w+ pos samples. ['<ul>', '<li>', '<p>']\n",
            "Since MTCNN is a Multi-task Network,we should pay attention to the format of training data.The format is: ['<ul>', '<li>', '<p>']\n",
            "[path to image][cls_label][bbox_label][landmark_label] ['<ul>', '<li>', '<p>']\n",
            "For pos sample,cls_label=1,bbox_label(calculate),landmark_label=[0,0,0,0,0,0,0,0,0,0]. ['<ul>', '<li>', '<p>']\n",
            "For part sample,cls_label=-1,bbox_label(calculate),landmark_label=[0,0,0,0,0,0,0,0,0,0]. ['<ul>', '<li>', '<p>']\n",
            "For landmark sample,cls_label=-2,bbox_label=[0,0,0,0],landmark_label(calculate). ['<ul>', '<li>', '<p>']\n",
            "For neg sample,cls_label=0,bbox_label=[0,0,0,0],landmark_label=[0,0,0,0,0,0,0,0,0,0]. ['<ul>', '<li>', '<p>']\n",
            "Since the training data for landmark is less.I use transform,random rotate and random flip to conduct data augment(the result of landmark detection is not that good). ['<ul>', '<li>', '<p>']\n",
            "Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao , \" Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks,\" IEEE Signal Processing Letter ['<ol>', '<li>']\n",
            "To sample from a checkpointed model, python sample.py. Sampling while the learning is still in progress (to check last checkpoint) works only in CPU or using another GPU. To force CPU mode, use export CUDA_VISIBLE_DEVICES=\"\" and unset CUDA_VISIBLE_DEVICES afterward (resp. set CUDA_VISIBLE_DEVICES=\"\" and set CUDA_VISIBLE_DEVICES= on Windows). ['<p>']\n",
            "cd data mkdir sherlock ['<div highlight highlight-source-shell\">', '<span pl-c1\">']\n",
            "cd sherlock wget https://sherlock-holm.es/stories/plain-text/cnus.txt mv cnus.txt input.txt ['<div highlight highlight-source-shell\">', '<span pl-c1\">']\n",
            "Start with as much clean input.txt as possible e.g. 50MiB ['<ol>', '<li>']\n",
            "Start by establishing a baseline using the default settings. ['<ol>', '<li>']\n",
            "Use tensorboard to compare all of your runs visually to aid in experimenting. ['<ol>', '<li>']\n",
            "Tweak --rnn_size up somewhat from 128 if you have a lot of input data. ['<ol>', '<li>']\n",
            "Tweak --num_layers from 2 to 3 but no higher unless you have experience. ['<ol>', '<li>']\n",
            "Tweak --seq_length up from 50 based on the length of a valid input string (e.g. names are &lt;= 12 characters, sentences may be up to 64 characters, etc). An lstm cell will \"remember\" for durations longer than this sequence, but the effect falls off for longer character distances. ['<ol>', '<li>']\n",
            "Finally once you've done all that, only then would I suggest adding some dropout. Start with --output_keep_prob 0.8 and maybe end up with both --input_keep_prob 0.8 --output_keep_prob 0.5 only after exhausting all the above values. ['<ol>', '<li>']\n",
            "Add explanatory comments ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Expose more command-line arguments ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Compare accuracy and performance with char-rnn ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "More Tensorboard instrumentation ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Leave feedback in the issues ['<ul>', '<li>']\n",
            "Open a Pull Request ['<ul>', '<li>']\n",
            "Join the ['<ul>', '<li>']\n",
            "Share your success stories and data sets! ['<ul>', '<li>']\n",
            "Name ['<em>']\n",
            "Paper Link ['<p>', '<em>', '<strong>']\n",
            "Value Function ['<pre>', '<em>', '<strong>']\n",
            "GAN ['<em>']\n",
            "LSGAN ['<em>']\n",
            "WGAN_GP ['<em>']\n",
            "DRAGAN ['<em>']\n",
            "CGAN ['<em>']\n",
            "infoGAN ['<em>']\n",
            "ACGAN ['<em>']\n",
            "EBGAN ['<em>']\n",
            "BEGAN ['<em>']\n",
            "Epoch 2 ['<strong>', '<em>']\n",
            "Epoch 10 ['<em>']\n",
            "Epoch 25 ['<em>']\n",
            "Epoch 1 ['<em>']\n",
            "Epoch 20 ['<em>']\n",
            "Epoch 40 ['<em>']\n",
            "Without hyper-parameter tuning from mnist-version, ACGAN/infoGAN does not work well as compared with CGAN. ACGAN tends to fall into mode-collapse. infoGAN tends to ignore noise-vector. It results in that various style within the same class can not be represented. ['<p>']\n",
            "Loss Function ['<p>', '<em>', '<strong>']\n",
            "VAE ['<em>']\n",
            "CVAE ['<em>']\n",
            "DVAE ['<p>', '<em>', '<strong>']\n",
            "(to be added) ['<p>', '<em>', '<strong>']\n",
            "AAE ['<p>', '<em>', '<strong>']\n",
            "├── main.py # gateway ├── data │   ├── mnist # mnist data (not included in this repo) │   |   ├── t10k-images-idx3-ubyte.gz │   |   ├── t10k-labels-idx1-ubyte.gz │   |   ├── train-images-idx3-ubyte.gz │   |   └── train-labels-idx1-ubyte.gz │   └── fashion-mnist # fashion-mnist data (not included in this repo) │       ├── t10k-images-idx3-ubyte.gz │       ├── t10k-labels-idx1-ubyte.gz │       ├── train-images-idx3-ubyte.gz │       └── train-labels-idx1-ubyte.gz ├── GAN.py # vanilla GAN ├── ops.py # some operations on layer ├── utils.py # utils ├── logs # log files for tensorboard to be saved here └── checkpoint # model files to be saved here ['<pre>']\n",
            "Ubuntu ['<ul>', '<li>']\n",
            "Windows ['<ul>', '<li>']\n",
            "Mac OSX ['<ul>', '<li>']\n",
            "ops.py ['<ul>', '<li>']\n",
            "operations ['<ul>', '<li>']\n",
            "from ops import * ['<ul>', '<li>']\n",
            "utils.py ['<ul>', '<li>']\n",
            "image processing ['<ul>', '<li>']\n",
            "from utils import * ['<ul>', '<li>']\n",
            "def network(x, is_training=True, reuse=False, scope=\"network\"):     with tf.variable_scope(scope, reuse=reuse):         x = conv(...)                  ...                  return logit ['<div highlight highlight-source-python\">']\n",
            "Image_Data_Class = ImageData(img_size, img_ch, augment_flag) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "trainA_dataset = ['./dataset/cat/trainA/a.jpg',                    './dataset/cat/trainA/b.png',                    './dataset/cat/trainA/c.jpeg',                    ...] ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "trainA = tf.data.Dataset.from_tensor_slices(trainA_dataset) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "trainA = trainA.map(Image_Data_Class.image_processing, num_parallel_calls=16) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "trainA = trainA.shuffle(buffer_size=10000).prefetch(buffer_size=batch_size).batch(batch_size).repeat() ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "trainA_iterator = trainA.make_one_shot_iterator() ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "data_A = trainA_iterator.get_next() ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "logit = network(data_A) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "See  for more information. ['<ul>', '<li>']\n",
            "padding='SAME' ['<ul>', '<li>']\n",
            "pad = ceil[ (kernel - stride) / 2 ] ['<ul>', '<li>']\n",
            "pad_type ['<ul>', '<li>']\n",
            "'zero' or 'reflect' ['<ul>', '<li>']\n",
            "sn ['<ul>', '<li>']\n",
            "use  or not ['<ul>', '<li>']\n",
            "If you don't want to share variable, set all scope names differently. ['<ul>', '<li>']\n",
            "weight_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.02) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "weight_regularizer = tf.contrib.layers.l2_regularizer(0.0001) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "weight_regularizer_fully = tf.contrib.layers.l2_regularizer(0.0001) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "Xavier : tf.contrib.layers.xavier_initializer() ['<ul>', '<li>', '<div highlight highlight-source-python\">']\n",
            "USE \"\"\"tf.contrib.layers.variance_scaling_initializer()\"\"\"      if uniform :     factor = gain * gain     mode = 'FAN_AVG'   else :     factor = (gain * gain) / 1.3     mode = 'FAN_AVG' ['<ul>', '<li>', '<div highlight highlight-source-python\">']\n",
            "He : tf.contrib.layers.variance_scaling_initializer() ['<ul>', '<li>', '<div highlight highlight-source-python\">']\n",
            "if uniform :     factor = gain * gain     mode = 'FAN_IN'   else :     factor = (gain * gain) / 1.3     mode = 'FAN_OUT' ['<ul>', '<li>', '<div highlight highlight-source-python\">']\n",
            "Normal : tf.random_normal_initializer(mean=0.0, stddev=0.02) ['<ul>', '<li>', '<div highlight highlight-source-python\">']\n",
            "Truncated_normal : tf.truncated_normal_initializer(mean=0.0, stddev=0.02) ['<ul>', '<li>', '<div highlight highlight-source-python\">']\n",
            "Orthogonal : tf.orthogonal_initializer(1.0) / # if relu = sqrt(2), the others = 1.0 ['<ul>', '<li>', '<div highlight highlight-source-python\">']\n",
            "l2_decay : tf.contrib.layers.l2_regularizer(0.0001) ['<ul>', '<li>']\n",
            "orthogonal_regularizer : orthogonal_regularizer(0.0001) &amp; orthogonal_regularizer_fully(0.0001) ['<ul>', '<li>']\n",
            "down ===&gt; [height, width] -&gt; [height // scale_factor, width // scale_factor] ['<ul>', '<li>']\n",
            "up ===&gt; [height, width] -&gt; [height * scale_factor, width * scale_factor] ['<ul>', '<li>']\n",
            "x = resblock(x, channels=64, is_training=is_training, use_bias=True, sn=True, scope='residual_block') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = resblock_down(x, channels=64, is_training=is_training, use_bias=True, sn=True, scope='residual_block_down') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = resblock_up(x, channels=64, is_training=is_training, use_bias=True, sn=True, scope='residual_block_up') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "down ===&gt; [height, width] -&gt; [height // 2, width // 2] ['<ul>', '<li>']\n",
            "up ===&gt; [height, width] -&gt; [height * 2, width * 2] ['<ul>', '<li>']\n",
            "n_db ===&gt; The number of dense-block ['<ul>', '<li>']\n",
            "n_rdb ===&gt; The number of RDB ['<ul>', '<li>']\n",
            "n_rdb_conv ===&gt; per RDB conv layer ['<ul>', '<li>']\n",
            "x = self_attention(x, use_bias=True, sn=True, scope='self_attention') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = self_attention_with_pooling(x, use_bias=True, sn=True, scope='self_attention_version_2') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = squeeze_excitation(x, ratio=16, use_bias=True, sn=True, scope='squeeze_excitation') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = convolution_block_attention(x, ratio=16, use_bias=True, sn=True, scope='convolution_block_attention') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = global_context_block(x, use_bias=True, sn=True, scope='gc_block') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = srm_block(x, use_bias=False, is_training=is_training, scope='srm_block') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = batch_norm(x, is_training=is_training, scope='batch_norm') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = layer_norm(x, scope='layer_norm') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = instance_norm(x, scope='instance_norm') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = group_norm(x, groups=32, scope='group_norm') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = pixel_norm(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = batch_instance_norm(x, scope='batch_instance_norm') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = layer_instance_norm(x, scope='layer_instance_norm') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = switch_norm(x, scope='switch_norm') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = condition_batch_norm(x, z, is_training=is_training, scope='condition_batch_norm'): ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = adaptive_instance_norm(x, gamma, beta) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = adaptive_layer_instance_norm(x, gamma, beta, smoothing=True, scope='adaLIN') ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "See  for how to use condition_batch_norm ['<ul>', '<li>']\n",
            "See  for how to use adaptive_instance_norm ['<ul>', '<li>']\n",
            "See  for how to use adaptive_layer_instance_norm &amp; layer_instance_norm ['<ul>', '<li>']\n",
            "x = relu(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = lrelu(x, alpha=0.2) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = tanh(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = sigmoid(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = swish(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = elu(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = nearest_up_sample(x, scale_factor=2) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = bilinear_up_sample(x, scale_factor=2) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = nearest_down_sample(x, scale_factor=2) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = bilinear_down_sample(x, scale_factor=2) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = max_pooling(x, pool_size=2) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = avg_pooling(x, pool_size=2) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = global_max_pooling(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = global_avg_pooling(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = flatten(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "x = hw_flatten(x) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "loss, accuracy = classification_loss(logit, label) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "loss = dice_loss(n_classes=10, logit, label) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "If you want to use regularizer, then you should write it ['<ul>', '<li>']\n",
            "loss = L1_loss(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "loss = L2_loss(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "loss = huber_loss(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "loss = histogram_loss(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "loss = gram_style_loss(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "loss = color_consistency_loss(x, y) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "histogram_loss means the difference in the color distribution of the image pixel values. ['<ul>', '<li>']\n",
            "gram_style_loss means the difference between the styles using gram matrix. ['<ul>', '<li>']\n",
            "color_consistency_loss means the color difference between the generated image and the input image. ['<ul>', '<li>']\n",
            "Ra ['<ul>', '<li>']\n",
            "loss_func ['<ul>', '<li>']\n",
            "gan ['<ul>', '<li>']\n",
            "lsgan ['<ul>', '<li>']\n",
            "hinge ['<ul>', '<li>']\n",
            "wgan-gp ['<ul>', '<li>']\n",
            "dragan ['<ul>', '<li>']\n",
            "See  for how to use gradient_penalty ['<ul>', '<li>']\n",
            "텐서플로우 공식 사이트에서 제공하는 안내서의 대부분의 내용을 다루고 있으며, 공식 사이트에서 제공하는 소스 코드보다는 훨씬 간략하게 작성하였으므로 쉽게 개념을 익힐 수 있을 것 입니다. 또한, 모든 주석은 한글로(!) 되어 있습니다. ['<p>']\n",
            "다만, 이론에 대한 깊은 이해와 정확한 구현보다는, 다양한 기법과 모델에 대한 기초적인 개념과 텐서플로우의 기본적인 사용법 학습에 촛점을 두었으므로, 구현이 미흡하게 되어 있는 부분이 많음을 고려해 주세요. ['<p>']\n",
            "TensorFlow &gt;= 1.8.0 ['<ul>', '<li>']\n",
            "Python &gt;= 3.6.1 ['<ul>', '<li>']\n",
            "numpy &gt;= 1.14.3 ['<ul>', '<li>']\n",
            "matplotlib &gt;= 2.2.2 ['<ul>', '<li>']\n",
            "pillow &gt;= 5.1 ['<ul>', '<li>']\n",
            "머신러닝의 기본 개념과, 왜 텐서플로를 써야하는가에 대해 이야기합니다. ['<ul>', '<li>']\n",
            "텐서플로 설치방법과 주피터 소개 ['<ul>', '<li>']\n",
            "텐서플로우의 연산의 개념과 그래프를 실행하는 방법을 익힙니다. ['<ul>', '<li>']\n",
            "텐서플로우의 플레이스홀더와 변수의 개념을 익힙니다. ['<ul>', '<li>']\n",
            "단순한 선형 회귀 모형을 만들어봅니다. ['<ul>', '<li>']\n",
            "신경망을 구성하여 간단한 분류 모델을 만들어봅니다. ['<ul>', '<li>']\n",
            "여러개의 신경망을 구성하는 방법을 익혀봅니다. ['<ul>', '<li>']\n",
            "자연어 분석에 매우 중요하게 사용되는 Word2Vec 모델을 간단하게 구현해봅니다. ['<ul>', '<li>']\n",
            "학습시킨 모델을 저장하고 재사용하는 방법을 배워봅니다. ['<ul>', '<li>']\n",
            "텐서보드를 이용해 신경망의 구성과 손실값의 변화를 시각적으로 확인해봅니다. ['<ul>', '<li>']\n",
            "텐서보드에 히스토그램을 추가해봅니다. ['<ul>', '<li>']\n",
            "머신러닝 학습의 Hello World 와 같은 MNIST(손글씨 숫자 인식) 문제를 신경망으로 풀어봅니다. ['<ul>', '<li>']\n",
            "과적합 방지를 위해 많이 사용되는 Dropout 기법을 사용해봅니다. ['<ul>', '<li>']\n",
            "이미지 처리 분야에서 가장 유명한 신경망 모델인 CNN 을 이용하여 더 높은 인식률을 만들어봅니다. ['<ul>', '<li>']\n",
            "신경망 구성을 손쉽게 해 주는 High level API 인 layers 를 사용해봅니다. ['<ul>', '<li>']\n",
            "대표적인 비감독(Unsupervised) 학습 방법인 Autoencoder 를 사용해봅니다. ['<ul>', '<li>']\n",
            "2016년에 가장 관심을 많이 받았던 비감독 학습 방법인 GAN 을 구현해봅니다. ['<ul>', '<li>']\n",
            "GAN 을 응용하여 원하는 숫자의 손글씨 이미지를 생성하는 모델을 만들어봅니다. 이런 방식으로 흑백 사진을 컬러로 만든다든가, 또는 선화를 채색한다든가 하는 응용이 가능합니다. ['<ul>', '<li>']\n",
            "자연어 처리나 음성 처리 분야에 많이 사용되는 RNN 의 기본적인 사용법을 익힙니다. ['<ul>', '<li>']\n",
            "순서가 있는 데이터에 강한 RNN 특징을 이용해, 단어 중 첫 세글자를 주면 단어를 완성하는 모델을 구현해봅니다. ['<ul>', '<li>']\n",
            "챗봇, 번역, 이미지 캡셔닝등에 사용되는 시퀀스 학습/생성 모델인 Seq2Seq 을 구현해봅니다. ['<ul>', '<li>']\n",
            "Seq2Seq 모델을 이용해 간단한 챗봇을 만들어봅니다. ['<ul>', '<li>']\n",
            "(홍콩 과기대 김성훈 교수님 강좌) ['<ul>', '<li>']\n",
            "(내가 만듬) ['<ul>', '<li>']\n",
            "Mac OS에서 matplotlib를 사용하는 코드가 실행이 안되거나 에러가 나는 경우 ['<ul>', '<li>']\n",
            "~/.matplotlib/matplotlibrc 파일을 생성하고 backend: TkAgg 라는 설정을 추가해 주시면 됩니다. ['<ul>', '<li>']\n",
            "This code is very old and doesn't run on modern TensorFlow. Indeed, TF now includes a resnet model in its core library: ['<p>']\n",
            "Implemenation of .  Includes a tool to use He et al's published trained Caffe weights in TensorFlow. ['<p>']\n",
            "Be able to use the pre-trained model's that . The convert.py will convert the weights for use with TensorFlow. ['<ul>', '<li>', '<p>']\n",
            "Implemented in the style of ['<ul>', '<li>', '<p>']\n",
            "not using any classes and making heavy use of variable scope. It should be easily usable in other models. ['<ul>', '<li>', '<p>']\n",
            "Foundation to experiment with changes to ResNet like , , and 1D convolutions for audio. (Not yet implemented.) ['<ul>', '<li>', '<p>']\n",
            "ResNet is fully convolutional and the implementation should allow inputs to be any size. ['<ul>', '<li>', '<p>']\n",
            "Be able to train out of the box on CIFAR-10, 100, and ImageNet. (Implementation incomplete) ['<ul>', '<li>', '<p>']\n",
            "To convert the published Caffe pretrained model, run convert.py. However Caffe is annoying to install so I'm providing a download of the output of convert.py: ['<p>']\n",
            "This code depends on  or later because ResNet needs 1x1 convolutions with stride 2. TF 0.8 is not new enough. ['<ul>', '<li>', '<p>']\n",
            "The convert.py script checks that activations are similiar to the caffe version but it's not exactly the same. This is probably due to differences between how TF and Caffe handle padding. Also preprocessing is done with color-channel means instead of pixel-wise means. ['<ul>', '<li>', '<p>']\n",
            "This version of the tutorial requires . For using the stable TensorFlow versions, please consider other branches such as ['<p>']\n",
            "Sequence-to-sequence (seq2seq) models (, ['<p>']\n",
            ") have enjoyed great success in a variety of tasks such as machine translation, speech recognition, and text summarization. This tutorial gives readers a full understanding of seq2seq models and shows how to build a competitive seq2seq model from scratch. We focus on the task of Neural Machine Translation (NMT) which was the very first testbed for seq2seq models with wild ['<p>']\n",
            ". The included code is lightweight, high-quality, production-ready, and incorporated with the latest research ideas. We achieve this goal by: ['<p>']\n",
            "Using the recent decoder / attention wrapper ['<ol>', '<li>']\n",
            ", TensorFlow 1.2 data iterator ['<ol>', '<li>']\n",
            "Incorporating our strong expertise in building recurrent and seq2seq models ['<ol>', '<li>']\n",
            "Providing tips and tricks for building the very best NMT models and replicating ['<ol>', '<li>']\n",
            "We believe that it is important to provide benchmarks that people can easily replicate. As a result, we have provided full experimental results and pretrained our models on the following publicly available datasets: ['<p>']\n",
            "Small-scale: English-Vietnamese parallel corpus of TED talks (133K sentence pairs) provided by the ['<ol>', '<li>']\n",
            "Large-scale: German-English parallel corpus (4.5M sentence pairs) provided by the . ['<ol>', '<li>']\n",
            "We first build up some basic knowledge about seq2seq models for NMT, explaining how to build and train a vanilla NMT model. The second part will go into details of building a competitive NMT model with attention mechanism. We then discuss tips and tricks to build the best possible NMT models (both in speed and translation quality) such as TensorFlow best practices (batching, bucketing), bidirectional RNNs, beam search, as well as scaling up to multiple GPUs using GNMT attention. ['<p>']\n",
            "Back in the old days, traditional phrase-based translation systems performed their task by breaking up source sentences into multiple chunks and then translated them phrase-by-phrase. This led to disfluency in the translation outputs and was not quite like how we, humans, translate. We read the entire source sentence, understand its meaning, and then produce a translation. Neural Machine Translation (NMT) mimics that! ['<p>']\n",
            "Figure 1. Encoder-decoder architecture – example of a general approach for NMT. An encoder converts a source sentence into a \"meaning\" vector which is passed through a decoder to produce a translation. ['<p align=\"center\">', '<br/>']\n",
            "Specifically, an NMT system first reads the source sentence using an encoder to build a ['<p>', '<em>']\n",
            ", a sequence of numbers that represents the sentence meaning; a decoder, then, processes the sentence vector to emit a translation, as illustrated in Figure 1. This is often referred to as the encoder-decoder architecture. In this manner, NMT addresses the local translation problem in the traditional phrase-based approach: it can capture long-range dependencies in languages, e.g., gender agreements; syntax structures; etc., and produce much more fluent translations as demonstrated by ['<p>', '<em>']\n",
            "NMT models vary in terms of their exact architectures. A natural choice for sequential data is the recurrent neural network (RNN), used by most NMT models. Usually an RNN is used for both the encoder and decoder. The RNN models, however, differ in terms of: (a) directionality – unidirectional or bidirectional; (b) depth – single- or multi-layer; and (c) type – often either a vanilla RNN, a Long Short-term Memory (LSTM), or a gated recurrent unit (GRU). Interested readers can find more information about RNNs and LSTM on this . ['<p>']\n",
            "In this tutorial, we consider as examples a deep multi-layer RNN which is unidirectional and uses LSTM as a recurrent unit. We show an example of such a model in Figure 2. In this example, we build a model to translate a source sentence \"I am a student\" into a target sentence \"Je suis étudiant\". At a high level, the NMT model consists of two recurrent neural networks: the encoder RNN simply consumes the input source words without making any prediction; the ['<p>', '<em>']\n",
            "decoder, on the other hand, processes the target sentence while predicting the next words. ['<p>', '<em>']\n",
            "For more information, we refer readers to  which this tutorial is based on. ['<p>']\n",
            "Figure 2. Neural machine translation – example of a deep recurrent architecture proposed by for translating a source sentence \"I am a student\" into a target sentence \"Je suis étudiant\". Here, \"&lt;s&gt;\" marks the start of the decoding process while \"&lt;/s&gt;\" tells the decoder to stop. ['<p align=\"center\">', '<br/>']\n",
            "To install this tutorial, you need to have TensorFlow installed on your system. This tutorial requires TensorFlow Nightly. To install TensorFlow, follow the . ['<p>']\n",
            "Let's first dive into the heart of building an NMT model with concrete code snippets through which we will explain Figure 2 in more detail. We defer data preparation and the full code to later. This part refers to file ['<p>']\n",
            "At the bottom layer, the encoder and decoder RNNs receive as input the following: first, the source sentence, then a boundary marker \"&lt;s&gt;\" which indicates the transition from the encoding to the decoding mode, and the target sentence.  For training, we will feed the system with the following tensors, which are in time-major format and contain word indices: ['<p>']\n",
            "encoder_inputs [max_encoder_time, batch_size]: source input words. ['<ul>', '<li>']\n",
            "decoder_inputs [max_decoder_time, batch_size]: target input words. ['<ul>', '<li>']\n",
            "decoder_outputs [max_decoder_time, batch_size]: target output words, these are decoder_inputs shifted to the left by one time step with an end-of-sentence tag appended on the right. ['<ul>', '<li>']\n",
            "Given the categorical nature of words, the model must first look up the source and target embeddings to retrieve the corresponding word representations. For this embedding layer to work, a vocabulary is first chosen for each language. Usually, a vocabulary size V is selected, and only the most frequent V words are treated as unique.  All other words are converted to an \"unknown\" token and all get the same embedding.  The embedding weights, one set per language, are usually learned during training. ['<p>']\n",
            "# Embedding ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "embedding_encoder = variable_scope.get_variable(     \"embedding_encoder\", [src_vocab_size, embedding_size], ...) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "# Look up embedding: ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "#   encoder_inputs: [max_time, batch_size] ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "#   encoder_emb_inp: [max_time, batch_size, embedding_size] ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "encoder_emb_inp = embedding_ops.embedding_lookup(     embedding_encoder, encoder_inputs) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "Similarly, we can build embedding_decoder and decoder_emb_inp. Note that one can choose to initialize embedding weights with pretrained word representations such as word2vec or Glove vectors. In general, given a large amount of training data we can learn these embeddings from scratch. ['<p>']\n",
            "Once retrieved, the word embeddings are then fed as input into the main network, which consists of two multi-layer RNNs – an encoder for the source language and a decoder for the target language. These two RNNs, in principle, can share the same weights; however, in practice, we often use two different RNN parameters (such models do a better job when fitting large training datasets). The ['<p>', '<em>']\n",
            "encoder RNN uses zero vectors as its starting states and is built as follows: ['<p>', '<em>']\n",
            "# Build RNN cell ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "# Run Dynamic RNN ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "#   encoder_outputs: [max_time, batch_size, num_units] ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "#   encoder_state: [batch_size, num_units] ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(     encoder_cell, encoder_emb_inp,     sequence_length=source_sequence_length, time_major=True) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "Note that sentences have different lengths to avoid wasting computation, we tell ['<p>', '<em>']\n",
            "dynamic_rnn the exact source sentence lengths through ['<p>', '<em>']\n",
            "source_sequence_length. Since our input is time major, we set ['<p>', '<em>']\n",
            "time_major=True. Here, we build only a single layer LSTM, encoder_cell. We will describe how to build multi-layer LSTMs, add dropout, and use attention in a later section. ['<p>', '<em>']\n",
            "The decoder also needs to have access to the source information, and one simple way to achieve that is to initialize it with the last hidden state of the encoder, encoder_state. In Figure 2, we pass the hidden state at the source word \"student\" to the decoder side. ['<p>']\n",
            "# Helper ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "helper = tf.contrib.seq2seq.TrainingHelper(     decoder_emb_inp, decoder_lengths, time_major=True) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "# Decoder ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "decoder = tf.contrib.seq2seq.BasicDecoder(     decoder_cell, helper, encoder_state,     output_layer=projection_layer) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "# Dynamic decoding ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "logits = outputs.rnn_output ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "Here, the core part of this code is the BasicDecoder object, decoder, which receives decoder_cell (similar to encoder_cell), a helper, and the previous ['<p>', '<em>']\n",
            "encoder_state as inputs. By separating out decoders and helpers, we can reuse different codebases, e.g., TrainingHelper can be substituted with ['<p>', '<em>']\n",
            "GreedyEmbeddingHelper to do greedy decoding. See more in ['<p>', '<em>']\n",
            "Lastly, we haven't mentioned projection_layer which is a dense matrix to turn the top hidden states to logit vectors of dimension V. We illustrate this process at the top of Figure 2. ['<p>']\n",
            "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(     labels=decoder_outputs, logits=logits) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "train_loss = (tf.reduce_sum(crossent * target_weights) /     batch_size) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "Here, target_weights is a zero-one matrix of the same size as ['<p>', '<em>']\n",
            "decoder_outputs. It masks padding positions outside of the target sequence lengths with values 0. ['<p>', '<em>']\n",
            "Important note: It's worth pointing out that we divide the loss by ['<p>', '<em>']\n",
            "batch_size, so our hyperparameters are \"invariant\" to batch_size. Some people divide the loss by (batch_size * num_time_steps), which plays down the errors made on short sentences. More subtly, our hyperparameters (applied to the former way) can't be used for the latter way. For example, if both approaches use SGD with a learning of 1.0, the latter approach effectively uses a much smaller learning rate of 1 / num_time_steps. ['<p>', '<em>']\n",
            "# Calculate and clip gradients ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "params = tf.trainable_variables() ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "gradients = tf.gradients(train_loss, params) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "clipped_gradients, _ = tf.clip_by_global_norm(     gradients, max_gradient_norm) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "One of the important steps in training RNNs is gradient clipping. Here, we clip by the global norm.  The max value, max_gradient_norm, is often set to a value like 5 or 1. The last step is selecting the optimizer.  The Adam optimizer is a common choice.  We also select a learning rate.  The value of learning_rate can is usually in the range 0.0001 to 0.001; and can be set to decrease as training progresses. ['<p>']\n",
            "# Optimization ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "optimizer = tf.train.AdamOptimizer(learning_rate) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "update_step = optimizer.apply_gradients(     zip(clipped_gradients, params)) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "In our own experiments, we use standard SGD (tf.train.GradientDescentOptimizer) with a decreasing learning rate schedule, which yields better performance. See the . ['<p>']\n",
            "Let's train our very first NMT model, translating from Vietnamese to English! The entry point of our code is ['<p>']\n",
            "We will use a small-scale parallel corpus of TED talks (133K training examples) for this exercise. All data we used here can be found at: ['<p>']\n",
            ". We will use tst2012 as our dev dataset, and tst2013 as our test dataset. ['<p>']\n",
            "mkdir /tmp/nmt_model python -m nmt.nmt \\     --src=vi --tgt=en \\     --vocab_prefix=/tmp/nmt_data/vocab  \\     --train_prefix=/tmp/nmt_data/train \\     --dev_prefix=/tmp/nmt_data/tst2012  \\     --test_prefix=/tmp/nmt_data/tst2013 \\     --out_dir=/tmp/nmt_model \\     --num_train_steps=12000 \\     --steps_per_stats=100 \\     --num_layers=2 \\     --num_units=128 \\     --dropout=0.2 \\     --metrics=bleu ['<div highlight highlight-source-shell\">']\n",
            "The above command trains a 2-layer LSTM seq2seq model with 128-dim hidden units and embeddings for 12 epochs. We use a dropout value of 0.2 (keep probability 0.8). If no error, we should see logs similar to the below with decreasing perplexity values as we train. ['<p>']\n",
            "# First evaluation, global step 0   eval dev: perplexity 17193.66   eval test: perplexity 17193.27 # Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017   sample train data:     src_reverse: &lt;/s&gt; &lt;/s&gt; Điều đó , dĩ nhiên , là câu chuyện trích ra từ học thuyết của Karl Marx .     ref: That , of course , was the &lt;unk&gt; distilled from the theories of Karl Marx . &lt;/s&gt; &lt;/s&gt; &lt;/s&gt;   epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00   epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00   epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00   epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00   epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00 ['<pre>']\n",
            "While you're training your NMT models (and once you have trained models), you can obtain translations given previously unseen source sentences. This process is called inference. There is a clear distinction between training and inference (testing): at inference time, we only have access to the source sentence, i.e., encoder_inputs. There are many ways to perform decoding.  Decoding methods include greedy, sampling, and beam-search decoding. Here, we will discuss the greedy decoding strategy. ['<p>']\n",
            "We still encode the source sentence in the same way as during training to obtain an encoder_state, and this encoder_state is used to initialize the decoder. ['<ol>', '<li>']\n",
            "The decoding (translation) process is started as soon as the decoder receives a starting symbol \"&lt;s&gt;\" (refer as tgt_sos_id in our code); ['<ol>', '<li>']\n",
            "For each timestep on the decoder side, we treat the RNN's output as a set of logits.  We choose the most likely word, the id associated with the maximum logit value, as the emitted word (this is the \"greedy\" behavior).  For example in Figure 3, the word \"moi\" has the highest translation probability in the first decoding step.  We then feed this word as input to the next timestep. ['<ol>', '<li>']\n",
            "The process continues until the end-of-sentence marker \"&lt;/s&gt;\" is produced as an output symbol (refer as tgt_eos_id in our code). ['<ol>', '<li>']\n",
            "Figure 3. Greedy decoding – example of how a trained NMT model produces a translation for a source sentence \"Je suis étudiant\" using greedy search. ['<p align=\"center\">', '<br/>']\n",
            "Step 3 is what makes inference different from training. Instead of always feeding the correct target words as an input, inference uses words predicted by the model. Here's the code to achieve greedy decoding.  It is very similar to the training decoder. ['<p>']\n",
            "helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(     embedding_decoder,     tf.fill([batch_size], tgt_sos_id), tgt_eos_id) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "outputs, _ = tf.contrib.seq2seq.dynamic_decode(     decoder, maximum_iterations=maximum_iterations) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "translations = outputs.sample_id ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "Here, we use GreedyEmbeddingHelper instead of TrainingHelper. Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths. One heuristic is to decode up to two times the source sentence lengths. ['<p>']\n",
            "cat &gt; /tmp/my_infer_file.vi ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "# (copy and paste some sentences from /tmp/nmt_data/tst2013.vi) ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "python -m nmt.nmt \\     --out_dir=/tmp/nmt_model \\     --inference_input_file=/tmp/my_infer_file.vi \\     --inference_output_file=/tmp/nmt_model/output_infer ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "cat /tmp/nmt_model/output_infer # To view the inference as output ['<div highlight highlight-source-shell\">', '<span pl-c\">']\n",
            "Note the above commands can also be run while the model is still being trained as long as there exists a training checkpoint. See  for more details. ['<p>']\n",
            "Having gone through the most basic seq2seq model, let's get more advanced! To build state-of-the-art neural machine translation systems, we will need more \"secret sauce\": the attention mechanism, which was first introduced by , then later refined by  and others. The key idea of the attention mechanism is to establish direct short-cut connections between the target and the source by paying \"attention\" to relevant source content as we translate. A nice byproduct of the attention mechanism is an easy-to-visualize alignment matrix between the source and target sentences (as shown in Figure 4). ['<p>']\n",
            "Figure 4. Attention visualization – example of the alignments between source and target sentences. Image is taken from (Bahdanau et al., 2015). ['<p align=\"center\">', '<br/>']\n",
            "Remember that in the vanilla seq2seq model, we pass the last source state from the encoder to the decoder when starting the decoding process. This works well for short and medium-length sentences; however, for long sentences, the single fixed-size hidden state becomes an information bottleneck. Instead of discarding all of the hidden states computed in the source RNN, the attention mechanism provides an approach that allows the decoder to peek at them (treating them as a dynamic memory of the source information). By doing so, the attention mechanism improves the translation of longer sentences. Nowadays, attention mechanisms are the defacto standard and have been successfully applied to many other tasks (including image caption generation, speech recognition, and text summarization). ['<p>']\n",
            "We now describe an instance of the attention mechanism proposed in (Luong et al., 2015), which has been used in several state-of-the-art systems including open-source toolkits such as  and in the TF seq2seq API in this tutorial. We will also provide connections to other variants of the attention mechanism. ['<p>']\n",
            "Figure 5. Attention mechanism – example of an attention-based NMT system as described in (Luong et al., 2015) . We highlight in detail the first step of the attention computation. For clarity, we don't show the embedding and projection layers in Figure (2). ['<p align=\"center\">', '<br/>']\n",
            "The current target hidden state is compared with all source states to derive ['<ol>', '<li>', '<em>']\n",
            "attention weights (can be visualized as in Figure 4). ['<ol>', '<li>', '<em>']\n",
            "Based on the attention weights we compute a context vector as the weighted average of the source states. ['<ol>', '<li>', '<em>']\n",
            "Combine the context vector with the current target hidden state to yield the final attention vector ['<ol>', '<li>', '<em>']\n",
            "The attention vector is fed as an input to the next time step (input feeding).  The first three steps can be summarized by the equations below: ['<ol>', '<li>', '<em>']\n",
            "Here, the function score is used to compared the target hidden state $$h_t$$ with each of the source hidden states $$\\overline{h}_s$$, and the result is normalized to produced attention weights (a distribution over source positions). There are various choices of the scoring function; popular scoring functions include the multiplicative and additive forms given in Eq. (4). Once computed, the attention vector $$a_t$$ is used to derive the softmax logit and loss.  This is similar to the target hidden state at the top layer of a vanilla seq2seq model. The function ['<p>', '<code>']\n",
            "f can also take other forms. ['<p>', '<code>']\n",
            "Various implementations of attention mechanisms can be found in ['<p>']\n",
            "As hinted in the above equations, there are many different attention variants. These variants depend on the form of the scoring function and the attention function, and on whether the previous state $$h_{t-1}$$ is used instead of $$h_t$$ in the scoring function as originally suggested in (Bahdanau et al., 2015). Empirically, we found that only certain choices matter. First, the basic form of attention, i.e., direct connections between target and source, needs to be present. Second, it's important to feed the attention vector to the next timestep to inform the network about past attention decisions as demonstrated in (Luong et al., 2015). Lastly, choices of the scoring function can often result in different performance. See more in the  section. ['<p>']\n",
            "In our implementation of the ['<p>', '<em>']\n",
            ", we borrow some terminology from  in their work on ['<p>', '<em>']\n",
            "memory networks. Instead of having readable &amp; writable memory, the attention mechanism presented in this tutorial is a read-only memory. Specifically, the set of source hidden states (or their transformed versions, e.g., $$W\\overline{h}_s$$ in Luong's scoring style or $$W_2\\overline{h}_s$$ in Bahdanau's scoring style) is referred to as the \"memory\". At each time step, we use the current target hidden state as a \"query\" to decide on which parts of the memory to read.  Usually, the query needs to be compared with keys corresponding to individual memory slots. In the above presentation of the attention mechanism, we happen to use the set of source hidden states (or their transformed versions, e.g., $$W_1h_t$$ in Bahdanau's scoring style) as \"keys\". One can be inspired by this memory-network terminology to derive other forms of attention! ['<p>', '<em>']\n",
            "Thanks to the attention wrapper, extending our vanilla seq2seq code with attention is trivial. This part refers to file ['<p>']\n",
            "# attention_states: [batch_size, max_time, num_units] ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "attention_states = tf.transpose(encoder_outputs, [1, 0, 2]) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "# Create an attention mechanism ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "attention_mechanism = tf.contrib.seq2seq.LuongAttention(     num_units, attention_states,     memory_sequence_length=source_sequence_length) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "In the previous  section, encoder_outputs is the set of all source hidden states at the top layer and has the shape of [max_time, batch_size, num_units] (since we use dynamic_rnn with time_major set to ['<p>', '<em>']\n",
            "True for efficiency). For the attention mechanism, we need to make sure the \"memory\" passed in is batch major, so we need to transpose ['<p>', '<em>']\n",
            "attention_states. We pass source_sequence_length to the attention mechanism to ensure that the attention weights are properly normalized (over non-padding positions only). ['<p>', '<em>']\n",
            "decoder_cell = tf.contrib.seq2seq.AttentionWrapper(     decoder_cell, attention_mechanism,     attention_layer_size=num_units) ['<div highlight highlight-source-python\">']\n",
            "To enable attention, we need to use one of luong, scaled_luong, bahdanau or normed_bahdanau as the value of the attention flag during training. The flag specifies which attention mechanism we are going to use. In addition, we need to create a new directory for the attention model, so we don't reuse the previously trained basic NMT model. ['<p>']\n",
            "mkdir /tmp/nmt_attention_model ['<div highlight highlight-source-shell\">']\n",
            "python -m nmt.nmt \\     --attention=scaled_luong \\     --src=vi --tgt=en \\     --vocab_prefix=/tmp/nmt_data/vocab  \\     --train_prefix=/tmp/nmt_data/train \\     --dev_prefix=/tmp/nmt_data/tst2012  \\     --test_prefix=/tmp/nmt_data/tst2013 \\     --out_dir=/tmp/nmt_attention_model \\     --num_train_steps=12000 \\     --steps_per_stats=100 \\     --num_layers=2 \\     --num_units=128 \\     --dropout=0.2 \\     --metrics=bleu ['<div highlight highlight-source-shell\">']\n",
            "python -m nmt.nmt \\     --out_dir=/tmp/nmt_attention_model \\     --inference_input_file=/tmp/my_infer_file.vi \\     --inference_output_file=/tmp/nmt_attention_model/output_infer ['<div highlight highlight-source-shell\">']\n",
            "The Training graph, which: ['<ul>', '<li>', '<p>']\n",
            "Batches, buckets, and possibly subsamples input data from a set of files/external inputs. ['<ul>', '<li>', '<p>']\n",
            "Includes the forward and backprop ops. ['<ul>', '<li>', '<p>']\n",
            "Constructs the optimizer, and adds the training op. ['<ul>', '<li>', '<p>']\n",
            "The Eval graph, which: ['<ul>', '<li>', '<p>']\n",
            "Batches and buckets input data from a set of files/external inputs. ['<ul>', '<li>', '<p>']\n",
            "Includes the training forward ops, and additional evaluation ops that aren't used for training. ['<ul>', '<li>', '<p>']\n",
            "The Inference graph, which: ['<ul>', '<li>', '<p>']\n",
            "May not batch input data. ['<ul>', '<li>', '<p>']\n",
            "Does not subsample or bucket input data. ['<ul>', '<li>', '<p>']\n",
            "Reads input data from placeholders (data can be fed directly to the graph via feed_dict or from a C++ TensorFlow serving binary). ['<ul>', '<li>', '<p>']\n",
            "Includes a subset of the model forward ops, and possibly additional special inputs/outputs for storing state between session.run calls. ['<ul>', '<li>', '<p>']\n",
            "The inference graph is usually very different from the other two, so it makes sense to build it separately. ['<ul>', '<li>']\n",
            "The eval graph becomes simpler since it no longer has all the additional backprop ops. ['<ul>', '<li>']\n",
            "Data feeding can be implemented separately for each graph. ['<ul>', '<li>']\n",
            "Variable reuse is much simpler.  For example, in the eval graph there's no need to reopen variable scopes with reuse=True just because the Training model created these variables already.  So the same code can be reused without sprinkling reuse= arguments everywhere. ['<ul>', '<li>']\n",
            "In distributed training, it is commonplace to have separate workers perform training, eval, and inference.  These need to build their own graphs anyway. So building the system this way prepares you for distributed training. ['<ul>', '<li>']\n",
            "The primary source of complexity becomes how to share Variables across the three graphs in a single machine setting. This is solved by using a separate session for each graph. The training session periodically saves checkpoints, and the eval session and the infer session restore parameters from checkpoints. The example below shows the main differences between the two approaches. ['<p>']\n",
            "with tf.variable_scope('root'):   train_inputs = tf.placeholder()   train_op, loss = BuildTrainModel(train_inputs)   initializer = tf.global_variables_initializer() ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "with tf.variable_scope('root', reuse=True):   eval_inputs = tf.placeholder()   eval_loss = BuildEvalModel(eval_inputs) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "with tf.variable_scope('root', reuse=True):   infer_inputs = tf.placeholder()   inference_output = BuildInferenceModel(infer_inputs) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "sess = tf.Session() ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "sess.run(initializer) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "for i in itertools.count():   train_input_data = ...   sess.run([loss, train_op], feed_dict={train_inputs: train_input_data}) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "if i % EVAL_STEPS == 0:     while data_to_eval:       eval_input_data = ...       sess.run([eval_loss], feed_dict={eval_inputs: eval_input_data}) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "if i % INFER_STEPS == 0:     sess.run(inference_output, feed_dict={infer_inputs: infer_input_data}) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "train_graph = tf.Graph() ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "eval_graph = tf.Graph() ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "infer_graph = tf.Graph() ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "with train_graph.as_default():   train_iterator = ...   train_model = BuildTrainModel(train_iterator)   initializer = tf.global_variables_initializer() ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "with eval_graph.as_default():   eval_iterator = ...   eval_model = BuildEvalModel(eval_iterator) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "with infer_graph.as_default():   infer_iterator, infer_inputs = ...   infer_model = BuildInferenceModel(infer_iterator) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "checkpoints_path = \"/tmp/model/checkpoints\" ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "train_sess = tf.Session(graph=train_graph) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "eval_sess = tf.Session(graph=eval_graph) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "infer_sess = tf.Session(graph=infer_graph) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "train_sess.run(initializer) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "train_sess.run(train_iterator.initializer) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "for i in itertools.count(): ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "train_model.train(train_sess) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "if i % EVAL_STEPS == 0:     checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)     eval_model.saver.restore(eval_sess, checkpoint_path)     eval_sess.run(eval_iterator.initializer)     while data_to_eval:       eval_model.eval(eval_sess) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "if i % INFER_STEPS == 0:     checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)     infer_model.saver.restore(infer_sess, checkpoint_path)     infer_sess.run(infer_iterator.initializer, feed_dict={infer_inputs: infer_input_data})     while data_to_infer:       infer_model.infer(infer_sess) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "One other difference in the new approach is that instead of using feed_dicts to feed data at each session.run call (and thereby performing our own batching, bucketing, and manipulating of data), we use stateful iterator objects.  These iterators make the input pipeline much easier in both the single-machine and distributed setting. We will cover the new input data pipeline (as introduced in TensorFlow 1.2) in the next section. ['<p>']\n",
            "Feed data directly via feed_dict at each training session.run call. ['<ol>', '<li>', '<em>']\n",
            "Use the queueing mechanisms in tf.train (e.g. tf.train.batch) and ['<ol>', '<li>', '<em>']\n",
            "tf.contrib.train. ['<ol>', '<li>', '<em>']\n",
            "Use helpers from a higher level framework like tf.contrib.learn or ['<ol>', '<li>', '<em>']\n",
            "tf.contrib.slim (which effectively use #2). ['<ol>', '<li>', '<em>']\n",
            "The first approach is easier for users who aren't familiar with TensorFlow or need to do exotic input modification (i.e., their own minibatch queueing) that can only be done in Python.  The second and third approaches are more standard but a little less flexible; they also require starting multiple python threads (queue runners).  Furthermore, if used incorrectly queues can lead to deadlocks or opaque error messages.  Nevertheless, queues are significantly more efficient than using feed_dict and are the standard for both single-machine and distributed training. ['<p>']\n",
            "Starting in TensorFlow 1.2, there is a new system available for reading data into TensorFlow models: dataset iterators, as found in the tf.data module. Data iterators are flexible, easy to reason about and to manipulate, and provide efficiency and multithreading by leveraging the TensorFlow C++ runtime. ['<p>']\n",
            "# Training dataset consists of multiple files. ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "train_dataset = tf.data.TextLineDataset(train_files) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "# Evaluation dataset uses a single file, but we may ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "# point to a different file for each evaluation round. ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "eval_file = tf.placeholder(tf.string, shape=()) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "eval_dataset = tf.data.TextLineDataset(eval_file) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "# For inference, feed input data to the dataset directly via feed_dict. ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "infer_batch = tf.placeholder(tf.string, shape=(num_infer_examples,)) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "infer_dataset = tf.data.Dataset.from_tensor_slices(infer_batch) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "All datasets can be treated similarly via input processing.  This includes reading and cleaning the data, bucketing (in the case of training and eval), filtering, and batching. ['<p>']\n",
            "Finally, we can perform a vocabulary lookup on each sentence.  Given a lookup table object table, this map converts the first tuple elements from a vector of strings to a vector of integers. ['<p>']\n",
            "Joining two datasets is also easy.  If two files contain line-by-line translations of each other and each one is read into its own dataset, then a new dataset containing the tuples of the zipped lines can be created via: ['<p>']\n",
            "Batching of variable-length sentences is straightforward. The following transformation batches batch_size elements from source_target_dataset, and respectively pads the source and target vectors to the length of the longest source and target vector in each batch. ['<p>']\n",
            "batched_dataset = source_target_dataset.padded_batch(         batch_size,         padded_shapes=((tf.TensorShape([None]),  # source vectors of unknown size                         tf.TensorShape([])),     # size(source)                        (tf.TensorShape([None]),  # target vectors of unknown size                         tf.TensorShape([]))),    # size(target)         padding_values=((src_eos_id,  # source vectors padded on the right with src_eos_id                          0),          # size(source) -- unused                         (tgt_eos_id,  # target vectors padded on the right with tgt_eos_id                          0)))         # size(target) -- unused ['<div highlight highlight-source-python\">']\n",
            "iterator[0][0] has the batched and padded source sentence matrices. ['<ul>', '<li>']\n",
            "iterator[0][1] has the batched source size vectors. ['<ul>', '<li>']\n",
            "iterator[1][0] has the batched and padded target sentence matrices. ['<ul>', '<li>']\n",
            "iterator[1][1] has the batched target size vectors. ['<ul>', '<li>']\n",
            "Finally, bucketing that batches similarly-sized source sentences together is also possible.  Please see the file ['<p>']\n",
            "for more details and the full implementation. ['<p>']\n",
            "batched_iterator = batched_dataset.make_initializable_iterator() ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "((source, source_lengths), (target, target_lengths)) = batched_iterator.get_next() ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "# At initialization time. ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "session.run(batched_iterator.initializer, feed_dict={...}) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "Bidirectionality on the encoder side generally gives better performance (with some degradation in speed as more layers are used). Here, we give a simplified example of how to build an encoder with a single bidirectional layer: ['<p>']\n",
            "# Construct forward and backward cells ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(     forward_cell, backward_cell, encoder_emb_inp,     sequence_length=source_sequence_length, time_major=True) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "encoder_outputs = tf.concat(bi_outputs, -1) ['<div highlight highlight-source-python\">', '<span pl-s1\">']\n",
            "The variables encoder_outputs and encoder_state can be used in the same way as in Section Encoder. Note that, for multiple bidirectional layers, we need to manipulate the encoder_state a bit, see , method ['<p>', '<em>']\n",
            "_build_bidirectional_rnn() for more details. ['<p>', '<em>']\n",
            "While greedy decoding can give us quite reasonable translation quality, a beam search decoder can further boost performance. The idea of beam search is to better explore the search space of all possible translations by keeping around a small set of top candidates as we translate. The size of the beam is called ['<p>', '<em>']\n",
            "beam width; a minimal beam width of, say size 10, is generally sufficient. For more information, we refer readers to Section 7.2.3 of . Here's an example of how beam search can be done: ['<p>', '<em>']\n",
            "# Replicate encoder infos beam_width times ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "decoder_initial_state = tf.contrib.seq2seq.tile_batch(     encoder_state, multiplier=hparams.beam_width) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "# Define a beam-search decoder ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "decoder = tf.contrib.seq2seq.BeamSearchDecoder(         cell=decoder_cell,         embedding=embedding_decoder,         start_tokens=start_tokens,         end_token=end_token,         initial_state=decoder_initial_state,         beam_width=beam_width,         output_layer=projection_layer,         length_penalty_weight=0.0,         coverage_penalty_weight=0.0) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">']\n",
            "Note that the same dynamic_decode() API call is used, similar to the Section . Once decoded, we can access the translations as follows: ['<p>']\n",
            "translations = outputs.predicted_ids ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# Make sure translations shape is [batch_size, beam_width, time] ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-k\">']\n",
            "if self.time_major:    translations = tf.transpose(translations, perm=[1, 2, 0]) ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-k\">']\n",
            "There are several hyperparameters that can lead to additional performances. Here, we list some based on our own experience [ Disclaimers: others might not agree on things we wrote! ]. ['<p>']\n",
            "Optimizer: while Adam can lead to reasonable results for \"unfamiliar\" architectures, SGD with scheduling will generally lead to better performance if you can train with SGD. ['<p>']\n",
            "Attention: Bahdanau-style attention often requires bidirectionality on the encoder side to work well; whereas Luong-style attention tends to work well for different settings. For this tutorial code, we recommend using the two improved variants of Luong &amp; Bahdanau-style attentions: scaled_luong &amp; normed bahdanau. ['<p>']\n",
            "Training a NMT model may take several days. Placing different RNN layers on different GPUs can improve the training speed. Here’s an example to create RNN layers on multiple GPUs. ['<p>']\n",
            "cells = [] ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "for i in range(num_layers):   cells.append(tf.contrib.rnn.DeviceWrapper(       tf.contrib.rnn.LSTMCell(num_units),       \"/gpu:%d\" % (num_layers % num_gpus))) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "cell = tf.contrib.rnn.MultiRNNCell(cells) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "You may notice the speed improvement of the attention based NMT model is very small as the number of GPUs increases. One major drawback of the standard attention architecture is using the top (final) layer’s output to query attention at each time step. That means each decoding step must wait its previous step completely finished; hence, we can’t parallelize the decoding process by simply placing RNN layers on multiple GPUs. ['<p>']\n",
            "The  parallelizes the decoder's computation by using the bottom (first) layer’s output to query attention. Therefore, each decoding step can start as soon as its previous step's first layer and attention computation finished. We implemented the architecture in ['<p>', '<em>']\n",
            ", a subclass of tf.contrib.rnn.MultiRNNCell. Here’s an example of how to create a decoder cell with the GNMTAttentionMultiCell. ['<p>', '<em>']\n",
            "attention_cell = cells.pop(0) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "attention_cell = tf.contrib.seq2seq.AttentionWrapper(     attention_cell,     attention_mechanism,     attention_layer_size=None,  # don't add an additional dense layer.     output_attention=False,) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "cell = GNMTAttentionMultiCell(attention_cell, cells) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "Train: 133K examples, vocab=vocab.(vi|en), train=train.(vi|en) dev=tst2012.(vi|en), test=tst2013.(vi|en), . ['<p>']\n",
            "Training details. We train 2-layer LSTMs of 512 units with bidirectional encoder (i.e., 1 bidirectional layers for the encoder), embedding dim is 512. LuongAttention (scale=True) is used together with dropout keep_prob of 0.8. All parameters are uniformly. We use SGD with learning rate 1.0 as follows: train for 12K steps (~ 12 epochs); after 8K steps, we start halving learning rate every 1K step. ['<p>']\n",
            "Below are the averaged results of 2 models (, ['<p>', '<br/>']\n",
            "). We measure the translation quality in terms of BLEU scores . ['<p>', '<br/>']\n",
            "Systems ['<p>', '<em>', '<strong>']\n",
            "tst2012 (dev) ['<br/>', '<strong>']\n",
            "test2013 (test) ['<br/>', '<strong>']\n",
            "NMT (greedy) ['<p>', '<strong>']\n",
            "23.2 ['<br/>', '<strong>']\n",
            "25.5 ['<br/>', '<strong>']\n",
            "NMT (beam=10) ['<p>', '<strong>']\n",
            "23.8 ['<br/>', '<strong>']\n",
            "26.1 ['<strong>']\n",
            "23.3 ['<br/>', '<strong>']\n",
            "Train: 4.5M examples, vocab=vocab.bpe.32000.(de|en), train=train.tok.clean.bpe.32000.(de|en), dev=newstest2013.tok.bpe.32000.(de|en), test=newstest2015.tok.bpe.32000.(de|en), ['<p>']\n",
            "Training details. Our training hyperparameters are similar to the English-Vietnamese experiments except for the following details. The data is split into subword units using  (32K operations). We train 4-layer LSTMs of 1024 units with bidirectional encoder (i.e., 2 bidirectional layers for the encoder), embedding dim is 1024. We train for 350K steps (~ 10 epochs); after 170K steps, we start halving learning rate every 17K step. ['<p>']\n",
            "The first 2 rows are the averaged results of 2 models (, ['<p>']\n",
            "). Results in the third row is with GNMT attention () ; trained with 4 GPUs. ['<p>']\n",
            "newstest2013 (dev) ['<p>', '<strong>']\n",
            "newstest2015 ['<p>', '<em>', '<strong>']\n",
            "27.1 ['<p>', '<strong>']\n",
            "27.6 ['<p>', '<em>', '<strong>']\n",
            "28.0 ['<p>', '<strong>']\n",
            "28.9 ['<p>', '<strong>']\n",
            "NMT + GNMT attention (beam=10) ['<p>', '<strong>']\n",
            "29.0 ['<p>', '<strong>']\n",
            "29.9 ['<p>', '<strong>']\n",
            "29.3 ['<p>', '<strong>']\n",
            "1 gpu ['<strong>']\n",
            "4 gpus ['<strong>']\n",
            "8 gpus ['<strong>']\n",
            "NMT (4 layers) ['<strong>']\n",
            "2.2s, 3.4K ['<strong>']\n",
            "1.9s, 3.9K ['<strong>']\n",
            "NMT (8 layers) ['<strong>']\n",
            "3.5s, 2.0K ['<strong>']\n",
            "2.9s, 2.4K ['<strong>']\n",
            "NMT + GNMT attention (4 layers) ['<strong>']\n",
            "2.6s, 2.8K ['<strong>']\n",
            "1.7s, 4.3K ['<strong>']\n",
            "NMT + GNMT attention (8 layers) ['<strong>']\n",
            "4.2s, 1.7K ['<strong>']\n",
            "1.9s, 3.8K ['<strong>']\n",
            "The first 2 rows are our models with GNMT attention: ['<p>']\n",
            "newstest2014 ['<p>', '<em>', '<strong>']\n",
            "Ours — NMT + GNMT attention (4 layers) ['<p>', '<em>', '<strong>']\n",
            "23.7 ['<p>', '<em>', '<strong>']\n",
            "26.5 ['<p>', '<em>', '<strong>']\n",
            "Ours — NMT + GNMT attention (8 layers) ['<p>', '<em>', '<strong>']\n",
            "24.4 ['<p>', '<em>', '<strong>']\n",
            "20.6 ['<p>', '<em>', '<strong>']\n",
            "24.9 ['<p>', '<em>', '<strong>']\n",
            "OpenNMT ['<p>', '<em>', '<strong>']\n",
            "19.3 ['<p>', '<em>', '<strong>']\n",
            "tf-seq2seq ['<p>', '<em>', '<strong>']\n",
            "22.2 ['<p>', '<em>', '<strong>']\n",
            "25.2 ['<p>', '<em>', '<strong>']\n",
            "GNMT ['<p>', '<em>', '<strong>']\n",
            "24.6 ['<p>', '<em>', '<strong>']\n",
            "We have provided ['<p>']\n",
            "for using pre-trained checkpoint for inference or training NMT architectures used in the Benchmark. ['<p>']\n",
            "python -m nmt.nmt \\     --src=de --tgt=en \\     --ckpt=/path/to/checkpoint/translate.ckpt \\     --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \\     --out_dir=/tmp/deen_gnmt \\     --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \\     --inference_input_file=/tmp/wmt16/newstest2014.tok.bpe.32000.de \\     --inference_output_file=/tmp/deen_gnmt/output_infer \\     --inference_ref_file=/tmp/wmt16/newstest2014.tok.bpe.32000.en ['<pre>']\n",
            "python -m nmt.nmt \\     --src=de --tgt=en \\     --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \\     --out_dir=/tmp/deen_gnmt \\     --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \\     --train_prefix=/tmp/wmt16/train.tok.clean.bpe.32000 \\     --dev_prefix=/tmp/wmt16/newstest2013.tok.bpe.32000 \\     --test_prefix=/tmp/wmt16/newstest2015.tok.bpe.32000 ['<pre>']\n",
            "For deeper reading on Neural Machine Translation and sequence-to-sequence models, we highly recommend the following materials by ['<p>']\n",
            "; and . ['<p>']\n",
            "There's a wide variety of tools for building seq2seq models, so we pick one per language: Stanford NMT ['<p>', '<em>']\n",
            "[Matlab]  tf-seq2seq ['<p>', '<em>']\n",
            "[TensorFlow]  Nemantus ['<p>', '<em>']\n",
            "[Theano]  OpenNMT  [Torch] OpenNMT-py  [PyTorch] ['<p>', '<em>']\n",
            "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.. ICLR. ['<ul>', '<li>']\n",
            "Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015.. EMNLP. ['<ul>', '<li>']\n",
            "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.. NIPS. ['<ul>', '<li>']\n",
            "@article{luong17,   author  = {Minh{-}Thang Luong and Eugene Brevdo and Rui Zhao},   title   = {Neural Machine Translation (seq2seq) Tutorial},   journal = {https://github.com/tensorflow/nmt},   year    = {2017}, } ['<pre>']\n",
            "Go to ./libs/datasets/pycocotools and run make ['<ol>', '<li>']\n",
            "Download  dataset, place it into ./data, then run python download_and_convert_data.py to build tf-records. It takes a while. ['<ol>', '<li>']\n",
            "Download pretrained resnet50 model, wget http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz, unzip it, place it into ./data/pretrained_models/ ['<ol>', '<li>']\n",
            "Go to ./libs and run make ['<ol>', '<li>']\n",
            "run python train/train.py for training ['<ol>', '<li>']\n",
            "There are certainly some bugs, please report them back, and let's solve them together. ['<ol>', '<li>']\n",
            "ROIAlign ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "COCO Data Provider ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Resnet50 ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Feature Pyramid Network ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Anchor and ROI layer ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Mask layer ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Speedup anchor layer with cython ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Combining all modules together. ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Testing and debugging (in progress) ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Training / evaluation on COCO ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Add image summary to show some results ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Converting ResneXt ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Training &gt;2 images ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Anything helps this repo, including discussion, testing, promotion and of course your awesome code. ['<ul>', '<li>']\n",
            "In models folder create a class named VGG that inherit the \"base_model\" class ['<ul>', '<li>']\n",
            "class VGGModel(BaseModel):         def __init__(self, config):             super(VGGModel, self).__init__(config)             #call the build_model and init_saver functions.             self.build_model()              self.init_saver() ['<div highlight highlight-source-python\">']\n",
            "Override these two functions \"build_model\" where you implement the vgg model, and \"init_saver\" where you define a tensorflow saver, then call them in the initalizer. ['<ul>', '<li>']\n",
            "def build_model(self):         # here you build the tensorflow graph of any model you want and also define the loss.         pass                   def init_saver(self):         # here you initalize the tensorflow saver that will be used in saving the checkpoints.         self.saver = tf.train.Saver(max_to_keep=self.config.max_to_keep) ['<div highlight highlight-source-python\">']\n",
            "In trainers folder create a VGG trainer that inherit from \"base_train\" class ['<ul>', '<li>']\n",
            "class VGGTrainer(BaseTrain):         def __init__(self, sess, model, data, config, logger):             super(VGGTrainer, self).__init__(sess, model, data, config, logger) ['<div highlight highlight-source-python\">']\n",
            "Override these two functions \"train_step\", \"train_epoch\" where you write the logic of the training process ['<ul>', '<li>']\n",
            "def train_epoch(self):         \"\"\" ['<div highlight highlight-source-python\">', '<span pl-s\">', '<span pl-k\">']\n",
            "implement the logic of epoch: ['<div highlight highlight-source-python\">', '<span pl-s\">', '<span pl-k\">']\n",
            "-loop on the number of iterations in the config and call the train step ['<div highlight highlight-source-python\">', '<span pl-s\">', '<span pl-k\">']\n",
            "-add any summaries you want using the summary ['<div highlight highlight-source-python\">', '<span pl-s\">', '<span pl-k\">']\n",
            "\"\"\"         pass ['<div highlight highlight-source-python\">', '<span pl-s\">', '<span pl-k\">']\n",
            "def train_step(self):         \"\"\" ['<div highlight highlight-source-python\">', '<span pl-s\">', '<span pl-k\">']\n",
            "implement the logic of the train step ['<div highlight highlight-source-python\">', '<span pl-s\">', '<span pl-k\">']\n",
            "- run the tensorflow session ['<div highlight highlight-source-python\">', '<span pl-s\">', '<span pl-k\">']\n",
            "- return any metrics you need to summarize ['<div highlight highlight-source-python\">', '<span pl-s\">', '<span pl-k\">']\n",
            "In main file, you create the session and instances of the following objects \"Model\", \"Logger\", \"Data_Generator\", \"Trainer\", and config ['<ul>', '<li>']\n",
            "sess = tf.Session()     # create instance of the model you want     model = VGGModel(config)     # create your data generator     data = DataGenerator(config)     # create tensorboard logger     logger = Logger(sess, config) ['<div highlight highlight-source-python\">']\n",
            "Pass the all these objects to the trainer object, and start your training by calling \"trainer.train()\" ['<ul>', '<li>']\n",
            "trainer = VGGTrainer(sess, model, data, config, logger) ['<div highlight highlight-source-python\">', '<span pl-c\">']\n",
            "# here you train your model     trainer.train() ['<div highlight highlight-source-python\">', '<span pl-c\">']\n",
            "├──  base │   ├── base_model.py   - this file contains the abstract class of the model. │   └── base_train.py   - this file contains the abstract class of the trainer. │ │ ├── model               - this folder contains any model of your project. │   └── example_model.py │ │ ├── trainer             - this folder contains trainers of your project. │   └── example_trainer.py │    ├──  mains              - here's the main(s) of your project (you may need more than one main). │    └── example_main.py  - here's an example of main that is responsible for the whole pipeline. ['<pre>']\n",
            "│   ├──  data _loader   │    └── data_generator.py  - here's the data_generator that is responsible for all data handling. │  └── utils      ├── logger.py      └── any_other_utils_you_need ['<pre>']\n",
            "Base model ['<ul>', '<li>', '<h4>', '<p>']\n",
            "Base model is an abstract class that must be Inherited by any model you create, the idea behind this is that there's much shared stuff between all models. The base model contains: ['<ul>', '<li>', '<h4>', '<p>']\n",
            "Save -This function to save a checkpoint to the desk. ['<ul>', '<li>', '<h4>', '<p>']\n",
            "Load -This function to load a checkpoint from the desk. ['<ul>', '<li>', '<h4>', '<p>']\n",
            "Cur_epoch, Global_step counters -These variables to keep track of the current epoch and global step. ['<ul>', '<li>', '<h4>', '<p>']\n",
            "Init_Saver An abstract function to initialize the saver used for saving and loading the checkpoint, Note: override this function in the model you want to implement. ['<ul>', '<li>', '<h4>', '<p>']\n",
            "Build_model Here's an abstract function to define the model, Note: override this function in the model you want to implement. ['<ul>', '<li>', '<h4>', '<p>']\n",
            "Your model ['<ul>', '<li>', '<h4>', '<p>']\n",
            "Here's where you implement your model. So you should : ['<ul>', '<li>', '<h4>', '<p>']\n",
            "Create your model class and inherit the base_model class ['<ul>', '<li>', '<h4>', '<p>']\n",
            "override \"build_model\" where you write the tensorflow model you want ['<ul>', '<li>', '<h4>', '<p>']\n",
            "override \"init_save\" where you create a tensorflow saver to use it to save and load checkpoint ['<ul>', '<li>', '<h4>', '<p>']\n",
            "call the \"build_model\" and \"init_saver\" in the initializer. ['<ul>', '<li>', '<h4>', '<p>']\n",
            "Base trainer ['<ul>', '<li>', '<h4>', '<p>', '<ol>']\n",
            "Base trainer is an abstract class that just wrap the training process. ['<ul>', '<li>', '<h4>', '<p>', '<ol>']\n",
            "Your trainer ['<ul>', '<li>', '<h4>', '<p>', '<ol>']\n",
            "Here's what you should implement in your trainer. ['<ul>', '<li>', '<h4>', '<p>', '<ol>']\n",
            "Create your trainer class and inherit the base_trainer class. ['<ul>', '<li>', '<h4>', '<p>', '<ol>']\n",
            "override these two functions \"train_step\", \"train_epoch\" where you implement the training process of each step and each epoch. ['<ul>', '<li>', '<h4>', '<p>', '<ol>']\n",
            "Parse the config file. ['<ol>', '<li>']\n",
            "Create a tensorflow session. ['<ol>', '<li>']\n",
            "Create an instance of \"Model\", \"Data_Generator\" and \"Logger\" and parse the config to all of them. ['<ol>', '<li>']\n",
            "Create an instance of \"Trainer\" and pass all previous objects to it. ['<ol>', '<li>']\n",
            "Now you can train your model by calling \"Trainer.train()\" ['<ol>', '<li>']\n",
            "Replace the data loader part with new tensorflow dataset API. ['<ul>', '<li>']\n",
            "Thanks for my colleague   for contributing in this work. and thanks for  for the review. ['<p>', '<strong>']\n",
            "Thanks for Jtoy for including the repo in  . ['<p>', '<strong>']\n",
            "title ['<p>', '<div>']\n",
            "categories ['<p>', '<div>']\n",
            "tags ['<p>', '<div>']\n",
            "top ['<p>', '<div>']\n",
            "abbrlink ['<p>', '<div>']\n",
            "tensorflow2官方教程目录导航 ['<p>', '<div>']\n",
            "tensorflow2官方教程 ['<p>', '<div>']\n",
            "tensorflow2.0教程 ['<p>', '<div>']\n",
            "1900 ['<p>', '<div>']\n",
            "tensorflow/tensorflow2-zh-readme ['<p>', '<div>']\n",
            "最全TensorFlow2.0学习路线 ['<blockquote>', '<p>']\n",
            "最新版本： 英文版本： 翻译建议： ['<blockquote>', '<p>']\n",
            "最新版本： 英文版本： 翻译建议PR： ['<p>']\n",
            "最新版本： 英文版本： 翻译建议PR：[ ['<blockquote>', '<p>']\n",
            "Example name ['<p>']\n",
            "Demo link ['<p>']\n",
            "Input data type ['<p>']\n",
            "Task type ['<p>']\n",
            "Model type ['<p>']\n",
            "Training ['<p>']\n",
            "Inference ['<p>']\n",
            "API type ['<p>']\n",
            "Save-load operations ['<p>']\n",
            "Numeric ['<p>']\n",
            "Loading data from local file and training in Node.js ['<p>']\n",
            "Multilayer perceptron ['<p>']\n",
            "Node.js ['<p>']\n",
            "Layers ['<p>']\n",
            "Saving to filesystem and loading in Node.js ['<p>']\n",
            "Sequence-to-sequence ['<p>']\n",
            "RNN: SimpleRNN, GRU and LSTM ['<p>']\n",
            "Browser ['<p>']\n",
            "Browser: Web Worker ['<p>']\n",
            "Multiclass classification ['<p>']\n",
            "Regression ['<p>']\n",
            "Reinforcement learning ['<p>']\n",
            "Policy gradient ['<p>']\n",
            "IndexedDB ['<p>']\n",
            "Image ['<p>']\n",
            "(Deploying TF.js in Chrome extension) ['<p>']\n",
            "Convnet ['<p>']\n",
            "(Defining a custom Layer subtype) ['<p>']\n",
            "Building a tf.data.Dataset from a remote CSV ['<p>']\n",
            "Building a tf.data.Dataset using a generator ['<p>']\n",
            "Text-to-text conversion ['<p>']\n",
            "Attention mechanism, RNN ['<p>']\n",
            "Browser and Node.js ['<p>']\n",
            "Saving to filesystem and loading in browser ['<p>']\n",
            "(Deploying TF.js in Electron-based desktop apps) ['<p>']\n",
            "Generative ['<p>']\n",
            "Variational autoencoder (VAE) ['<p>']\n",
            "Export trained model from tfjs-node and load it in browser ['<p>']\n",
            "Multiclass classification, object detection, segmentation ['<p>']\n",
            "Sequence ['<p>']\n",
            "Sequence-to-prediction ['<p>']\n",
            "MLP and RNNs ['<p>']\n",
            "Browser and Node ['<p>']\n",
            "Sequence prediction ['<p>']\n",
            "RNN: LSTM ['<p>']\n",
            "Convolutional neural network ['<p>']\n",
            "Generative Adversarial Network (GAN) ['<p>']\n",
            "Convolutional neural network; GAN ['<p>']\n",
            "Saving to filesystem from Node.js and loading it in the browser ['<p>']\n",
            "Core (Ops) ['<p>']\n",
            "Saving to filesystem ['<p>']\n",
            "Multiclass classification (transfer learning) ['<p>']\n",
            "Loading pretrained model ['<p>']\n",
            "Shallow neural network ['<p>']\n",
            "Various ['<p>']\n",
            "Demonstrates the effect of post-training weight quantization ['<p>']\n",
            "Sequence-to-binary-prediction ['<p>']\n",
            "LSTM, 1D convnet ['<p>']\n",
            "Node.js or Python ['<p>']\n",
            "Load model from Keras and tfjs-node ['<p>']\n",
            "Object detection ['<p>']\n",
            "Convolutional neural network (transfer learning) ['<p>']\n",
            "Deep Q-Network (DQN) ['<p>']\n",
            "LSTM encoder and decoder ['<p>']\n",
            "Load model converted from Keras ['<p>']\n",
            "Dimension reduction and data visualization ['<p>']\n",
            "tSNE ['<p>']\n",
            "Binary classification ['<p>']\n",
            "Node.js version 8.9 or higher ['<ul>', '<li>']\n",
            "OR ['<ul>', '<li>']\n",
            "cd mnist-core yarn yarn watch ['<div highlight highlight-source-shell\">']\n",
            "cd mnist-core npm install npm run watch ['<div highlight highlight-source-shell\">']\n",
            "yarn watch or npm run watch: starts a local development HTTP server which watches the filesystem for changes so you can edit the code (JS or HTML) and see changes when you refresh the page immediately. ['<ul>', '<li>', '<p>']\n",
            "yarn build or npm run build: generates a dist/ folder which contains the build artifacts and can be used for deployment. ['<ul>', '<li>', '<p>']\n",
            "If you want to contribute an example, please reach out to us on ['<p>']\n",
            "before sending us a pull request as we are trying to keep this set of examples small and highly curated. ['<p>']\n",
            "Before you send a pull request, it is a good idea to run the presubmit tests and make sure they all pass. To do that, execute the following commands in the root directory of tfjs-examples: ['<p>']\n",
            "The yarn presubmit command executes the unit tests and lint checks of all the exapmles that contain the yarn test and/or yarn lint scripts. You may also run the tests for individual exampls by cd'ing into their respective subdirectory and executing yarn, followed by yarn test and/or yarn lint. ['<p>']\n",
            "Requirements for Tensorflow (see: ) ['<ol>', '<li>', '<p>']\n",
            "Python packages you might not have: cython, python-opencv, easydict ['<ol>', '<li>', '<p>']\n",
            "For training the end-to-end version of Faster R-CNN with VGG16, 3G of GPU memory is sufficient (using CUDNN) ['<ol>', '<li>']\n",
            "Clone the Faster R-CNN repository ['<ol>', '<li>']\n",
            "Build the Cython modules ['<ol start=\"2\">', '<li>', '<div highlight highlight-source-shell\">']\n",
            "cd $FRCN_ROOT/lib make ['<ol start=\"2\">', '<li>', '<div highlight highlight-source-shell\">']\n",
            "Download the training, validation, test data and VOCdevkit ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "Extract all of these tars into one directory named VOCdevkit ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "tar xvf VOCtrainval_06-Nov-2007.tar tar xvf VOCtest_06-Nov-2007.tar tar xvf VOCdevkit_08-Jun-2007.tar ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "It should have this basic structure ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "$VOCdevkit/                           # development kit ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "$VOCdevkit/VOCcode/                   # VOC utility code ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "$VOCdevkit/VOC2007                    # image sets, annotations, etc. ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "# ... and several other directories ... ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "Create symlinks for the PASCAL VOC dataset ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "cd $FRCN_ROOT/data ln -s $VOCdevkit VOCdevkit2007 ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "Download pre-trained ImageNet models ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "Download the pre-trained ImageNet models ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "mv VGG_imagenet.npy $FRCN_ROOT/data/pretrain_model/VGG_imagenet.npy ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "Run script to train and test model ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "cd $FRCN_ROOT ./experiments/scripts/faster_rcnn_end2end.sh $DEVICE $DEVICE_ID VGG16 pascal_voc ['<ol>', '<li>', '<p>', '<div highlight highlight-source-shell\">', '<span pl-smi\">', '<span pl-c\">']\n",
            "Classes ['<div highlight highlight-source-shell\">']\n",
            "AP ['<div highlight highlight-source-shell\">']\n",
            "aeroplane ['<div highlight highlight-source-shell\">']\n",
            "0.698 ['<div highlight highlight-source-shell\">']\n",
            "bicycle ['<div highlight highlight-source-shell\">']\n",
            "0.788 ['<div highlight highlight-source-shell\">']\n",
            "bird ['<div highlight highlight-source-shell\">']\n",
            "0.657 ['<div highlight highlight-source-shell\">']\n",
            "boat ['<div highlight highlight-source-shell\">']\n",
            "0.565 ['<div highlight highlight-source-shell\">']\n",
            "bottle ['<div highlight highlight-source-shell\">']\n",
            "0.478 ['<div highlight highlight-source-shell\">']\n",
            "bus ['<div highlight highlight-source-shell\">']\n",
            "0.762 ['<div highlight highlight-source-shell\">']\n",
            "car ['<div highlight highlight-source-shell\">']\n",
            "0.797 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "cat ['<div highlight highlight-source-shell\">']\n",
            "0.793 ['<div highlight highlight-source-shell\">']\n",
            "chair ['<div highlight highlight-source-shell\">']\n",
            "0.479 ['<div highlight highlight-source-shell\">']\n",
            "cow ['<div highlight highlight-source-shell\">']\n",
            "0.724 ['<div highlight highlight-source-shell\">']\n",
            "diningtable ['<div highlight highlight-source-shell\">']\n",
            "0.648 ['<div highlight highlight-source-shell\">']\n",
            "dog ['<div highlight highlight-source-shell\">']\n",
            "0.803 ['<div highlight highlight-source-shell\">']\n",
            "horse ['<div highlight highlight-source-shell\">']\n",
            "motorbike ['<div highlight highlight-source-shell\">']\n",
            "0.732 ['<div highlight highlight-source-shell\">']\n",
            "person ['<div highlight highlight-source-shell\">']\n",
            "0.770 ['<div highlight highlight-source-shell\">']\n",
            "pottedplant ['<div highlight highlight-source-shell\">']\n",
            "0.384 ['<div highlight highlight-source-shell\">']\n",
            "sheep ['<div highlight highlight-source-shell\">']\n",
            "0.664 ['<div highlight highlight-source-shell\">']\n",
            "sofa ['<div highlight highlight-source-shell\">']\n",
            "0.650 ['<div highlight highlight-source-shell\">']\n",
            "train ['<div highlight highlight-source-shell\">']\n",
            "0.766 ['<div highlight highlight-source-shell\">']\n",
            "tvmonitor ['<div highlight highlight-source-shell\">']\n",
            "0.666 ['<div highlight highlight-source-shell\">']\n",
            "0.681 ['<div highlight highlight-source-shell\">']\n",
            "Featured code sample ['<b>', '<h2>']\n",
            "Code from the Google Cloud NEXT 2018 session \"Tensorflow, deep             learning and modern convnets, without a PhD\". Other samples from the \"Tensorflow without a PhD\" series are in             this repository too. ['<b>', '<h2>']\n",
            "The basics of building neural networks for software engineers. Neural weights and biases, activation functions, supervised learning and gradient descent.                      Tips and best practices for efficient training: learning rate decay, dropout regularisation and the intricacies of overfitting. Dense and convolutional neural networks. This session starts with low-level                      Tensorflow and also has a sample of high-level Tensorflow code using layers and Datasets. Code sample: MNIST handwritten digit recognition with 99% accuracy. Duration: 55 min ['<b>', '<div align=\"center\">', '<br/>', '<p>']\n",
            "What is batch normalisation, how to use it appropriately and how to see if it is working or not.                                           Code sample: MNIST handwritten digit recognition with 99.5% accuracy. Duration: 25 min ['<b>', '<div align=\"center\">', '<br/>', '<p>']\n",
            "RNN basics: the RNN cell as a state machine, training and unrolling (backpropagation through time).                  More complex RNN cells: LSTM and GRU cells. Application to language modeling and generation. Tensorflow APIs for RNNs.                  Code sample: RNN-generated Shakespeare play. Duration: 55 min ['<b>', '<div align=\"center\">', '<br/>', '<p>']\n",
            "Convolutional neural network architectures for image processing. Convnet basics, convolution filters and how to stack them.                    Learnings from the Inception model: modules with parallel convolutions, 1x1 convolutions. A simple modern convnet architecture: Squeezenet.                   Convenets for detection: the YOLO (You Look Only Once) architecture. Full-scale model training and serving with Tensorflow's Estimator API on Google                   Cloud ML Engine and Cloud TPUs (Tensor Processing Units).                   Application: airplane detection in aerial imagery. Duration: 55 min ['<b>', '<div align=\"center\">', '<br/>', '<p>']\n",
            "Advanced RNN architectures for natural language processing. Word embeddings, text classification,                      bidirectional models, sequence to sequence models for translation. Attention mechanisms. This session also explores                      Tensorflow's powerful seq2seq API. Applications: toxic comment detection and langauge translation.                      Co-author: Nithum Thain. Duration: 55 min ['<b>', '<div align=\"center\">', '<br/>', '<p>']\n",
            "A neural network trained to play the game of Pong from just the pixels of the game.             Uses reinforcement learning and policy gradients. The approach can be generalized to             other problems involving a non-differentiable step that cannot be trained using traditional supervised learning techniques.             A practical application: neural architecture search - neural networks designing neural networks. Co-author: Yu-Han Liu. Duration: 40 min ['<b>', '<div align=\"center\">', '<br/>', '<p>']\n",
            "Quick access to all code samples: ['<p>', '<b>']\n",
            "dense and convolutional neural network tutorial ['<p>', '<b>']\n",
            "recurrent neural network tutorial using temperature series ['<p>', '<b>']\n",
            "\"pong\" with reinforcement learning ['<p>', '<b>']\n",
            "airplane detection model ['<p>', '<b>']\n",
            "Toxic comment detection with RNNs and attention ['<p>', '<b>']\n",
            "*Disclaimer: This is not an official Google product but sample code provided for an educational purpose* ['<b>']\n",
            "Sonnet is a library built on top of  designed to provide simple, composable abstractions for machine learning research. ['<p>']\n",
            "Sonnet has been designed and built by researchers at DeepMind. It can be used to construct neural networks for many different purposes (un/supervised learning, reinforcement learning, ...). We find it is a successful abstraction for our organization, you might too! ['<p>']\n",
            "More specifically, Sonnet provides a simple but powerful programming model centered around a single concept: snt.Module. Modules can hold references to parameters, other modules and methods that apply some function on the user input. Sonnet ships with many predefined modules (e.g. snt.Linear, ['<p>', '<code>']\n",
            "snt.Conv2D, snt.BatchNorm) and some predefined networks of modules (e.g. ['<p>', '<code>']\n",
            "snt.nets.MLP) but users are also encouraged to build their own modules. ['<p>', '<code>']\n",
            "Unlike many frameworks Sonnet is extremely unopinionated about how you will use your modules. Modules are designed to be self contained and entirely decoupled from one another. Sonnet does not ship with a training framework and users are encouraged to build their own or adopt those built by others. ['<p>']\n",
            "Sonnet is also designed to be simple to understand, our code is (hopefully!) clear and focussed. Where we have picked defaults (e.g. defaults for initial parameter values) we try to point out why. ['<p>']\n",
            "import sonnet as snt ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-en\">']\n",
            "print(\"TensorFlow version {}\".format(tf.__version__)) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-en\">']\n",
            "print(\"Sonnet version {}\".format(snt.__version__)) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-en\">']\n",
            "Sonnet ships with a number of built in modules that you can trivially use. For example to define an MLP we can use the snt.Sequential module to call a sequence of modules, passing the output of a given module as the input for the next module. We can use snt.Linear and tf.nn.relu to actually define our computation: ['<p>']\n",
            "mlp = snt.Sequential([     snt.Linear(1024),     tf.nn.relu,     snt.Linear(10), ]) ['<div highlight highlight-source-python\">']\n",
            "It is also very common to request all the parameters for your module. Most modules in Sonnet create their parameters the first time they are called with some input (since in most cases the shape of the parameters is a function of the input). Sonnet modules provide two properties for accessing parameters. ['<p>']\n",
            "It is worth noting that tf.Variables are not just used for parameters of your model. For example they are used to hold state in metrics used in ['<p>', '<code>']\n",
            "snt.BatchNorm. In most cases users retrieve the module variables to pass them to an optimizer to be updated. In this case non-trainable variables should typically not be in that list as they are updated via a different mechanism. TensorFlow has a built in mechanism to mark variables as \"trainable\" (parameters of your model) vs. non-trainable (other variables). Sonnet provides a mechanism to gather all trainable variables from your module which is probably what you want to pass to an optimizer: ['<p>', '<code>']\n",
            "class MyLinear(snt.Module): ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-en\">']\n",
            "def __init__(self, output_size, name=None):     super(MyLinear, self).__init__(name=name)     self.output_size = output_size ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-en\">']\n",
            "@snt.once   def _initialize(self, x):     initial_w = tf.random.normal([x.shape[1], self.output_size])     self.w = tf.Variable(initial_w, name=\"w\")     self.b = tf.Variable(tf.zeros([self.output_size]), name=\"b\") ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-en\">']\n",
            "def __call__(self, x):     self._initialize(x)     return tf.matmul(x, self.w) + self.b ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-en\">']\n",
            "By subclassing snt.Module you get many nice properties for free. For example a default implementation of __repr__ which shows constructor arguments (very useful for debugging and introspection): ['<p>']\n",
            "&gt;&gt;&gt; mod.variables (&lt;tf.Variable 'my_linear/b:0' shape=(10,) ...)&gt;,  &lt;tf.Variable 'my_linear/w:0' shape=(1, 10) ...)&gt;) ['<div highlight highlight-source-python\">']\n",
            "You may notice the my_linear prefix on the variables above. This is because Sonnet modules also enter the modules name scope whenever methods are called. By entering the module name scope we provide a much more useful graph for tools like TensorBoard to consume (e.g. all operations that occur inside my_linear will be in a group called my_linear). ['<p>']\n",
            "Sonnet supports multiple serialization formats. The simplest format we support is Python's pickle, and all built in modules are tested to make sure they can be saved/loaded via pickle in the same Python process. In general we discourage the use of pickle, it is not well supported by many parts of TensorFlow and in our experience can be quite brittle. ['<p>']\n",
            "TensorFlow checkpointing can be used to save the value of parameters periodically during training. This can be useful to save the progress of training in case your program crashes or is stopped. Sonnet is designed to work cleanly with TensorFlow checkpointing: ['<p>']\n",
            "checkpoint_root = \"/tmp/checkpoints\" ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "checkpoint_name = \"example\" ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "save_prefix = os.path.join(checkpoint_root, checkpoint_name) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "my_module = create_my_sonnet_module()  # Can be anything extending snt.Module. ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# A `Checkpoint` object manages checkpointing of the TensorFlow state associated ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# with the objects passed to it's constructor. Note that Checkpoint supports ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# restore on create, meaning that the variables of `my_module` do **not** need ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# to be created before you restore from a checkpoint (their value will be ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# restored when they are created). ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "checkpoint = tf.train.Checkpoint(module=my_module) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# Most training scripts will want to restore from a checkpoint if one exists. This ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# would be the case if you interrupted your training (e.g. to use your GPU for ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# something else, or in a cloud environment if your instance is preempted). ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "latest = tf.train.latest_checkpoint(checkpoint_root) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "if latest is not None:   checkpoint.restore(latest) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "for step_num in range(num_steps):   train(my_module) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# During training we will occasionally save the values of weights. Note that   # this is a blocking call and can be slow (typically we are writing to the   # slowest storage on the machine). If you have a more reliable setup it might be   # appropriate to save less frequently.   if step_num and not step_num % 1000:     checkpoint.save(save_prefix) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# Make sure to save your final values!! ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "checkpoint.save(save_prefix) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "TensorFlow saved models can be used to save a copy of your network that is decoupled from the Python source for it. This is enabled by saving a TensorFlow graph describing the computation and a checkpoint containing the value of weights. ['<p>']\n",
            "Next, we need to create another module describing the specific parts of our model that we want to export. We advise doing this (rather than modifying the original model in-place) so you have fine grained control over what is actually exported. This is typically important to avoid creating very large saved models, and such that you only share the parts of your model you want to (e.g. you only want to share the generator for a GAN but keep the discriminator private). ['<p>']\n",
            "@tf.function(input_signature=[tf.TensorSpec([None, input_size])]) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "def inference(x):   return my_module(x) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "to_save = snt.Module() ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "to_save.inference = inference ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "to_save.all_variables = list(my_module.variables) ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "tf.saved_model.save(to_save, \"/tmp/example_saved_model\") ['<div highlight highlight-source-python\">', '<span pl-k\">', '<span pl-s1\">']\n",
            "$ ls -lh /tmp/example_saved_model total 24K drwxrwsr-t 2 tomhennigan 154432098 4.0K Apr 28 00:14 assets -rw-rw-r-- 1 tomhennigan 154432098  14K Apr 28 00:15 saved_model.pb drwxrwsr-t 2 tomhennigan 154432098 4.0K Apr 28 00:15 variables ['<div highlight highlight-source-shell\">']\n",
            "loaded = tf.saved_model.load(\"/tmp/example_saved_model\") ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "# Use the inference method. Note this doesn't run the Python code from `to_save` ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "# but instead uses the TensorFlow Graph that is part of the saved model. ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "loaded.inference(tf.ones([1, input_size])) ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "# The all_variables property can be used to retrieve the restored variables. ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "assert len(loaded.all_variables) &gt; 0 ['<div highlight highlight-source-python\">', '<span pl-c\">', '<span pl-s1\">', '<span pl-k\">']\n",
            "Note that the loaded object is not a Sonnet module, it is a container object that has the specific methods (e.g. inference) and properties (e.g. ['<p>', '<code>']\n",
            "all_variables) that we added in the previous block. ['<p>', '<code>']\n",
            "A key difference between Sonnet and distributed training using tf.keras is that Sonnet modules and optimizers do not behave differently when run under distribution strategies (e.g. we do not average your gradients or sync your batch norm stats). We believe that users should be in full control of these aspects of their training and they should not be baked into the library. The trade off here is that you need to implement these features in your training script (typically this is just 2 lines of code to all reduce your gradients before applying your optimizer) or swap in modules that are explicitly distribution aware (e.g. snt.distribute.CrossReplicaBatchNorm). ['<p>']\n",
            "AC-GAN ['<ul>', '<li>']\n",
            "Info-GAN ['<ul>', '<li>']\n",
            "AC-GAN result: ['<p>']\n",
            "The results were obtained after training for ~6-7 hrs on a 12GB TitanX. ['<ul>', '<li>']\n",
            "The code was originally written and tested with tensorflow0.11 and python2.7. The tf.summary calls have been updated to work with tensorflow version 0.12. To work with older versions of tensorflow use branch . ['<ul>', '<li>']\n",
            "Some of the problems while working with tensorflow1.0 and in windows have been discussed in . ['<ul>', '<li>']\n",
            "To train model simply execute python FCN.py ['<ul>', '<li>']\n",
            "To visualize results for a random batch of images use flag --mode=visualize ['<ul>', '<li>']\n",
            "debug flag can be set during training to add information regarding activations, gradients, variables etc. ['<ul>', '<li>']\n",
            "The  in logs folder can be used to view results in color as below. ['<ul>', '<li>']\n",
            "The small batch size was necessary to fit the training model in memory but explains the slow learning ['<ul>', '<li>']\n",
            "Concepts that had many examples seem to be correctly identified and segmented - in the example above you can see that cars, persons were identified better. I believe this can be solved by training for longer epochs. ['<ul>', '<li>']\n",
            "Also the resizing of images cause loss of information - you can notice this in the fact smaller objects are segmented with less accuracy. ['<ul>', '<li>']\n",
            "If you closely watch the gradients you will notice the inital training is almost entirely on the new layers added - it is only after these layers are reasonably trained do we see the VGG layers get some gradient flow. This is understandable as changes the new layers affect the loss objective much more in the beginning. ['<ul>', '<li>']\n",
            "The earlier layers of the netowrk are initialized with VGG weights and so conceptually would require less tuning unless the train data is extremely varied - which in this case is not. ['<ul>', '<li>']\n",
            "The first layer of convolutional model captures low level information and since this entrirely dataset dependent you notice the gradients adjusting the first layer weights to accustom the model to the dataset. ['<ul>', '<li>']\n",
            "The other conv layers from VGG have very small gradients flowing as the concepts captured here are good enough for our end objective - Segmentation. ['<ul>', '<li>']\n",
            "This is the core reason Transfer Learning works so well. Just thought of pointing this out while here. ['<ul>', '<li>']\n",
            "Video of the presentaion given by the authors on the paper - ['<ul>', '<li>']\n",
            "The last few years have seen a rise in novel differentiable graphics layers which can be inserted in neural network architectures. From spatial transformers to differentiable graphics renderers, these new layers leverage the knowledge acquired over years of computer vision and graphics research to build new and more efficient network architectures. Explicitly modeling geometric priors and constraints into neural networks opens up the door to architectures that can be trained robustly, efficiently, and more importantly, in a self-supervised fashion. ['<p>']\n",
            "At a high level, a computer graphics pipeline requires a representation of 3D objects and their absolute positioning in the scene, a description of the material they are made of, lights and a camera. This scene description is then interpreted by a renderer to generate a synthetic rendering. ['<p>']\n",
            "In comparison, a computer vision system would start from an image and try to infer the parameters of the scene. This allows the prediction of which objects are in the scene, what materials they are made of, and their three-dimensional position and orientation. ['<p>']\n",
            "Training machine learning systems capable of solving these complex 3D vision tasks most often requires large quantities of data. As labelling data is a costly and complex process, it is important to have mechanisms to design machine learning models that can comprehend the three dimensional world while being trained without much supervision. Combining computer vision and computer graphics techniques provides a unique opportunity to leverage the vast amounts of readily available unlabelled data. As illustrated in the image below, this can, for instance, be achieved using analysis by synthesis where the vision system extracts the scene parameters and the graphics system renders back an image based on them. If the rendering matches the original image, the vision system has accurately extracted the scene parameters. In this setup, computer vision and computer graphics go hand in hand, forming a single machine learning system similar to an autoencoder, which can be trained in a self-supervised manner. ['<p>']\n",
            "Tensorflow Graphics is being developed to help tackle these types of challenges and to do so, it provides a set of differentiable graphics and geometry layers (e.g. cameras, reflectance models, spatial transformations, mesh convolutions) and 3D viewer functionalities (e.g. 3D TensorBoard) that can be used to train and debug your machine learning models of choice. ['<p>']\n",
            "TensorFlow Graphics is fully compatible with the latest stable release of TensorFlow, tf-nightly, and tf-nightly-2.0-preview. All the functions are compatible with graph and eager execution. ['<p>']\n",
            "Tensorflow Graphics heavily relies on L2 normalized tensors, as well as having the inputs to specific function be in a pre-defined range. Checking for all of this takes cycles, and hence is not activated by default. It is recommended to turn these checks on during a couple epochs of training to make sure that everything behaves as expected. This ['<p>']\n",
            "provides the instructions to enable these checks. ['<p>']\n",
            "To help you get started with some of the functionalities provided by TF Graphics, some Colab notebooks are available below and roughly ordered by difficulty. These Colabs touch upon a large range of topics including, object pose estimation, interpolation, object materials, lighting, non-rigid surface deformation, spherical harmonics, and mesh convolutions. ['<p>']\n",
            "NOTE: the tutorials are maintained carefully. However, they are not considered part of the API and they can change at any time without warning. It is not advised to write code that takes dependency on them. ['<p>']\n",
            "Visual debugging is a great way to assess whether an experiment is going in the right direction. To this end, TensorFlow Graphics comes with a TensorBoard plugin to interactively visualize 3D meshes and point clouds. ['<p>']\n",
            "shows how to use the plugin. Follow ['<p>']\n",
            "to install and configure TensorBoard 3D. Note that TensorBoard 3D is currently not compatible with eager execution nor TensorFlow 2. ['<p>']\n",
            ": Ask or answer technical questions. ['<ul>', '<li>']\n",
            ": Report bugs or make feature requests. ['<ul>', '<li>']\n",
            ": Stay up to date on content from the TensorFlow team and best articles from the community. ['<ul>', '<li>']\n",
            ": Follow TensorFlow shows. ['<ul>', '<li>']\n",
            "@inproceedings{TensorflowGraphicsIO2019,    author = {Valentin, Julien and Keskin, Cem and Pidlypenskyi, Pavel and Makadia, Ameesh and Sud, Avneesh and Bouaziz, Sofien},    title = {TensorFlow Graphics: Computer Graphics Meets Deep Learning},    year = {2019} } ['<pre>']\n",
            "Sofien Bouaziz () ['<ul>', '<li>']\n",
            "Jay Busch ['<ul>', '<li>']\n",
            "Forrester Cole ['<ul>', '<li>']\n",
            "Ambrus Csaszar ['<ul>', '<li>']\n",
            "Boyang Deng ['<ul>', '<li>']\n",
            "Ariel Gordon ['<ul>', '<li>']\n",
            "Christian Häne ['<ul>', '<li>']\n",
            "Cem Keskin ['<ul>', '<li>']\n",
            "Ameesh Makadia ['<ul>', '<li>']\n",
            "Rohit Pandey ['<ul>', '<li>']\n",
            "Romain Prévost ['<ul>', '<li>']\n",
            "Pavel Pidlypenskyi ['<ul>', '<li>']\n",
            "Stefan Popov ['<ul>', '<li>']\n",
            "Konstantinos Rematas ['<ul>', '<li>']\n",
            "Omar Sanseviero ['<ul>', '<li>']\n",
            "Aviv Segal ['<ul>', '<li>']\n",
            "Avneesh Sud ['<ul>', '<li>']\n",
            "Andrea Tagliasacchi ['<ul>', '<li>']\n",
            "Anastasia Tkach ['<ul>', '<li>']\n",
            "Julien Valentin ['<ul>', '<li>']\n",
            "He Wang ['<ul>', '<li>']\n",
            "Yinda Zhang ['<ul>', '<li>']\n",
            "It’s developed and maintained by Google. As such, a continued support and development is ensured ['<blockquote>', '<ul>', '<li>']\n",
            "Very large and active community ['<blockquote>', '<ul>', '<li>']\n",
            "Low-level and high-level interfaces to network training ['<blockquote>', '<ul>', '<li>']\n",
            "Tensorboard is the powerful visualization suite which is developed to track both the network topology and performance, making debugging even simpler. ['<blockquote>', '<ul>', '<li>']\n",
            "Written in Python (even though some parts crucial for performance is implemented in C++) which is a very attractive language to read and develop in ['<blockquote>', '<ul>', '<li>']\n",
            "Multiple GPUs support. So you can freely run the code on different machines without having to stop or restart the program ['<blockquote>', '<ul>', '<li>']\n",
            "Faster model compilation than Theano-based options ['<blockquote>', '<ul>', '<li>']\n",
            "Faster compile times than Theano ['<blockquote>', '<ul>', '<li>']\n",
            "Is about more than deep learning. TensorFlow actually has tools to support reinforcement learning and other algorithms. ['<blockquote>', '<ul>', '<li>']\n",
            "There is no or very limited explanation of what is happening in the code. ['<blockquote>', '<ul>', '<li>']\n",
            "Different parts are not connected in a meaningful way. ['<blockquote>', '<ul>', '<li>']\n",
            "The code implementation is too vague or complicated. ['<blockquote>', '<ul>', '<li>']\n",
            "The focus is on either advanced or elementary level of Tensorflow implementation. ['<blockquote>', '<ul>', '<li>']\n",
            "The aim here is to explain how to install TensorFlow library \"step by step\" and on different operating systems. TensorFlow is a python library. Similar to many others, we tried installing many side packages and libraries and experienced lots of problems and errors. ['<p>']\n",
            "0 ['<p>', '<code>']\n",
            "Installation ['<li>']\n",
            "Basics ['<li>']\n",
            "Logistic_Regression ['<li>']\n",
            "Feed_Forward_Neural_Network ['<li>']\n",
            "Tensorboard ['<li>']\n",
            "AutoEncoder ['<li>']\n",
            "Convolutional_Neural_Network ['<li>']\n",
            "- Simple and ready-to-use tutorials for TensorFlow ['<blockquote>', '<ul>', '<li>']\n",
            "Data Formats ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Predict Server ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Predict Client ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Network Models ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Logistic regression ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Deep neural network ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Convolution neural network ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Wide and deep model ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Regression model ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Customized models ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Other Features ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Checkpoint ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "TensorBoard ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Exporter ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Optimizers ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Learning rate decay ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Batch normalization ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Benchmark mode ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "cd ./data/cancer/ ['<pre>']\n",
            "./generate_csv_tfrecords.py ['<pre>']\n",
            "cd ./data/a8a/ ['<pre>']\n",
            "./generate_libsvm_tfrecord.py ['<pre>']\n",
            "./dense_classifier.py ['<pre>']\n",
            "./sparse_classifier.py ['<pre>']\n",
            "./dense_classifier.py --train_file ./data/iris/iris_train.csv.tfrecords --validate_file ./data/iris/iris_test.csv.tfrecords --feature_size 4 --label_size 3  --enable_colored_log ['<pre>']\n",
            "./dense_classifier.py --train_file ./data/iris/iris_train.csv --validate_file ./data/iris/iris_test.csv --feature_size 4 --label_size 3 --input_file_format csv --enable_colored_log ['<pre>']\n",
            "./predict_client.py --host 127.0.0.1 --port 9000 --model_name dense --model_version 1 ['<pre>']\n",
            "mvn compile exec:java -Dexec.mainClass=\"com.tobe.DensePredictClient\" -Dexec.args=\"127.0.0.1 9000 dense 1\" ['<pre>']\n",
            "A modified version of DeepFM is used to win the 4th Place for . See the slide  how we deal with fields containing sequences, how we incoporate various FM components into deep model. ['<ul>', '<li>']\n",
            "Xi: [[ind1_1, ind1_2, ...], [ind2_1, ind2_2, ...], ..., [indi_1, indi_2, ..., indi_j, ...], ...] ['<ul contains-task-list\">', '<li task-list-item\">', '<ul>', '<li>']\n",
            "indi_j is the feature index of feature field j of sample i in the dataset ['<ul contains-task-list\">', '<li task-list-item\">', '<ul>', '<li>']\n",
            "Xv: [[val1_1, val1_2, ...], [val2_1, val2_2, ...], ..., [vali_1, vali_2, ..., vali_j, ...], ...] ['<ul contains-task-list\">', '<li task-list-item\">', '<ul>', '<li>']\n",
            "vali_j is the feature value of feature field j of sample i in the dataset ['<ul contains-task-list\">', '<li task-list-item\">', '<ul>', '<li>']\n",
            "vali_j can be either binary (1/0, for binary/categorical features) or float (e.g., 10.24, for numerical features) ['<ul contains-task-list\">', '<li task-list-item\">', '<ul>', '<li>']\n",
            "y: target of each sample in the dataset (1/0 for classification, numeric number for regression) ['<ul contains-task-list\">', '<li task-list-item\">', '<ul>', '<li>']\n",
            "import tensorflow as tf from sklearn.metrics import roc_auc_score ['<pre>']\n",
            "# params dfm_params = {     \"use_fm\": True,     \"use_deep\": True,     \"embedding_size\": 8,     \"dropout_fm\": [1.0, 1.0],     \"deep_layers\": [32, 32],     \"dropout_deep\": [0.5, 0.5, 0.5],     \"deep_layers_activation\": tf.nn.relu,     \"epoch\": 30,     \"batch_size\": 1024,     \"learning_rate\": 0.001,     \"optimizer_type\": \"adam\",     \"batch_norm\": 1,     \"batch_norm_decay\": 0.995,     \"l2_reg\": 0.01,     \"verbose\": True,     \"eval_metric\": roc_auc_score,     \"random_seed\": 2017 } ['<pre>']\n",
            "# prepare training and validation data in the required format Xi_train, Xv_train, y_train = prepare(...) Xi_valid, Xv_valid, y_valid = prepare(...) ['<pre>']\n",
            "# init a DeepFM model dfm = DeepFM(**dfm_params) ['<pre>']\n",
            "# fit a DeepFM model dfm.fit(Xi_train, Xv_train, y_train) ['<pre>']\n",
            "# make prediction dfm.predict(Xi_valid, Xv_valid) ['<pre>']\n",
            "# evaluate a trained model dfm.evaluate(Xi_valid, Xv_valid, y_valid) ['<pre>']\n",
            "$ cd example $ python main.py ['<pre>']\n",
            "You should tune the parameters for each model in order to get reasonable performance. ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "You can also try to ensemble these models or ensemble them with other models (e.g., XGBoost or LightGBM). ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "He Xiangnan's ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Jian Zhang's  (yellowfin optimizer is taken from here) ['<ul contains-task-list\">', '<li task-list-item\">']\n",
            "Download YOLOv3 weights from . ['<ol>', '<li>']\n",
            "Convert the Darknet YOLO model to a Keras model. ['<ol>', '<li>']\n",
            "Run YOLO detection. ['<ol>', '<li>']\n",
            "wget https://pjreddie.com/media/files/yolov3.weights python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python yolo_video.py [OPTIONS...] --image, for image detection mode, OR python yolo_video.py [video_path] [output_path (optional)] ['<pre>']\n",
            "usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output] ['<pre>']\n",
            "positional arguments:   --input        Video input path   --output       Video output path ['<pre>']\n",
            "optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments ['<pre>']\n",
            "MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the . ['<ol start=\"4\">', '<li>']\n",
            "Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: ['<ol>', '<li>', '<p>', '<pre>', '<code>']\n",
            "path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ... ['<ol>', '<li>', '<p>', '<pre>', '<code>']\n",
            "Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights. ['<ol>', '<li>', '<p>', '<pre>', '<code>']\n",
            "Modify train.py and start training. ['<ol>', '<li>', '<p>', '<pre>', '<code>']\n",
            "python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file. ['<ol>', '<li>', '<p>', '<pre>', '<code>']\n",
            "If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py ['<p>']\n",
            "The test environment is ['<ol>', '<li>', '<p>', '<ul>']\n",
            "Keras 2.1.5 ['<ol>', '<li>', '<p>', '<ul>']\n",
            "tensorflow 1.6.0 ['<ol>', '<li>', '<p>', '<ul>']\n",
            "Default anchors are used. If you use your own anchors, probably some changes are needed. ['<ol>', '<li>', '<p>', '<ul>']\n",
            "The inference result is not totally the same as Darknet but the difference is small. ['<ol>', '<li>', '<p>', '<ul>']\n",
            "The speed is slower than Darknet. Replacing PIL with opencv may help a little. ['<ol>', '<li>', '<p>', '<ul>']\n",
            "Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning. ['<ol>', '<li>', '<p>', '<ul>']\n",
            "The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed. ['<ol>', '<li>', '<p>', '<ul>']\n",
            "For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See  for more information on bottleneck features. ['<ol>', '<li>', '<p>', '<ul>']\n",
            "Natural Language Processing（自然语言处理） ['<ul>', '<li>', '<p>']\n",
            "IMDB（ENG） ['<ul>', '<li>', '<p>']\n",
            "CLUE Emotion Analysis Dataset (CHN) ['<ul>', '<li>', '<p>']\n",
            "SNLI（ENG） ['<ul>', '<li>', '<p>']\n",
            "微众银行智能客服（CHN） ['<ul>', '<li>', '<p>']\n",
            "蚂蚁金融语义相似度 (CHN) ['<ul>', '<li>', '<p>']\n",
            "ATIS（ENG） ['<ul>', '<li>', '<p>']\n",
            "ElasticSearch ['<ul>', '<li>', '<p>']\n",
            "Sparse Retrieval ['<ul>', '<li>', '<p>']\n",
            "Dense Retrieval ['<ul>', '<li>', '<p>']\n",
            "Large-scale Chinese Conversation Dataset (CHN) ['<ul>', '<li>', '<p>']\n",
            "20k 腾讯 AI 研发数据（CHN） ['<ul>', '<li>', '<p>']\n",
            "Facebook's Hierarchical Task Oriented Dialog（ENG） ['<ul>', '<li>', '<p>']\n",
            "bAbI（ENG） ['<ul>', '<li>', '<p>']\n",
            "Topic Modelling ['<ul>', '<li>', '<p>']\n",
            "Explain Prediction ['<ul>', '<li>', '<p>']\n",
            "Knowledge Graph（知识图谱） ['<ul>', '<li>', '<p>']\n",
            "Movielens 1M（English Data） ['<ul>', '<li>', '<p>']\n",
            "└── finch/tensorflow2/text_classification/imdb \t│ \t├── data \t│   └── glove.840B.300d.txt          # pretrained embedding, download and put here \t│   └── make_data.ipynb              # step 1. make data and vocab: train.txt, test.txt, word.txt \t│   └── train.txt  \t\t     # incomplete sample, format &lt;label, text&gt; separated by \\t  \t│   └── test.txt   \t\t     # incomplete sample, format &lt;label, text&gt; separated by \\t \t│   └── train_bt_part1.txt  \t     # (back-translated) incomplete sample, format &lt;label, text&gt; separated by \\t \t│ \t├── vocab \t│   └── word.txt                     # incomplete sample, list of words in vocabulary \t│\t \t└── main \t\t└── sliced_rnn.ipynb         # step 2: train and evaluate model \t\t└── ... ['<pre>']\n",
            "Task: （English Data） ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Training Data: 25000, Testing Data: 25000, Labels: 2 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Back-Translation increases training data from 25000 to 50000    which is done by \"english -&gt; french -&gt; english\" translation ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Model: TF-IDF + Logistic Regression () ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Logistic Regression ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Binary TF ['<ul>', '<li>', '<p>', '<pre>']\n",
            "NGram Range ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Knowledge Dist ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Testing Accuracy ['<ul>', '<li>', '<p>', '<pre>']\n",
            "False ['<ul>', '<li>', '<p>', '<pre>']\n",
            "(1, 1) ['<ul>', '<li>', '<p>', '<pre>']\n",
            "88.3% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "True ['<ul>', '<li>', '<p>', '<pre>']\n",
            "88.8% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "(1, 2) ['<ul>', '<li>', '<p>', '<pre>']\n",
            "89.6% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "90.7% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "-&gt;  Equivalent ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Model: , CNN and RNN ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Code ['<ul>', '<li>', '<p>', '<pre>']\n",
            "FastText (Unigram) ['<ul>', '<li>', '<p>', '<pre>']\n",
            "87.3% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "FastText (Bigram) ['<ul>', '<li>', '<p>', '<pre>']\n",
            "89.8% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "FastText (AutoTune) ['<ul>', '<li>', '<p>', '<pre>']\n",
            "90.1% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "91.8% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "92.6% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Model: Large-scale Transformer ['<ul>', '<li>', '<p>', '<pre>']\n",
            "TensorFlow 2 + ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Batch Size ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Max Length ['<ul>', '<li>', '<p>', '<pre>']\n",
            "32 ['<p>', '<code>']\n",
            "128 ['<p>', '<code>']\n",
            "16 ['<p>', '<code>']\n",
            "200 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "93.3% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "12 ['<p>', '<code>']\n",
            "256 ['<p>', '<code>']\n",
            "93.8% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "300 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "94% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "94.7% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "└── finch/tensorflow2/text_classification/clue \t│ \t├── data \t│   └── make_data.ipynb              # step 1. make data and vocab \t│   └── train.txt  \t\t     # download from clue benchmark \t│   └── test.txt   \t\t     # download from clue benchmark \t│ \t├── vocab \t│   └── label.txt                    # list of emotion labels \t│\t \t└── main \t\t└── bert_finetune.ipynb      # step 2: train and evaluate model \t\t└── ... ['<pre>']\n",
            "Task: （Chinese Data） ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Training Data: 31728, Testing Data: 3967, Labels: 7 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Model: TF-IDF + Linear Model ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Split By ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Char ['<ul>', '<li>', '<p>', '<pre>']\n",
            "57.4% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Word ['<ul>', '<li>', '<p>', '<pre>']\n",
            "57.7% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "57.8% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "58.3% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "59.1% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "59.4% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Model: Deep Model ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Env ['<ul>', '<li>', '<p>', '<pre>']\n",
            "TF2 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "61.7% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "BERT +  () ['<ul>', '<li>', '<p>', '<pre>']\n",
            "62.3% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "└── finch/tensorflow2/text_matching/snli \t│ \t├── data \t│   └── glove.840B.300d.txt       # pretrained embedding, download and put here \t│   └── download_data.ipynb       # step 1. run this to download snli dataset \t│   └── make_data.ipynb           # step 2. run this to generate train.txt, test.txt, word.txt  \t│   └── train.txt  \t\t  # incomplete sample, format &lt;label, text1, text2&gt; separated by \\t  \t│   └── test.txt   \t\t  # incomplete sample, format &lt;label, text1, text2&gt; separated by \\t \t│ \t├── vocab \t│   └── word.txt                  # incomplete sample, list of words in vocabulary \t│\t \t└── main               \t\t└── dam.ipynb      \t  # step 3. train and evaluate model \t\t└── esim.ipynb      \t  # step 3. train and evaluate model \t\t└── ...... ['<pre>']\n",
            "Training Data: 550152, Testing Data: 10000, Labels: 3 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Reference ['<ul>', '<li>', '<p>', '<pre>']\n",
            "85.3% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "87.1% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "87.4% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "87.7% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "RE3 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "90.4% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "91.1% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "└── finch/tensorflow2/text_matching/chinese \t│ \t├── data \t│   └── make_data.ipynb           # step 1. run this to generate char.txt and char.npy \t│   └── train.csv  \t\t  # incomplete sample, format &lt;text1, text2, label&gt; separated by comma  \t│   └── test.csv   \t\t  # incomplete sample, format &lt;text1, text2, label&gt; separated by comma \t│ \t├── vocab \t│   └── cc.zh.300.vec             # pretrained embedding, download and put here \t│   └── char.txt                  # incomplete sample, list of chinese characters \t│   └── char.npy                  # saved pretrained embedding matrix for this task \t│\t \t└── main               \t\t└── pyramid.ipynb      \t  # step 2. train and evaluate model \t\t└── esim.ipynb      \t  # step 2. train and evaluate model \t\t└── ...... ['<pre>']\n",
            "Training Data: 100000, Testing Data: 10000, Labels: 2, Balanced ['<ul>', '<li>', '<p>', '<pre>']\n",
            "(数据示例) ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Model\t(can be compared to  since the dataset is the same) ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Split by ['<ul>', '<li>', '<p>', '<pre>']\n",
            "82.5% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "82.7% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "83.8% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "TF1 + ['<ul>', '<li>', '<p>', '<pre>']\n",
            "84.75% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "└── finch/tensorflow2/text_matching/ant \t│ \t├── data \t│   └── make_data.ipynb           # step 1. run this to generate char.txt and char.npy \t│   └── train.json           \t  # incomplete sample, format &lt;text1, text2, label&gt; separated by comma  \t│   └── dev.json   \t\t  # incomplete sample, format &lt;text1, text2, label&gt; separated by comma \t│ \t├── vocab \t│   └── cc.zh.300.vec             # pretrained embedding, download and put here \t│   └── char.txt                  # incomplete sample, list of chinese characters \t│   └── char.npy                  # saved pretrained embedding matrix for this task \t│\t \t└── main               \t\t└── pyramid.ipynb      \t  # step 2. train and evaluate model \t\t└── bert.ipynb      \t  # step 2. train and evaluate model \t\t└── ...... ['<pre>']\n",
            "Training Data: 34334, Testing Data: 4316, Labels: 2, Imbalanced ['<ul>', '<li>', '<p>', '<pre>']\n",
            "66.5% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "69.0% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Match Pyramid + Joint Training ['<ul>', '<li>', '<p>', '<pre>']\n",
            "70.3% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "73.8% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "74.3% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Joint training ['<ul>', '<li>', '<p>', '<pre>']\n",
            "set data_1 = 微众银行智能客服 (size: 100000) ['<ul>', '<li>', '<p>', '<pre>']\n",
            "set data_2 = 蚂蚁金融语义相似度 (size: 34334) ['<ul>', '<li>', '<p>', '<pre>']\n",
            "joint training (size: 100000 + 34334 = 134334) ['<ul>', '<li>', '<p>', '<pre>']\n",
            "train by data_1 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "train by data_2 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "joint train ['<ul>', '<li>', '<p>', '<pre>']\n",
            "joint train + ['<ul>', '<li>', '<p>', '<pre>']\n",
            "data_1 accuracy ['<ul>', '<li>', '<p>', '<pre>']\n",
            "84.4% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "85.0% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "data_2 accuracy ['<ul>', '<li>', '<p>', '<pre>']\n",
            "74.0% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "74.9% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "└── finch/tensorflow2/spoken_language_understanding/atis \t│ \t├── data \t│   └── glove.840B.300d.txt           # pretrained embedding, download and put here \t│   └── make_data.ipynb               # step 1. run this to generate vocab: word.txt, intent.txt, slot.txt  \t│   └── atis.train.w-intent.iob       # incomplete sample, format &lt;text, slot, intent&gt; \t│   └── atis.test.w-intent.iob        # incomplete sample, format &lt;text, slot, intent&gt; \t│ \t├── vocab \t│   └── word.txt                      # list of words in vocabulary \t│   └── intent.txt                    # list of intents in vocabulary \t│   └── slot.txt                      # list of slots in vocabulary \t│\t \t└── main               \t\t└── bigru_clr.ipynb               # step 2. train and evaluate model \t\t└── ... ['<pre>']\n",
            "Training Data: 4978, Testing Data: 893 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Helper ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Intent Accuracy ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Slot Micro-F1 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "97.4% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "95.4% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "+ CRF ['<ul>', '<li>', '<p>', '<pre>']\n",
            "97.2% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "95.8% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "96.5% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "95.5% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Transformer ['<ul>', '<li>', '<p>', '<pre>']\n",
            "95.6% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "97.5% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Bi-GRU ['<ul>', '<li>', '<p>', '<pre>']\n",
            "TF1 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "96.1% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "+ ELMO + CRF ['<ul>', '<li>', '<p>', '<pre>']\n",
            "97.3% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "96.3% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Task: Build a chatbot answering fundamental questions ['<ul>', '<li>', '<p>']\n",
            "Engine ['<ul>', '<li>', '<p>']\n",
            "Encoder ['<ul>', '<li>', '<p>']\n",
            "Vector Type ['<ul>', '<li>', '<p>']\n",
            "Unit Test Accuracy ['<ul>', '<li>', '<p>']\n",
            "Elastic Search ['<ul>', '<li>', '<p>']\n",
            "Default (TF-IDF) ['<ul>', '<li>', '<p>']\n",
            "Sparse ['<ul>', '<li>', '<p>']\n",
            "80% ['<ul>', '<li>', '<p>']\n",
            "Default (TF-IDF) + ['<ul>', '<li>', '<p>']\n",
            "90% ['<ul>', '<li>', '<p>']\n",
            "Dense ['<ul>', '<li>', '<p>']\n",
            "100% ['<ul>', '<li>', '<p>']\n",
            "└── finch/tensorflow2/semantic_parsing/tree_slu \t│ \t├── data \t│   └── glove.840B.300d.txt     \t# pretrained embedding, download and put here \t│   └── make_data.ipynb           \t# step 1. run this to generate vocab: word.txt, intent.txt, slot.txt  \t│   └── train.tsv   \t\t  \t# incomplete sample, format &lt;text, tokenized_text, tree&gt; \t│   └── test.tsv    \t\t  \t# incomplete sample, format &lt;text, tokenized_text, tree&gt; \t│ \t├── vocab \t│   └── source.txt                \t# list of words in vocabulary for source (of seq2seq) \t│   └── target.txt                \t# list of words in vocabulary for target (of seq2seq) \t│\t \t└── main \t\t└── lstm_seq2seq_tf_addons.ipynb           # step 2. train and evaluate model \t\t└── ...... ['<pre>']\n",
            "Training Data: 31279, Testing Data: 9042 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Testing Exact Match ['<ul>', '<li>', '<p>', '<pre>']\n",
            "74.1% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "LSTM Seq2Seq ['<details>', '<ol>', '<li>']\n",
            "80.4% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "GRU Pointer-Generator + Char Embedding ['<ul>', '<li>', '<p>', '<pre>']\n",
            "80.7% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "The Exact Match result is higher than ['<ul>', '<li>', '<p>', '<pre>']\n",
            "└── finch/tensorflow2/knowledge_graph_completion/wn18 \t│ \t├── data \t│   └── download_data.ipynb       \t# step 1. run this to download wn18 dataset \t│   └── make_data.ipynb           \t# step 2. run this to generate vocabulary: entity.txt, relation.txt \t│   └── wn18  \t\t          \t# wn18 folder (will be auto created by download_data.ipynb) \t│   \t└── train.txt  \t\t  \t# incomplete sample, format &lt;entity1, relation, entity2&gt; separated by \\t \t│   \t└── valid.txt  \t\t  \t# incomplete sample, format &lt;entity1, relation, entity2&gt; separated by \\t  \t│   \t└── test.txt   \t\t  \t# incomplete sample, format &lt;entity1, relation, entity2&gt; separated by \\t \t│ \t├── vocab \t│   └── entity.txt                  \t# incomplete sample, list of entities in vocabulary \t│   └── relation.txt                \t# incomplete sample, list of relations in vocabulary \t│\t \t└── main               \t\t└── distmult_1-N.ipynb    \t# step 3. train and evaluate model \t\t└── ... ['<pre>']\n",
            "Task: WN18 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Training Data: 141442, Testing Data: 5000 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "We use the idea of  to accelerate evaluation ['<ul>', '<li>', '<p>', '<pre>']\n",
            "MRR ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Hits@10 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Hits@3 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Hits@1 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.938 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.902 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.688 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.885 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.939 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.909 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.853 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.958 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.948 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.925 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Rule-based System（基于规则的系统） ['<ul>', '<li>', '<p>', '<pre>']\n",
            "For example, we want to answer the following questions with car knowledge: ['<ul>', '<li>', '<p>', '<pre>']\n",
            "What is BMW?      \tI want to know about the BMW      \tPlease introduce the BMW to me      \tHow is the BMW?      \tHow is the BMW compared to the Benz? ['<ul>', '<li>', '<p>', '<pre>']\n",
            "└── finch/tensorflow1/question_answering/babi \t│ \t├── data \t│   └── make_data.ipynb           \t\t# step 1. run this to generate vocabulary: word.txt  \t│   └── qa5_three-arg-relations_train.txt       # one complete example of babi dataset \t│   └── qa5_three-arg-relations_test.txt\t# one complete example of babi dataset \t│ \t├── vocab \t│   └── word.txt                  \t\t# complete list of words in vocabulary \t│\t \t└── main               \t\t└── dmn_train.ipynb \t\t└── dmn_serve.ipynb \t\t└── attn_gru_cell.py ['<pre>']\n",
            "Model: ['<ul>', '<li>', '<p>', '<pre>']\n",
            "TensorFlow 1 ['<ul>', '<li>', '<p>']\n",
            "Model: TF-IDF + LDA ['<ul>', '<li>', '<p>']\n",
            "Data: IMDB Movie Reviews ['<ul>', '<li>', '<p>']\n",
            "└── finch/tensorflow1/recommender/movielens \t│ \t├── data \t│   └── make_data.ipynb           \t\t# run this to generate vocabulary \t│ \t├── vocab \t│   └── user_job.txt \t│   └── user_id.txt \t│   └── user_gender.txt \t│   └── user_age.txt \t│   └── movie_types.txt \t│   └── movie_title.txt \t│   └── movie_id.txt \t│\t \t└── main               \t\t└── dnn_softmax.ipynb \t\t└── ...... ['<pre>']\n",
            "Training Data: 900228, Testing Data: 99981, Users: 6000, Movies: 4000, Rating: 1-5 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Scoring ['<ul>', '<li>', '<p>', '<pre>']\n",
            "LR Decay ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Testing MAE ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Sigmoid (Continuous) ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Exponential ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.663 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Cyclical ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.661 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Softmax (Discrete) ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.633 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "0.628 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "The MAE results seem better than the  and ['<ul>', '<li>', '<p>', '<pre>']\n",
            "└── finch/tensorflow1/multi_turn_rewrite/chinese/ \t│ \t├── data \t│   └── make_data.ipynb         # run this to generate vocab, split train &amp; test data, make pretrained embedding \t│   └── corpus.txt\t\t# original data downloaded from external \t│   └── train_pos.txt\t\t# processed positive training data after {make_data.ipynb} \t│   └── train_neg.txt\t\t# processed negative training data after {make_data.ipynb} \t│   └── test_pos.txt\t\t# processed positive testing data after {make_data.ipynb} \t│   └── test_neg.txt\t\t# processed negative testing data after {make_data.ipynb} \t│ \t├── vocab \t│   └── cc.zh.300.vec\t\t# fastText pretrained embedding downloaded from external \t│   └── char.npy\t\t# chinese characters and their embedding values (300 dim)\t \t│   └── char.txt\t\t# list of chinese characters used in this project  \t│\t \t└── main               \t\t└── baseline_lstm_train.ipynb \t\t└── baseline_lstm_predict.ipynb \t\t└── ... ['<pre>']\n",
            "Task: 20k 腾讯 AI 研发数据（Chinese Data） ['<ul>', '<li>', '<p>', '<pre>']\n",
            "data split as: training data (positive): 18986, testing data (positive): 1008 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Training data = 2 * 18986 because of 1:1 Negative Sampling ['<ul>', '<li>', '<p>', '<pre>']\n",
            "There are six incorrect data and we have deleted them ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Model (results can be compared to  with the same dataset) ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Exact Match ['<ul>', '<li>', '<p>', '<pre>']\n",
            "BLEU-1 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "BLEU-2 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "BLEU-4 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "LSTM Seq2Seq + ['<ul>', '<li>', '<p>', '<pre>']\n",
            "56.2% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "94.6 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "89.1 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "78.5 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "GRU Seq2Seq + Dynamic Memory ['<ul>', '<li>', '<p>', '<pre>']\n",
            "95.0 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "89.5 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "78.9 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "GRU ['<ul>', '<li>', '<p>', '<pre>']\n",
            "59.2% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "93.2 ['<p>', '<strong>']\n",
            "87.7 ['<strong>']\n",
            "77.2 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "GRU  + Multi-Attention ['<ul>', '<li>', '<p>', '<pre>']\n",
            "60.2% ['<ul>', '<li>', '<p>', '<pre>']\n",
            "94.2 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "88.7 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "78.3 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Deployment: ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Inference Code ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Environment ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Java ['<ul>', '<li>', '<p>', '<pre>']\n",
            "└── finch/tensorflow1/free_chat/chinese_lccc \t│ \t├── data \t│   └── LCCC-base.json           \t# raw data downloaded from external \t│   └── LCCC-base_test.json         # raw data downloaded from external \t│   └── make_data.ipynb           \t# step 1. run this to generate vocab {char.txt} and data {train.txt &amp; test.txt} \t│   └── train.txt           \t\t# processed text file generated by {make_data.ipynb} \t│   └── test.txt           \t\t\t# processed text file generated by {make_data.ipynb} \t│ \t├── vocab \t│   └── char.txt                \t# list of chars in vocabulary for chinese \t│   └── cc.zh.300.vec\t\t\t# fastText pretrained embedding downloaded from external \t│   └── char.npy\t\t\t# chinese characters and their embedding values (300 dim)\t \t│\t \t└── main \t\t└── lstm_seq2seq_train.ipynb    # step 2. train and evaluate model \t\t└── lstm_seq2seq_infer.ipynb    # step 4. model inference \t\t└── ... ['<pre>']\n",
            "Task: ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Training Data: 5000000 (sampled due to small memory), Testing Data: 19008 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Data ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Test Case ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Perplexity ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Transformer Encoder + LSTM Generator ['<ul>', '<li>', '<p>', '<pre>']\n",
            "42.465 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "LSTM Encoder + LSTM Generator ['<ul>', '<li>', '<p>', '<pre>']\n",
            "41.250 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "LSTM Encoder + LSTM ['<ul>', '<li>', '<p>', '<pre>']\n",
            "36.525 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "If you want to deploy model in Java production ['<ul>', '<li>', '<p>', '<pre>']\n",
            "└── FreeChatInference  \t│  \t├── data  \t│   └── transformer_export/  \t│   └── char.txt  \t│   └── libtensorflow-1.14.0.jar  \t│   └── tensorflow_jni.dll  \t│  \t└── src                \t\t└── ModelInference.java ['<ul>', '<li>', '<p>', '<pre>']\n",
            "If you don't know the input and output node names in Java, you can display the node names: ['<ul>', '<li>', '<p>', '<pre>']\n",
            "!saved_model_cli show --dir ../model/xxx/1587959473/ --tag_set serve --signature_def serving_default ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Large Pre-trained ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Author ['<ul>', '<li>', '<p>', '<pre>']\n",
            "GPT LCCC base ['<ul>', '<li>', '<p>', '<pre>']\n",
            "GPT LCCC large ['<ul>', '<li>', '<p>', '<pre>']\n",
            "NEZHA LM ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Diverse Response Comparsion ['<ul>', '<li>', '<p>', '<pre>']\n",
            "问题 / 模型回复 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Our RNN ['<ul>', '<li>', '<p>', '<pre>']\n",
            "GPT base ['<ul>', '<li>', '<p>', '<pre>']\n",
            "GPT large ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Nezha ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 你好 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你好 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你好，你好，我是大好人。 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 您好 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 早上好 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 早上好 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 早上好，好久不见 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 早上好！ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 早啊早啊 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 晚上好 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 晚上好 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 好久不见 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 晚上好！ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 再见 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 再见 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 不见 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你是不是不爱我了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我要走了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 好久不见 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 是呀，好久不见。 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 是啊，好久不见。 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 是啊，最近怎么样啊 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 想死你了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也是 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也想你， ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我不信 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 哈哈哈哈哈 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 谢谢你 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 不客气 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 不用谢 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 不用 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 不谢不谢， ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 爱你 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 爱你 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你是爱我 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也爱你， ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也爱你，你最美 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 你叫什么名字 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你猜 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也想知道 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 叫你 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你是不是想说我的名字太俗了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 你几岁了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 问女孩子年龄可不是绅士所为哦 ~ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我不知道 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 现在几点了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 11点 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你说呢？ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 十二点 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也不知道啊 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 今天天气怎么样 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 下雨了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 今天很冷，但是我还是觉得很冷 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 很好 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 不错 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 我们现在在哪里 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 在这里 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我在这里等你呢 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我不知道你在说什么 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 在我们宿舍楼下。。。 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 你能给我讲个笑话吗 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 不能 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 讲个笑话 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 讲给谁听 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 你是男孩还是女孩呀 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我是萌妹纸 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你猜呀 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 你会几种语言呀 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 不会 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你猜呀~ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 只会说一句 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 你能陪我玩吗 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 可以啊 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 可以 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我可以啊 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 说话可以大声一点吗 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 可以的 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 不要 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我不 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 天气真好 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 是的 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 是啊！ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 天气好是因为有你 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 是啊。 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 天气太糟糕了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 是啊 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 下雨了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你们那边也下雨了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 是呀 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 雨好大 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 今天下雨了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 是的，好大雨 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 对呀，我们还穿短袖 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 我讨厌艳阳天 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也讨厌艳阳天 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也是。 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我喜欢阴天 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我喜欢 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 好晒啊 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 是啊！！ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 是啊，不是很热的 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 今天好冷 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 多穿点衣服 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你们都不来 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你那儿也冷了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 今天好热 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 今天热死了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 热死我了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 对啊 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 对啊，我都热疯 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 风好大 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 风太大了。。。 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你不是在上课嘛 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 嗯嗯嗯，风好大，冷 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 雾太大了看不清路 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 雾太大了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 哈哈，你这是在逗我笑 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也看不清 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你是不是傻啊 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 打雷了好可怕 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 可怕 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我们这边也是，我们这边下雨了，我们这边也没有下雨。 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我们这边下雨了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 下雪了诶 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 下雪了？ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 是滴！ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 嗯嗯嗯！！好大的雪！！ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 下了一天了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 好烦啊 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也好烦 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我好烦 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 好开心 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 开心就好 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你是不是想我了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 哈哈哈，我也是！ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 太激动了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 激动什么 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也激动 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 哈哈，还是不要了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 哈哈 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 我好难过 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 怎么了？ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也是，我们都是 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 不要难过，我陪你 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你怎么难过 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 我想哭 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 抱抱 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 抱紧你 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我不想哭！你不要想太多！ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 哭吧哭吧，我陪你哭 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 太好笑了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 笑死我了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你也笑了？ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也觉得 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 我好伤心 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 怎么了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你伤心什么 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你是不是想笑死我！！下次再给你拍 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 心好痛 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你心痛什么 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 心碎一地了。 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 好累啊 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 加油 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你不会还是一个人在寝室吧 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 好好休息 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 累了就睡觉 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 我好疲惫 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也差不多。。 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我好困 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你不是一直都很疲惫么 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 我爱你 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也爱你 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我们都爱你 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 爱我就来见面 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 我讨厌你 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也讨厌你 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你不是一直很喜欢我 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 不是我的 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 你真是太棒啦 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 谢谢 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你最棒 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你也棒棒哒 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 谢谢！ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 你好厉害啊 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 是不是 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你也不差呀 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你也可以的 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 吓死我了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我都不敢出门了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 哈哈哈哈哈！！！ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 哈哈哈哈哈笑死我了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 我想回家 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也想回家 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也想！！ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 想家的时候回来，想家的时候离开。 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 回来吧，家真的好啊！ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 我想爸妈了 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也想爸妈 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也想 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 想我吗 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 不知道小孩在家有没有听话 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也不知道 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 没有 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 听话的话肯定是会听话的。 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我也是听不懂啊 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Q: 想回家撸猫 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 你也想啊？ ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 我们这也有一个 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "A: 回呀回呀 ['<ul>', '<li>', '<p>', '<pre>']\n",
            "Create a decent standalone speech recognition for Linux etc. Some people say we have the models but not enough training data. We disagree: There is plenty of training data (100GB  and 21GB  , synthetic Text to Speech snippets, Movies with transcripts, Gutenberg, YouTube with captions etc etc) we just need a simple yet powerful model. It's only a question of time... ['<p>']\n",
            "git clone https://github.com/pannous/tensorflow-speech-recognition cd tensorflow-speech-recognition git clone https://github.com/pannous/layer.git git clone https://github.com/pannous/tensorpeers.git ['<pre>']\n",
            "git clone  https://git.assembla.com/portaudio.git ./configure --prefix=/path/to/your/local make make install export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/your/local/lib export LIDRARY_PATH=$LIBRARY_PATH:/path/to/your/local/lib export CPATH=$CPATH:/path/to/your/local/include source ~/.bashrc ['<pre>']\n",
            "Toy examples: ['<p>', '<code>']\n",
            "./number_classifier_tflearn.py ['<p>', '<code>']\n",
            "./speaker_classifier_tflearn.py ['<p>', '<code>']\n",
            "Later: ['<p>', '<code>']\n",
            "./train.sh ['<p>', '<code>']\n",
            "./record.py ['<p>', '<code>']\n",
            "Watch video : ['<ul>', '<li>']\n",
            "Understand and correct the corresponding code: ['<ul>', '<li>']\n",
            "Data Augmentation :  create on-the-fly modulation of the data: increase the speech frequency, add background noise, alter the pitch etc,... ['<ul>', '<li>']\n",
            "see ['<ul>', '<li>']\n",
            "Incremental collaborative snapshots ('') ! ['<ul>', '<li>']\n",
            "Modular graphs/models + persistance ['<ul>', '<li>']\n",
            "Swift for TensorFlow: No boundaries. ['<blockquote>', '<p>']\n",
            "Swift for TensorFlow is a next-generation platform for machine learning, incorporating the latest research across machine learning, compilers, differentiable programming, systems design, and beyond. This is an early-stage project: it is not feature-complete nor production-ready, but it is ready for ['<p>', '<em>']\n",
            "pioneers to try in projects, give feedback, and help shape the future! ['<p>', '<em>']\n",
            "Advanced ML researchers who are limited by current ML frameworks. Swift for TensorFlow's advantages include seamless integration with a modern general-purpose language, allowing for more dynamic and sophisticated models. Fast abstractions can be developed in \"user-space\" (as opposed to in C/C++, aka \"framework-space\"), resulting in modular APIs that can be easily customized. ['<ol>', '<li>', '<p>']\n",
            "ML learners who are just getting started with machine learning. Thanks to Swift's support for quality tooling (e.g. context-aware autocompletion), Swift for TensorFlow can be one of the most productive ways to start learning the fundamentals of machine learning. ['<ol>', '<li>', '<p>']\n",
            "Google Colaboratory: The fastest way to get started is to try out Swift for TensorFlow right in your browser. Just open up , or start from a ! Read more in our . ['<ul>', '<li>', '<p>']\n",
            "Install locally: You can . After installation, you can follow these ['<ul>', '<li>', '<p>']\n",
            "to build and execute a Swift script on your computer. ['<ul>', '<li>', '<p>']\n",
            "Run on GCP: You can spin up a GCE instance using a Swift for TensorFlow ['<ul>', '<li>', '<p>']\n",
            "image, with all drivers and the toolchain pre-installed. Instructions can be found in the ['<ul>', '<li>', '<p>']\n",
            "Compile from source: If you'd like to customize Swift for TensorFlow or contribute back, follow our  on building Swift for TensorFlow from source. ['<ul>', '<li>', '<p>']\n",
            "Tutorial ['<p>']\n",
            "Last Updated ['<p>']\n",
            "March 2019 ['<p>']\n",
            "August 2019 ['<p>']\n",
            "November 2020 ['<p>']\n",
            "December 2019 ['<p>']\n",
            "May 2020 ['<p>']\n",
            "Please join the ['<p>']\n",
            "to hear the latest announcements, get help, and share your thoughts! ['<p>']\n",
            "Swift for TensorFlow is a new way to develop machine learning models. It gives you the power of ['<p>', '<strong>']\n",
            "directly integrated into the ['<p>', '<strong>']\n",
            ". We believe that machine learning paradigms are so important that they deserve ['<p>', '<strong>']\n",
            "first-class language and compiler support. ['<p>', '<strong>']\n",
            "A fundamental primitive in machine learning is gradient-based optimization: computing function derivatives to optimize parameters. With Swift for TensorFlow, you can easily differentiate functions using differential operators like , or differentiate with respect to an entire model by calling method . These differentiation APIs are not just available for Tensor-related concepts—they are generalized for all types that conform to the  protocol, including Float, Double, SIMD vectors, and your own data structures. ['<p>']\n",
            "// Custom differentiable type. ['<div highlight highlight-source-swift\">', '<span pl-c\">', '<span pl-k\">', '<span pl-c1\">']\n",
            "struct Model: Differentiable {     var w: Float     var b: Float     func applied(to input: Float) -&gt; Float {         return w * input + b     } } ['<div highlight highlight-source-swift\">', '<span pl-c\">', '<span pl-k\">', '<span pl-c1\">']\n",
            "// Differentiate using `gradient(at:_:in:)`. ['<div highlight highlight-source-swift\">', '<span pl-c\">', '<span pl-k\">', '<span pl-c1\">']\n",
            "let model = Model(w: 4, b: 3) ['<div highlight highlight-source-swift\">', '<span pl-c\">', '<span pl-k\">', '<span pl-c1\">']\n",
            "let input: Float = 2 ['<div highlight highlight-source-swift\">', '<span pl-c\">', '<span pl-k\">', '<span pl-c1\">']\n",
            "let (𝛁model, 𝛁input) = gradient(at: model, input) { model, input in     model.applied(to: input) } ['<div highlight highlight-source-swift\">', '<span pl-c\">', '<span pl-k\">', '<span pl-c1\">']\n",
            "print(𝛁model) // Model.TangentVector(w: 2.0, b: 1.0) ['<div highlight highlight-source-swift\">', '<span pl-c\">', '<span pl-k\">', '<span pl-c1\">']\n",
            "print(𝛁input) // 4.0 ['<div highlight highlight-source-swift\">', '<span pl-c\">', '<span pl-k\">', '<span pl-c1\">']\n",
            "Beyond derivatives, the Swift for TensorFlow project comes with a sophisticated toolchain to make users more productive. You can run Swift interactively in a Jupyter notebook, and get helpful autocomplete suggestions to help you explore the massive API surface of a modern deep learning library. You can ! ['<p>']\n",
            "Migrating to Swift for TensorFlow is really easy thanks to Swift's powerful Python integration. You can incrementally migrate your Python code over (or continue to use your favorite Python libraries), because you can easily call your favorite Python library with a familiar syntax: ['<p>']\n",
            "import TensorFlow ['<div highlight highlight-source-swift\">', '<span pl-k\">', '<span pl-c\">']\n",
            "import Python ['<div highlight highlight-source-swift\">', '<span pl-k\">', '<span pl-c\">']\n",
            "let np = Python.import(\"numpy\") ['<div highlight highlight-source-swift\">', '<span pl-k\">', '<span pl-c\">']\n",
            "let array = np.arange(100).reshape(10, 10)  // Create a 10x10 numpy array. ['<div highlight highlight-source-swift\">', '<span pl-k\">', '<span pl-c\">']\n",
            "let tensor = Tensor&lt;Float&gt;(numpy: array)  // Seamless integration! ['<div highlight highlight-source-swift\">', '<span pl-k\">', '<span pl-c\">']\n",
            "Beware: the project is moving very quickly, and thus some of these documents are slightly out of date as compared to the current state-of-the-art. ['<blockquote>', '<p>']\n",
            "Document ['<p>']\n",
            "April 2018 ['<p>']\n",
            "Current ['<p>']\n",
            "Outdated ['<p>']\n",
            "The Swift for TensorFlow project builds on top of powerful theoretical foundations. For insight into some of the underlying technologies, check out the following documentation. ['<p>']\n",
            "January 2020 ['<p>']\n",
            "June 2019 ['<p>']\n",
            "October 2018 ['<p>']\n",
            ": high-level API familiar to Keras users. ['<ul>', '<li>']\n",
            "Swift for TensorFlow is no longer a fork of the official Swift language; development was previously done on the tensorflow branch of the ['<blockquote>', '<p>']\n",
            "repository. Language additions were designed to fit with the direction of Swift and are going through the  process. ['<blockquote>', '<p>']\n",
            "is a repository of machine learning models built with Swift for TensorFlow. It intended to provide examples of how to use Swift for TensorFlow, to allow for end-to-end tests of machine learning APIs, and to host model benchmarking infrastructure. ['<p>']\n",
            "is a high-level API for Swift for TensorFlow, modeled after the ['<p>']\n",
            "For questions about general use or feature requests, please send an email to the  or search for relevant issues in the . ['<p>']\n",
            "In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. ['<p>']\n",
            "The Swift for TensorFlow community is guided by our , which we encourage everybody to read before participating. ['<p>']\n",
            "It takes 100ms on a 2015 Titan X to style the MIT Stata Center (1024×680) like Udnie, by Francis Picabia. ['<p align=\"center\">']\n",
            "Spec ['<br/>']\n",
            "Operating System ['<br/>']\n",
            "Windows 10 Home ['<br/>']\n",
            "GPU ['<br/>']\n",
            "Nvidia GTX 2080 TI ['<br/>']\n",
            "CUDA Version ['<br/>']\n",
            "11.0 ['<br/>']\n",
            "Driver Version ['<br/>']\n",
            "445.75 ['<br/>']\n",
            "conda create -n tf-gpu tensorflow-gpu=2.1.0 conda activate tf-gpu conda install jupyterlab jupyter lab ['<pre>']\n",
            "python style.py --style path/to/style/img.jpg \\   --checkpoint-dir checkpoint/path \\   --test path/to/test/img.jpg \\   --test-dir path/to/test/dir \\   --content-weight 1.5e1 \\   --checkpoint-iterations 1000 \\   --batch-size 20 ['<pre>']\n",
            "python evaluate.py --checkpoint path/to/style/model.ckpt \\   --in-path dir/of/test/imgs/ \\   --out-path dir/for/results/ ['<pre>']\n",
            "python transform_video.py --in-path path/to/input/vid.mp4 \\   --checkpoint path/to/style/model.ckpt \\   --out-path out/video.mp4 \\   --device /gpu:0 \\   --batch-size 4 ['<pre>']\n",
            "TensorFlow 0.11.0 ['<ul>', '<li>']\n",
            "Python 2.7.9, Pillow 3.4.2, scipy 0.18.1, numpy 1.11.2 ['<ul>', '<li>']\n",
            "If you want to train (and don't want to wait for 4 months): ['<ul>', '<li>']\n",
            "A decent GPU ['<ul>', '<li>']\n",
            "All the required NVIDIA software to run TF on a GPU (cuda, etc) ['<ul>', '<li>']\n",
            "ffmpeg 3.1.3 if you want to stylize video ['<ul>', '<li>']\n",
            "@misc{engstrom2016faststyletransfer,     author = {Logan Engstrom},     title = {Fast Style Transfer},     year = {2016},     howpublished = {\\url{https://github.com/lengstrom/fast-style-transfer/}},     note = {commit xxxxxxx}   } ['<pre>']\n",
            "This project could not have happened without the advice (and GPU access) given by . ['<ul>', '<li>']\n",
            "The project also borrowed some code from Anish's ['<ul>', '<li>']\n",
            "Some readme/docs formatting was borrowed from Justin Johnson's ['<ul>', '<li>']\n",
            "The image of the Stata Center at the very beginning of the README was taken by ['<ul>', '<li>']\n",
            "Michael Ramos ported this network ['<ul>', '<li>']\n",
            "numpy scipy pillow matplotlib ['<p>']\n",
            "vgg = vgg16.Vgg16() vgg.build(images, train=True, num_classes=num_classes, random_init_fc8=True) ['<pre>']\n",
            "FCN32 ['<ul>', '<li>']\n",
            "FCN16 ['<ul>', '<li>']\n",
            "FCN8 ['<ul>', '<li>']\n",
            "Provide finetuned FCN weights. ['<ul>', '<li>']\n",
            "Provide general training code ['<ul>', '<li>']\n",
            "TensorFLow Basics ['<li>', '<em>']\n",
            "Linear Models ['<em>']\n",
            "Predictive Models ['<em>']\n",
            "Support Vector Machines ['<em>']\n",
            "Convolutional Neural Network ['<em>']\n",
            "Autoencoder ['<em>']\n",
            "Recurrent Neural Network ['<em>']\n",
            "python==3.5.6 ['<ul>', '<li>']\n",
            "tensorflow-gpu==1.10.0 ['<ul>', '<li>']\n",
            "带标签的训练集：labeledTrainData.tsv ['<ul>', '<li>']\n",
            "不带标签的训练集：unlabeledTrainData.tsv ['<ul>', '<li>']\n",
            "测试集：testData.tsv ['<ul>', '<li>']\n",
            "id  电影评论的id ['<ul>', '<li>']\n",
            "review  电影评论的内容 ['<ul>', '<li>']\n",
            "sentiment  情感分类的标签（只有labeledTrainData.tsv数据集中有） ['<ul>', '<li>']\n",
            "去除各种标点符号 ['<ul>', '<li>']\n",
            "生成训练word2vec模型的输入数据 /data/preProcess/wordEmbedding.txt ['<ul>', '<li>']\n",
            "预训练的词向量保存为bin格式 /word2vec/word2Vec.bin ['<ul>', '<li>']\n",
            "textCNN可以看作是一个由三个单层的卷积网络的输出结果进行拼接的融合模型，作者提出了三种大小的卷积核[3, 4, 5]，卷积核的滑动使得其 类似于NLP中的n-grams，因此当你需要更多尺度的n-grams时，你可以选择增加不同大小的卷积核，比如大小为2的卷积核可以代表 2-grams. ['<p>']\n",
            "参数配置类 Config （包括训练参数，模型参数和其他参数） ['<ul>', '<li>']\n",
            "数据预处理类 Dataset （包括生成词汇空间，获得预训练词向量，分割训练集和验证集） ['<ul>', '<li>']\n",
            "textCNN模型类 TextCNN ['<ul>', '<li>']\n",
            "模型训练 ['<ul>', '<li>']\n",
            "char-CNN是一种基于字符级的文本分类器，将所有的文本都用字符表示， ['<p>', '<em>']\n",
            "注意这里的数据预处理时不可以去掉标点符号或者其他的各种符号，最好是保存论文中提出的69种字符，我一开始使用去掉特殊符号的字符后的文本输入到模型中会无法收敛。 此外由于训练数据集比较少，即使论文中最小的网络也无法收敛，此时可以减小模型的复杂度，包括去掉一些卷积层等。 ['<p>', '<em>']\n",
            "Bi-LSTM + Attention 就是在Bi-LSTM的模型上加入Attention层，在Bi-LSTM中我们会用最后一个时序的输出向量 作为特征向量，然后进行softmax分类。Attention是先计算每个时序的权重，然后将所有时序 的向量进行加权和作为特征向量，然后进行softmax分类。在实验中，加上Attention确实对结果有所提升。 ['<p>']\n",
            "利用Bi-LSTM获得上下文的信息，类似于语言模型 ['<ul>', '<li>']\n",
            "将Bi-LSTM获得的隐层输出和词向量拼接[fwOutput, wordEmbedding, bwOutput] ['<ul>', '<li>']\n",
            "将拼接后的向量非线性映射到低维 ['<ul>', '<li>']\n",
            "向量中的每一个位置的值都取所有时序上的最大值，得到最终的特征向量，该过程类似于max-pool ['<ul>', '<li>']\n",
            "softmax分类 ['<ul>', '<li>']\n",
            "adversarialLSTM的核心思想是通过对word Embedding上添加噪音生成对抗样本，将对抗样本以和原始样本 同样的形式喂给模型，得到一个Adversarial Loss，通过和原始样本的loss相加得到新的损失，通过优化该新 的损失来训练模型，作者认为这种方法能对word embedding加上正则化，避免过拟合。 ['<p>']\n",
            "Transformer模型有两个结构：Encoder和Decoder，在进行文本分类时只需要用到 Encoder结构，Decoder结构是生成式模型，用于自然语言生成的。Transformer的核心结构是 self-Attention机制，具体的介绍见。 ['<p>']\n",
            "BERT模型是基于双向Transformer实现的语言模型，集预训练和下游任务于一个模型中， 因此在使用的时候我们不需要搭建自己的下游任务模型，直接用BERT模型即可，我们将谷歌开源的源码下载 下来放在bert文件夹中，在进行文本分类只需要修改run_classifier.py文件即可，另外我们需要将训练集 和验证集分割后保存在两个不同的文件中，放置在/BERT/data下。然后还需要下载谷歌预训练好的模型放置在 /BERT/modelParams文件夹下，还需要建一个/BERT/output文件夹用来放置训练后的模型文件 ['<p>']\n",
            "python run_classifier.py      --data_dir=$MY_DATASET      --task_name=imdb      --vocab_file=$BERT_BASE_DIR/vocab.txt      --bert_config_file=$BERT_BASE_DIR/bert_config.json      --output_dir=../output/      --do_train=true      --do_eval=true      --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt      --max_seq_length=200      --train_batch_size=16      --learning_rate=5e-5     --num_train_epochs=3.0 ['<p>']\n",
            "Complete list (12 notebooks) ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq using topic modelling, test accuracy 13.22% ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq + Luong Attention using topic modelling, test accuracy 12.39% ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq + Beam Decoder using topic modelling, test accuracy 10.67% ['<details>', '<ol>', '<li>']\n",
            "LSTM Bidirectional + Luong Attention + Beam Decoder using topic modelling, test accuracy 8.29% ['<details>', '<ol>', '<li>']\n",
            "Pointer-Generator + Bahdanau, , test accuracy 15.51% ['<details>', '<ol>', '<li>']\n",
            "Copynet, test accuracy 11.15% ['<details>', '<ol>', '<li>']\n",
            "Pointer-Generator + Luong, , test accuracy 16.51% ['<details>', '<ol>', '<li>']\n",
            "Dilated Seq2Seq, test accuracy 10.88% ['<details>', '<ol>', '<li>']\n",
            "Dilated Seq2Seq + Self Attention, test accuracy 11.54% ['<details>', '<ol>', '<li>']\n",
            "BERT + Dilated CNN Seq2seq, test accuracy 13.5% ['<details>', '<ol>', '<li>']\n",
            "self-attention + Pointer-Generator, test accuracy 4.34% ['<details>', '<ol>', '<li>']\n",
            "Dilated-CNN Seq2seq + Pointer-Generator, test accuracy 5.57% ['<details>', '<ol>', '<li>']\n",
            "Complete list (54 notebooks) ['<details>', '<ol>', '<li>']\n",
            "Basic cell Seq2Seq-manual ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq-manual ['<details>', '<ol>', '<li>']\n",
            "GRU Seq2Seq-manual ['<details>', '<ol>', '<li>']\n",
            "Basic cell Seq2Seq-API Greedy ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq-API Greedy ['<details>', '<ol>', '<li>']\n",
            "GRU Seq2Seq-API Greedy ['<details>', '<ol>', '<li>']\n",
            "Basic cell Bidirectional Seq2Seq-manual ['<details>', '<ol>', '<li>']\n",
            "LSTM Bidirectional Seq2Seq-manual ['<details>', '<ol>', '<li>']\n",
            "GRU Bidirectional Seq2Seq-manual ['<details>', '<ol>', '<li>']\n",
            "Basic cell Bidirectional Seq2Seq-API Greedy ['<details>', '<ol>', '<li>']\n",
            "LSTM Bidirectional Seq2Seq-API Greedy ['<details>', '<ol>', '<li>']\n",
            "GRU Bidirectional Seq2Seq-API Greedy ['<details>', '<ol>', '<li>']\n",
            "Basic cell Seq2Seq-manual + Luong Attention ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq-manual + Luong Attention ['<details>', '<ol>', '<li>']\n",
            "GRU Seq2Seq-manual + Luong Attention ['<details>', '<ol>', '<li>']\n",
            "Basic cell Seq2Seq-manual + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq-manual + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "GRU Seq2Seq-manual + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "LSTM Bidirectional Seq2Seq-manual + Luong Attention ['<details>', '<ol>', '<li>']\n",
            "GRU Bidirectional Seq2Seq-manual + Luong Attention ['<details>', '<ol>', '<li>']\n",
            "LSTM Bidirectional Seq2Seq-manual + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "GRU Bidirectional Seq2Seq-manual + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "LSTM Bidirectional Seq2Seq-manual + backward Bahdanau + forward Luong ['<details>', '<ol>', '<li>']\n",
            "GRU Bidirectional Seq2Seq-manual + backward Bahdanau + forward Luong ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq-API Greedy + Luong Attention ['<details>', '<ol>', '<li>']\n",
            "GRU Seq2Seq-API Greedy + Luong Attention ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq-API Greedy + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "GRU Seq2Seq-API Greedy + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq-API Beam Decoder ['<details>', '<ol>', '<li>']\n",
            "GRU Seq2Seq-API Beam Decoder ['<details>', '<ol>', '<li>']\n",
            "LSTM Bidirectional Seq2Seq-API + Luong Attention + Beam Decoder ['<details>', '<ol>', '<li>']\n",
            "GRU Bidirectional Seq2Seq-API + Luong Attention + Beam Decoder ['<details>', '<ol>', '<li>']\n",
            "LSTM Bidirectional Seq2Seq-API + backward Bahdanau + forward Luong + Stack Bahdanau Luong Attention + Beam Decoder ['<details>', '<ol>', '<li>']\n",
            "GRU Bidirectional Seq2Seq-API + backward Bahdanau + forward Luong + Stack Bahdanau Luong Attention + Beam Decoder ['<details>', '<ol>', '<li>']\n",
            "Bytenet ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq + tf.estimator ['<details>', '<ol>', '<li>']\n",
            "Capsule layers + LSTM Seq2Seq-API Greedy ['<details>', '<ol>', '<li>']\n",
            "Capsule layers + LSTM Seq2Seq-API + Luong Attention + Beam Decoder ['<details>', '<ol>', '<li>']\n",
            "LSTM Bidirectional Seq2Seq-API + backward Bahdanau + forward Luong + Stack Bahdanau Luong Attention + Beam Decoder + Dropout + L2 ['<details>', '<ol>', '<li>']\n",
            "DNC Seq2Seq ['<details>', '<ol>', '<li>']\n",
            "LSTM Bidirectional Seq2Seq-API + Luong Monotic Attention + Beam Decoder ['<details>', '<ol>', '<li>']\n",
            "LSTM Bidirectional Seq2Seq-API + Bahdanau Monotic Attention + Beam Decoder ['<details>', '<ol>', '<li>']\n",
            "End-to-End Memory Network + Basic cell ['<details>', '<ol>', '<li>']\n",
            "End-to-End Memory Network + LSTM cell ['<details>', '<ol>', '<li>']\n",
            "Attention is all you need ['<details>', '<ol>', '<li>']\n",
            "Transformer-XL ['<details>', '<ol>', '<li>']\n",
            "Attention is all you need + Beam Search ['<details>', '<ol>', '<li>']\n",
            "Transformer-XL + LSTM ['<details>', '<ol>', '<li>']\n",
            "GPT-2 + LSTM ['<details>', '<ol>', '<li>']\n",
            "CNN Seq2seq ['<details>', '<ol>', '<li>']\n",
            "Conv-Encoder + LSTM ['<details>', '<ol>', '<li>']\n",
            "Tacotron + Greedy decoder ['<details>', '<ol>', '<li>']\n",
            "Tacotron + Beam decoder ['<details>', '<ol>', '<li>']\n",
            "Google NMT ['<details>', '<ol>', '<li>']\n",
            "Complete list (8 notebooks) ['<details>', '<ol>', '<li>']\n",
            "Bidirectional RNN + CRF + Biaffine, arc accuracy 70.48%, types accuracy 65.18%, root accuracy 66.4% ['<details>', '<ol>', '<li>']\n",
            "Bidirectional RNN + Bahdanau + CRF + Biaffine, arc accuracy 70.82%, types accuracy 65.33%, root accuracy 66.77% ['<details>', '<ol>', '<li>']\n",
            "Bidirectional RNN + Luong + CRF + Biaffine, arc accuracy 71.22%, types accuracy 65.73%, root accuracy 67.23% ['<details>', '<ol>', '<li>']\n",
            "BERT Base + CRF + Biaffine, arc accuracy 64.30%, types accuracy 62.89%, root accuracy 74.19% ['<details>', '<ol>', '<li>']\n",
            "Bidirectional RNN + Biaffine Attention + Cross Entropy, arc accuracy 72.42%, types accuracy 63.53%, root accuracy 68.51% ['<details>', '<ol>', '<li>']\n",
            "BERT Base + Biaffine Attention + Cross Entropy, arc accuracy 72.85%, types accuracy 67.11%, root accuracy 73.93% ['<details>', '<ol>', '<li>']\n",
            "Bidirectional RNN + Stackpointer, arc accuracy 61.88%, types accuracy 48.20%, root accuracy 89.39% ['<details>', '<ol>', '<li>']\n",
            "XLNET Base + Biaffine Attention + Cross Entropy, arc accuracy 74.41%, types accuracy 71.37%, root accuracy 73.17% ['<details>', '<ol>', '<li>']\n",
            "Complete list (9 notebooks) ['<details>', '<ol>', '<li>']\n",
            "Bidirectional RNN + CRF, test accuracy 96% ['<details>', '<ol>', '<li>']\n",
            "Bidirectional RNN + Luong Attention + CRF, test accuracy 93% ['<details>', '<ol>', '<li>']\n",
            "Bidirectional RNN + Bahdanau Attention + CRF, test accuracy 95% ['<details>', '<ol>', '<li>']\n",
            "Char Ngrams + Bidirectional RNN + Bahdanau Attention + CRF, test accuracy 96% ['<details>', '<ol>', '<li>']\n",
            "Char Ngrams + Residual Network + Bahdanau Attention + CRF, test accuracy 69% ['<details>', '<ol>', '<li>']\n",
            "Char Ngrams + Attention is you all Need + CRF, test accuracy 90% ['<details>', '<ol>', '<li>']\n",
            "BERT, test accuracy 99% ['<details>', '<ol>', '<li>']\n",
            "XLNET-Base, test accuracy 99% ['<details>', '<ol>', '<li>']\n",
            "Complete list (4 notebooks) ['<details>', '<ol>', '<li>']\n",
            "LSTM RNN, test accuracy 16.13% ['<details>', '<ol>', '<li>']\n",
            "Dilated-CNN, test accuracy 15.54% ['<details>', '<ol>', '<li>']\n",
            "Multihead Attention, test accuracy 26.33% ['<details>', '<ol>', '<li>']\n",
            "BERT-Base ['<p>', '<code>']\n",
            "Complete list (15 notebooks) ['<details>', '<ol>', '<li>']\n",
            "Character-wise RNN + LSTM ['<details>', '<ol>', '<li>']\n",
            "Character-wise RNN + Beam search ['<details>', '<ol>', '<li>']\n",
            "Character-wise RNN + LSTM + Embedding ['<details>', '<ol>', '<li>']\n",
            "Word-wise RNN + LSTM ['<details>', '<ol>', '<li>']\n",
            "Word-wise RNN + LSTM + Embedding ['<details>', '<ol>', '<li>']\n",
            "Character-wise + Seq2Seq + GRU ['<details>', '<ol>', '<li>']\n",
            "Word-wise + Seq2Seq + GRU ['<details>', '<ol>', '<li>']\n",
            "Character-wise RNN + LSTM + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "Character-wise RNN + LSTM + Luong Attention ['<details>', '<ol>', '<li>']\n",
            "Word-wise + Seq2Seq + GRU + Beam ['<details>', '<ol>', '<li>']\n",
            "Character-wise + Seq2Seq + GRU + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "Word-wise + Seq2Seq + GRU + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "Character-wise Dilated CNN + Beam search ['<details>', '<ol>', '<li>']\n",
            "Transformer + Beam search ['<details>', '<ol>', '<li>']\n",
            "Transformer XL + Beam search ['<details>', '<ol>', '<li>']\n",
            "Complete list (1 notebooks) ['<details>', '<ol>', '<li>']\n",
            "Fast-text Char N-Grams ['<details>', '<ol>', '<li>']\n",
            "Complete list (53 notebooks) ['<details>', '<p>']\n",
            "1.basic-seq2seq 2.lstm-seq2seq 3.gru-seq2seq 4.basic-seq2seq-contrib-greedy 5.lstm-seq2seq-contrib-greedy 6.gru-seq2seq-contrib-greedy 7.basic-birnn-seq2seq 8.lstm-birnn-seq2seq 9.gru-birnn-seq2seq 10.basic-birnn-seq2seq-contrib-greedy 11.lstm-birnn-seq2seq-contrib-greedy 12.gru-birnn-seq2seq-contrib-greedy 13.basic-seq2seq-luong 14.lstm-seq2seq-luong 15.gru-seq2seq-luong 16.basic-seq2seq-bahdanau 17.lstm-seq2seq-bahdanau 18.gru-seq2seq-bahdanau 19.basic-birnn-seq2seq-bahdanau 20.lstm-birnn-seq2seq-bahdanau 21.gru-birnn-seq2seq-bahdanau 22.basic-birnn-seq2seq-luong 23.lstm-birnn-seq2seq-luong 24.gru-birnn-seq2seq-luong 25.lstm-seq2seq-contrib-greedy-luong 26.gru-seq2seq-contrib-greedy-luong 27.lstm-seq2seq-contrib-greedy-bahdanau 28.gru-seq2seq-contrib-greedy-bahdanau 29.lstm-seq2seq-contrib-beam-luong 30.gru-seq2seq-contrib-beam-luong 31.lstm-seq2seq-contrib-beam-bahdanau 32.gru-seq2seq-contrib-beam-bahdanau 33.lstm-birnn-seq2seq-contrib-beam-bahdanau 34.lstm-birnn-seq2seq-contrib-beam-luong 35.gru-birnn-seq2seq-contrib-beam-bahdanau 36.gru-birnn-seq2seq-contrib-beam-luong 37.lstm-birnn-seq2seq-contrib-beam-luongmonotonic 38.gru-birnn-seq2seq-contrib-beam-luongmonotic 39.lstm-birnn-seq2seq-contrib-beam-bahdanaumonotonic 40.gru-birnn-seq2seq-contrib-beam-bahdanaumonotic 41.residual-lstm-seq2seq-greedy-luong 42.residual-gru-seq2seq-greedy-luong 43.residual-lstm-seq2seq-greedy-bahdanau 44.residual-gru-seq2seq-greedy-bahdanau 45.memory-network-lstm-decoder-greedy 46.google-nmt 47.transformer-encoder-transformer-decoder 48.transformer-encoder-lstm-decoder-greedy 49.bertmultilanguage-encoder-bertmultilanguage-decoder 50.bertmultilanguage-encoder-lstm-decoder 51.bertmultilanguage-encoder-transformer-decoder 52.bertenglish-encoder-transformer-decoder 53.transformer-t2t-2gpu ['<details>', '<p>']\n",
            "Complete list (2 notebooks) ['<details>', '<ol>', '<li>']\n",
            "CNN + LSTM RNN, test accuracy 100% ['<details>', '<ol>', '<li>']\n",
            "Im2Latex, test accuracy 100% ['<details>', '<ol>', '<li>']\n",
            "Bidirectional RNN + CRF, test accuracy 92% ['<details>', '<ol>', '<li>']\n",
            "Bidirectional RNN + Luong Attention + CRF, test accuracy 91% ['<details>', '<ol>', '<li>']\n",
            "Bidirectional RNN + Bahdanau Attention + CRF, test accuracy 91% ['<details>', '<ol>', '<li>']\n",
            "Char Ngrams + Bidirectional RNN + Bahdanau Attention + CRF, test accuracy 91% ['<details>', '<ol>', '<li>']\n",
            "Char Ngrams + Residual Network + Bahdanau Attention + CRF, test accuracy 3% ['<details>', '<ol>', '<li>']\n",
            "Char Ngrams + Attention is you all Need + CRF, test accuracy 89% ['<details>', '<ol>', '<li>']\n",
            "End-to-End Memory Network + GRU cell ['<details>', '<ol>', '<li>']\n",
            "Dynamic Memory ['<details>', '<ol>', '<li>']\n",
            "Complete list (11 notebooks) ['<details>', '<ol>', '<li>']\n",
            "Tacotron, , test accuracy 77.09% ['<details>', '<ol>', '<li>']\n",
            "BiRNN LSTM, test accuracy 84.66% ['<details>', '<ol>', '<li>']\n",
            "BiRNN Seq2Seq + Luong Attention + Cross Entropy, test accuracy 87.86% ['<details>', '<ol>', '<li>']\n",
            "BiRNN Seq2Seq + Bahdanau Attention + Cross Entropy, test accuracy 89.28% ['<details>', '<ol>', '<li>']\n",
            "BiRNN Seq2Seq + Bahdanau Attention + CTC, test accuracy 86.35% ['<details>', '<ol>', '<li>']\n",
            "BiRNN Seq2Seq + Luong Attention + CTC, test accuracy 80.30% ['<details>', '<ol>', '<li>']\n",
            "CNN RNN + Bahdanau Attention, test accuracy 80.23% ['<details>', '<ol>', '<li>']\n",
            "Dilated CNN RNN, test accuracy 31.60% ['<details>', '<ol>', '<li>']\n",
            "Wavenet, test accuracy 75.11% ['<details>', '<ol>', '<li>']\n",
            "Deep Speech 2, test accuracy 81.40% ['<details>', '<ol>', '<li>']\n",
            "Wav2Vec Transfer learning BiRNN LSTM, test accuracy 83.24% ['<details>', '<ol>', '<li>']\n",
            "XLNET-Base ['<details>', '<ol>', '<li>']\n",
            "BERT-Base Fast ['<details>', '<ol>', '<li>']\n",
            "BERT-Base accurate ['<details>', '<ol>', '<li>']\n",
            "BERT, ['<details>', '<ol>', '<li>', '<div highlight highlight-source-json\">']\n",
            "{\"exact_match\": 77.57805108798486, \"f1\": 86.18327335287402} ['<details>', '<ol>', '<li>', '<div highlight highlight-source-json\">']\n",
            "Complete list (6 notebooks) ['<details>', '<ol>', '<li>']\n",
            "LSTM + Seq2Seq + Beam ['<details>', '<ol>', '<li>']\n",
            "GRU + Seq2Seq + Beam ['<details>', '<ol>', '<li>']\n",
            "LSTM + BiRNN + Seq2Seq + Beam ['<details>', '<ol>', '<li>']\n",
            "GRU + BiRNN + Seq2Seq + Beam ['<details>', '<ol>', '<li>']\n",
            "DNC + Seq2Seq + Greedy ['<details>', '<ol>', '<li>']\n",
            "BiRNN + Bahdanau + Copynet ['<details>', '<ol>', '<li>']\n",
            "Pretrained Glove ['<details>', '<ol>', '<li>']\n",
            "GRU VAE-seq2seq-beam TF-probability ['<details>', '<ol>', '<li>']\n",
            "LSTM VAE-seq2seq-beam TF-probability ['<details>', '<ol>', '<li>']\n",
            "GRU VAE-seq2seq-beam + Bahdanau Attention TF-probability ['<details>', '<ol>', '<li>']\n",
            "VAE + Deterministic Bahdanau Attention, ['<details>', '<ol>', '<li>']\n",
            "VAE + VAE Bahdanau Attention, ['<details>', '<ol>', '<li>']\n",
            "BERT-Base + Nucleus Sampling ['<details>', '<ol>', '<li>']\n",
            "XLNET-Base + Nucleus Sampling ['<details>', '<ol>', '<li>']\n",
            "Complete list (79 notebooks) ['<details>', '<ol>', '<li>']\n",
            "Basic cell RNN ['<details>', '<ol>', '<li>']\n",
            "Basic cell RNN + Hinge ['<details>', '<ol>', '<li>']\n",
            "Basic cell RNN + Huber ['<details>', '<ol>', '<li>']\n",
            "Basic cell Bidirectional RNN ['<details>', '<ol>', '<li>']\n",
            "Basic cell Bidirectional RNN + Hinge ['<details>', '<ol>', '<li>']\n",
            "Basic cell Bidirectional RNN + Huber ['<details>', '<ol>', '<li>']\n",
            "LSTM cell RNN ['<details>', '<ol>', '<li>']\n",
            "LSTM cell RNN + Hinge ['<details>', '<ol>', '<li>']\n",
            "LSTM cell RNN + Huber ['<details>', '<ol>', '<li>']\n",
            "LSTM cell Bidirectional RNN ['<details>', '<ol>', '<li>']\n",
            "LSTM cell Bidirectional RNN + Huber ['<details>', '<ol>', '<li>']\n",
            "LSTM cell RNN + Dropout + L2 ['<details>', '<ol>', '<li>']\n",
            "GRU cell RNN ['<details>', '<ol>', '<li>']\n",
            "GRU cell RNN + Hinge ['<details>', '<ol>', '<li>']\n",
            "GRU cell RNN + Huber ['<details>', '<ol>', '<li>']\n",
            "GRU cell Bidirectional RNN ['<details>', '<ol>', '<li>']\n",
            "GRU cell Bidirectional RNN + Hinge ['<details>', '<ol>', '<li>']\n",
            "GRU cell Bidirectional RNN + Huber ['<details>', '<ol>', '<li>']\n",
            "LSTM RNN + Conv2D ['<details>', '<ol>', '<li>']\n",
            "K-max Conv1d ['<details>', '<ol>', '<li>']\n",
            "LSTM RNN + Conv1D + Highway ['<details>', '<ol>', '<li>']\n",
            "LSTM RNN + Basic Attention ['<details>', '<ol>', '<li>']\n",
            "LSTM Dilated RNN ['<details>', '<ol>', '<li>']\n",
            "Layer-Norm LSTM cell RNN ['<details>', '<ol>', '<li>']\n",
            "Only Attention Neural Network ['<details>', '<ol>', '<li>']\n",
            "Multihead-Attention Neural Network ['<details>', '<ol>', '<li>']\n",
            "Neural Turing Machine ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq + Luong Attention ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq + Beam Decoder ['<details>', '<ol>', '<li>']\n",
            "LSTM Bidirectional Seq2Seq ['<details>', '<ol>', '<li>']\n",
            "Pointer Net ['<details>', '<ol>', '<li>']\n",
            "LSTM cell RNN + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "LSTM cell RNN + Luong Attention ['<details>', '<ol>', '<li>']\n",
            "LSTM cell RNN + Stack Bahdanau Luong Attention ['<details>', '<ol>', '<li>']\n",
            "LSTM cell Bidirectional RNN + backward Bahdanau + forward Luong ['<details>', '<ol>', '<li>']\n",
            "Fast-slow LSTM ['<details>', '<ol>', '<li>']\n",
            "Siamese Network ['<details>', '<ol>', '<li>']\n",
            "Capsule layers + RNN LSTM ['<details>', '<ol>', '<li>']\n",
            "Capsule layers + LSTM Seq2Seq ['<details>', '<ol>', '<li>']\n",
            "Capsule layers + LSTM Bidirectional Seq2Seq ['<details>', '<ol>', '<li>']\n",
            "Nested LSTM ['<details>', '<ol>', '<li>']\n",
            "LSTM Seq2Seq + Highway ['<details>', '<ol>', '<li>']\n",
            "Triplet loss + LSTM ['<details>', '<ol>', '<li>']\n",
            "DNC (Differentiable Neural Computer) ['<details>', '<ol>', '<li>']\n",
            "ConvLSTM ['<details>', '<ol>', '<li>']\n",
            "Temporal Convd Net ['<details>', '<ol>', '<li>']\n",
            "Batch-all Triplet-loss + LSTM ['<details>', '<ol>', '<li>']\n",
            "Fast-text ['<details>', '<ol>', '<li>']\n",
            "Gated Convolution Network ['<details>', '<ol>', '<li>']\n",
            "Simple Recurrent Unit ['<details>', '<ol>', '<li>']\n",
            "LSTM Hierarchical Attention Network ['<details>', '<ol>', '<li>']\n",
            "Bidirectional Transformers ['<details>', '<ol>', '<li>']\n",
            "Dynamic Memory Network ['<details>', '<ol>', '<li>']\n",
            "Entity Network ['<details>', '<ol>', '<li>']\n",
            "End-to-End Memory Network ['<details>', '<ol>', '<li>']\n",
            "BOW-Chars Deep sparse Network ['<details>', '<ol>', '<li>']\n",
            "Residual Network using Atrous CNN ['<details>', '<ol>', '<li>']\n",
            "Residual Network using Atrous CNN + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "Deep pyramid CNN ['<details>', '<ol>', '<li>']\n",
            "Transfer learning GPT-2 345M ['<details>', '<ol>', '<li>']\n",
            "Quasi-RNN ['<details>', '<ol>', '<li>']\n",
            "Tacotron ['<details>', '<ol>', '<li>']\n",
            "Slice GRU ['<details>', '<ol>', '<li>']\n",
            "Slice GRU + Bahdanau ['<details>', '<ol>', '<li>']\n",
            "Wavenet ['<details>', '<ol>', '<li>']\n",
            "Transfer learning BERT Base ['<details>', '<ol>', '<li>']\n",
            "Transfer learning XL-net Large ['<details>', '<ol>', '<li>']\n",
            "LSTM BiRNN global Max and average pooling ['<details>', '<ol>', '<li>']\n",
            "Transfer learning BERT Base drop 6 layers ['<details>', '<ol>', '<li>']\n",
            "Transfer learning BERT Large drop 12 layers ['<details>', '<ol>', '<li>']\n",
            "Transfer learning XL-net Base ['<details>', '<ol>', '<li>']\n",
            "Transfer learning ALBERT ['<details>', '<ol>', '<li>']\n",
            "Transfer learning ELECTRA Base ['<details>', '<ol>', '<li>']\n",
            "Transfer learning ELECTRA Large ['<details>', '<ol>', '<li>']\n",
            "Complete list (10 notebooks) ['<details>', '<ol>', '<li>']\n",
            "BiRNN + Contrastive loss, test accuracy 73.032% ['<details>', '<ol>', '<li>']\n",
            "BiRNN + Cross entropy, test accuracy 74.265% ['<details>', '<ol>', '<li>']\n",
            "BiRNN + Circle loss, test accuracy 75.857% ['<details>', '<ol>', '<li>']\n",
            "BiRNN + Proxy loss, test accuracy 48.37% ['<details>', '<ol>', '<li>']\n",
            "BERT Base + Cross entropy, test accuracy 91.123% ['<details>', '<ol>', '<li>']\n",
            "BERT Base + Circle loss, test accuracy 89.903% ['<details>', '<ol>', '<li>']\n",
            "ELECTRA Base + Cross entropy, test accuracy 96.317% ['<details>', '<ol>', '<li>']\n",
            "ELECTRA Base + Circle loss, test accuracy 95.603% ['<details>', '<ol>', '<li>']\n",
            "XLNET Base + Cross entropy, test accuracy 93.998% ['<details>', '<ol>', '<li>']\n",
            "XLNET Base + Circle loss, test accuracy 94.033% ['<details>', '<ol>', '<li>']\n",
            "Tacotron, ['<details>', '<ol>', '<li>']\n",
            "CNN Seq2seq + Dilated CNN vocoder ['<details>', '<ol>', '<li>']\n",
            "Seq2Seq + Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "Seq2Seq + Luong Attention ['<details>', '<ol>', '<li>']\n",
            "Dilated CNN + Monothonic Attention + Dilated CNN vocoder ['<details>', '<ol>', '<li>']\n",
            "Dilated CNN + Self Attention + Dilated CNN vocoder ['<details>', '<ol>', '<li>']\n",
            "Deep CNN + Monothonic Attention + Dilated CNN vocoder ['<details>', '<ol>', '<li>']\n",
            "Deep CNN + Self Attention + Dilated CNN vocoder ['<details>', '<ol>', '<li>']\n",
            "TAT-LSTM ['<details>', '<ol>', '<li>']\n",
            "TAV-LSTM ['<details>', '<ol>', '<li>']\n",
            "MTA-LSTM ['<details>', '<ol>', '<li>']\n",
            "Dilated CNN Seq2seq ['<details>', '<ol>', '<li>']\n",
            "Complete list (3 notebooks) ['<details>', '<ol>', '<li>']\n",
            "LDA2Vec ['<details>', '<ol>', '<li>']\n",
            "BERT Attention ['<details>', '<ol>', '<li>']\n",
            "XLNET Attention ['<details>', '<ol>', '<li>']\n",
            "Skip-thought Vector ['<details>', '<ol>', '<li>']\n",
            "Word Vector using CBOW sample softmax ['<details>', '<ol>', '<li>']\n",
            "Word Vector using CBOW noise contrastive estimation ['<details>', '<ol>', '<li>']\n",
            "Word Vector using skipgram sample softmax ['<details>', '<ol>', '<li>']\n",
            "Word Vector using skipgram noise contrastive estimation ['<details>', '<ol>', '<li>']\n",
            "Supervised Embedded ['<details>', '<ol>', '<li>']\n",
            "Triplet-loss + LSTM ['<details>', '<ol>', '<li>']\n",
            "LSTM Auto-Encoder ['<details>', '<ol>', '<li>']\n",
            "Batch-All Triplet-loss LSTM ['<details>', '<ol>', '<li>']\n",
            "ELMO (biLM) ['<details>', '<ol>', '<li>']\n",
            "Triplet-loss + BERT ['<details>', '<ol>', '<li>']\n",
            "Attention heatmap on Bahdanau Attention ['<details>', '<ol>', '<li>']\n",
            "Attention heatmap on Luong Attention ['<details>', '<ol>', '<li>']\n",
            "BERT attention, ['<details>', '<ol>', '<li>']\n",
            "XLNET attention ['<details>', '<ol>', '<li>']\n",
            "Dilated CNN ['<details>', '<ol>', '<li>']\n",
            "Bahdanau ['<details>', '<ol>', '<li>']\n",
            "Luong ['<details>', '<ol>', '<li>']\n",
            "Hierarchical ['<details>', '<ol>', '<li>']\n",
            "Additive ['<details>', '<ol>', '<li>']\n",
            "Soft ['<details>', '<ol>', '<li>']\n",
            "Attention-over-Attention ['<details>', '<ol>', '<li>']\n",
            "Bahdanau API ['<details>', '<ol>', '<li>']\n",
            "Luong API ['<details>', '<ol>', '<li>']\n",
            "Markov chatbot ['<ol>', '<li>']\n",
            "Decomposition summarization (3 notebooks) ['<ol>', '<li>']\n",
            "Code: You'll find code for each chapter inside of the ['<ul>', '<li>']\n",
            "Errata: Errata will be added to the  directory as they are discovered. Send in a pull request if you have errata to report! ['<ul>', '<li>']\n",
            "H=128 ['<p>', '<strong>']\n",
            "H=256 ['<p>', '<strong>']\n",
            "H=512 ['<p>', '<strong>']\n",
            "H=768 ['<p>', '<strong>']\n",
            "L=2 ['<p>', '<strong>']\n",
            "L=4 ['<p>', '<strong>']\n",
            "L=6 ['<p>', '<strong>']\n",
            "L=8 ['<p>', '<strong>']\n",
            "L=10 ['<p>', '<strong>']\n",
            "L=12 ['<p>', '<strong>']\n",
            "Score ['<strong>']\n",
            "CoLA ['<strong>']\n",
            "SST-2 ['<strong>']\n",
            "MRPC ['<strong>']\n",
            "STS-B ['<strong>']\n",
            "QQP ['<strong>']\n",
            "MNLI-m ['<strong>']\n",
            "MNLI-mm ['<strong>']\n",
            "QNLI(v2) ['<strong>']\n",
            "RTE ['<strong>']\n",
            "WNLI ['<strong>']\n",
            "AX ['<strong>']\n",
            "BERT-Tiny ['<strong>']\n",
            "64.2 ['<strong>']\n",
            "0.0 ['<strong>']\n",
            "83.2 ['<strong>']\n",
            "81.1/71.1 ['<strong>']\n",
            "74.3/73.6 ['<strong>']\n",
            "62.2/83.4 ['<strong>']\n",
            "70.2 ['<strong>']\n",
            "70.3 ['<strong>']\n",
            "81.5 ['<strong>']\n",
            "57.2 ['<strong>']\n",
            "62.3 ['<strong>']\n",
            "21.0 ['<strong>']\n",
            "BERT-Mini ['<strong>']\n",
            "65.8 ['<strong>']\n",
            "85.9 ['<strong>']\n",
            "81.1/71.8 ['<strong>']\n",
            "75.4/73.3 ['<strong>']\n",
            "66.4/86.2 ['<strong>']\n",
            "74.8 ['<strong>']\n",
            "74.3 ['<strong>']\n",
            "84.1 ['<strong>']\n",
            "57.9 ['<strong>']\n",
            "BERT-Small ['<strong>']\n",
            "71.2 ['<strong>']\n",
            "27.8 ['<strong>']\n",
            "89.7 ['<strong>']\n",
            "83.4/76.2 ['<strong>']\n",
            "78.8/77.0 ['<strong>']\n",
            "68.1/87.0 ['<strong>']\n",
            "77.6 ['<strong>']\n",
            "77.0 ['<strong>']\n",
            "86.4 ['<strong>']\n",
            "61.8 ['<strong>']\n",
            "28.6 ['<strong>']\n",
            "BERT-Medium ['<strong>']\n",
            "73.5 ['<strong>']\n",
            "38.0 ['<strong>']\n",
            "89.6 ['<strong>']\n",
            "86.6/81.6 ['<strong>']\n",
            "80.4/78.4 ['<strong>']\n",
            "69.6/87.9 ['<strong>']\n",
            "80.0 ['<strong>']\n",
            "79.1 ['<strong>']\n",
            "62.2 ['<strong>']\n",
            "30.5 ['<strong>']\n",
            "batch sizes: 8, 16, 32, 64, 128 ['<ul>', '<li>']\n",
            "learning rates: 3e-4, 1e-4, 5e-5, 3e-5 ['<ul>', '<li>']\n",
            "@article{turc2019,   title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},   author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},   journal={arXiv preprint arXiv:1908.08962v2 },   year={2019} } ['<pre>']\n",
            "The new technique is called Whole Word Masking. In this case, we always mask ['<p>', '<em>']\n",
            "all of the the tokens corresponding to a word at once. The overall masking rate remains the same. ['<p>', '<em>']\n",
            "The training is identical -- we still predict each masked WordPiece token independently. The improvement comes from the fact that the original prediction task was too 'easy' for words that had been split into multiple WordPieces. ['<p>']\n",
            "Pre-trained models with Whole Word Masking are linked below. The data and training were otherwise identical, and the models have identical structure and vocab to the original models. We only include BERT-Large models. When using these models, please make it clear in the paper that you are using the Whole Word Masking variant of BERT-Large. ['<p>']\n",
            ": 24-layer, 1024-hidden, 16-heads, 340M parameters ['<ul>', '<li>']\n",
            "SQUAD 1.1 F1/EM ['<p>']\n",
            "Multi NLI Accuracy ['<p>']\n",
            "BERT-Large, Uncased (Original) ['<p>']\n",
            "91.0/84.3 ['<p>']\n",
            "86.05 ['<p>']\n",
            "BERT-Large, Uncased (Whole Word Masking) ['<p>']\n",
            "92.8/86.7 ['<p>']\n",
            "87.07 ['<p>']\n",
            "BERT-Large, Cased (Original) ['<p>']\n",
            "91.5/84.8 ['<p>']\n",
            "86.09 ['<p>']\n",
            "BERT-Large, Cased (Whole Word Masking) ['<p>']\n",
            "92.9/86.7 ['<p>']\n",
            "86.46 ['<p>']\n",
            "BERT has been uploaded to . See ['<p>', '<code>']\n",
            "run_classifier_with_tfhub.py for an example of how to use the TF Hub module, or run an example in the browser on ['<p>', '<code>']\n",
            "We uploaded a new multilingual model which does not perform any normalization on the input (no lower casing, accent stripping, or Unicode normalization), and additionally inclues Thai and Mongolian. ['<p>']\n",
            ": 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters ['<ul>', '<li>']\n",
            "We released code changes to reproduce our 83% F1 SQuAD 2.0 system, which is currently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of the README for details. ['<p>']\n",
            "NLP researchers from HuggingFace made a ['<p>']\n",
            "which is compatible with our pre-trained checkpoints and is able to reproduce our results. Sosuke Kobayashi also made a ['<p>']\n",
            "(Thanks!) We were not involved in the creation or maintenance of the PyTorch implementation so please direct any questions towards the authors of that repository. ['<p>']\n",
            "(Not recommended, use Multilingual Cased instead): 102 languages, 12-layer, 768-hidden, 12-heads, 110M parameters ['<ul>', '<li>']\n",
            ": Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters ['<ul>', '<li>']\n",
            "We use character-based tokenization for Chinese, and WordPiece tokenization for all other languages. Both models should work out-of-the-box without any code changes. We did update the implementation of BasicTokenizer in ['<p>', '<code>']\n",
            "tokenization.py to support Chinese character tokenization, so please update if you forked it. However, we did not change the tokenization API. ['<p>', '<code>']\n",
            "BERT, or Bidirectional Encoder Representations from ['<p>', '<strong>']\n",
            "Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. ['<p>', '<strong>']\n",
            "Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: ['<p>']\n",
            "To give a few numbers, here are the results on the ['<p>']\n",
            "question answering task: ['<p>']\n",
            "SQuAD v1.1 Leaderboard (Oct 8th 2018) ['<p>', '<strong>']\n",
            "Test EM ['<p>', '<strong>']\n",
            "Test F1 ['<p>', '<strong>']\n",
            "1st Place Ensemble - BERT ['<p>', '<strong>']\n",
            "87.4 ['<p>', '<strong>']\n",
            "2nd Place Ensemble - nlnet ['<p>', '<strong>']\n",
            "86.0 ['<p>', '<strong>']\n",
            "91.7 ['<p>', '<strong>']\n",
            "1st Place Single Model - BERT ['<p>', '<strong>']\n",
            "85.1 ['<p>', '<strong>']\n",
            "91.8 ['<p>', '<strong>']\n",
            "2nd Place Single Model - nlnet ['<p>', '<strong>']\n",
            "83.5 ['<p>', '<strong>']\n",
            "90.1 ['<p>', '<strong>']\n",
            "System ['<p>', '<code>']\n",
            "MultiNLI ['<strong>']\n",
            "Question NLI ['<strong>']\n",
            "SWAG ['<strong>']\n",
            "86.7 ['<strong>']\n",
            "91.1 ['<strong>']\n",
            "86.3 ['<strong>']\n",
            "OpenAI GPT (Prev. SOTA) ['<strong>']\n",
            "82.2 ['<strong>']\n",
            "88.1 ['<strong>']\n",
            "75.0 ['<strong>']\n",
            "If you already know what BERT is and you just want to get started, you can ['<p>']\n",
            "in only a few minutes. ['<p>']\n",
            "BERT is a method of pre-training language representations, meaning that we train a general-purpose \"language understanding\" model on a large text corpus (like Wikipedia), and then use that model for downstream NLP tasks that we care about (like question answering). BERT outperforms previous methods because it is the first unsupervised, deeply bidirectional system for pre-training NLP. ['<p>']\n",
            "Unsupervised means that BERT was trained using only a plain text corpus, which is important because an enormous amount of plain text data is publicly available on the web in many languages. ['<p>']\n",
            "Pre-trained representations can also either be context-free or contextual, and contextual representations can further be unidirectional or ['<p>', '<em>', '<code>']\n",
            "bidirectional. Context-free models such as ['<p>', '<em>', '<code>']\n",
            "generate a single \"word embedding\" representation for each word in the vocabulary, so bank would have the same representation in bank deposit and river bank. Contextual models instead generate a representation of each word that is based on the other words in the sentence. ['<p>', '<em>', '<code>']\n",
            "BERT was built upon recent work in pre-training contextual representations — including , ['<p>', '<em>', '<code>']\n",
            "— but crucially these models are all unidirectional or shallowly bidirectional. This means that each word is only contextualized using the words to its left (or right). For example, in the sentence I made a bank deposit the unidirectional representation of bank is only based on I made a but not ['<p>', '<em>', '<code>']\n",
            "deposit. Some previous work does combine the representations from separate left-context and right-context models, but only in a \"shallow\" manner. BERT represents \"bank\" using both its left and right context — I made a ... deposit — starting from the very bottom of a deep neural network, so it is deeply bidirectional. ['<p>', '<em>', '<code>']\n",
            "BERT uses a simple approach for this: We mask out 15% of the words in the input, run the entire sequence through a deep bidirectional ['<p>']\n",
            "encoder, and then predict only the masked words. For example: ['<p>']\n",
            "Input: the man went to the [MASK1] . he bought a [MASK2] of milk. Labels: [MASK1] = store; [MASK2] = gallon ['<pre>']\n",
            "In order to learn relationships between sentences, we also train on a simple task which can be generated from any monolingual corpus: Given two sentences A and B, is B the actual next sentence that comes after A, or just a random sentence from the corpus? ['<p>']\n",
            "Sentence A: the man went to the store . Sentence B: he bought a gallon of milk . Label: IsNextSentence ['<pre>']\n",
            "Sentence A: the man went to the store . Sentence B: penguins are flightless . Label: NotNextSentence ['<pre>']\n",
            "We then train a large model (12-layer to 24-layer Transformer) on a large corpus (Wikipedia + ) for a long time (1M update steps), and that's BERT. ['<p>']\n",
            "Pre-training is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a one-time procedure for each language (current models are English-only, but multilingual models will be released in the near future). We are releasing a number of pre-trained models from the paper which were pre-trained at Google. Most NLP researchers will never need to pre-train their own model from scratch. ['<p>']\n",
            "Fine-tuning is inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model. SQuAD, for example, can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%, which is the single system state-of-the-art. ['<p>']\n",
            "The other important aspect of BERT is that it can be adapted to many types of NLP tasks very easily. In the paper, we demonstrate state-of-the-art results on sentence-level (e.g., SST-2), sentence-pair-level (e.g., MultiNLI), word-level (e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specific modifications. ['<p>']\n",
            "TensorFlow code for the BERT model architecture (which is mostly a standard ['<ul>', '<li>', '<code>']\n",
            "architecture). ['<ul>', '<li>', '<code>']\n",
            "Pre-trained checkpoints for both the lowercase and cased version of ['<ul>', '<li>', '<code>']\n",
            "BERT-Base and BERT-Large from the paper. ['<ul>', '<li>', '<code>']\n",
            "TensorFlow code for push-button replication of the most important fine-tuning experiments from the paper, including SQuAD, MultiNLI, and MRPC. ['<ul>', '<li>', '<code>']\n",
            "We are releasing the BERT-Base and BERT-Large models from the paper. ['<p>', '<code>']\n",
            "Uncased means that the text has been lowercased before WordPiece tokenization, e.g., John Smith becomes john smith. The Uncased model also strips out any accent markers. Cased means that the true case and accent markers are preserved. Typically, the Uncased model is better unless you know that case information is important for your task (e.g., Named Entity Recognition or Part-of-Speech tagging). ['<p>', '<code>']\n",
            "When using a cased model, make sure to pass --do_lower=False to the training scripts. (Or pass do_lower_case=False directly to FullTokenizer if you're using your own script.) ['<p>']\n",
            ": 12-layer, 768-hidden, 12-heads, 110M parameters ['<ul>', '<li>']\n",
            ": 12-layer, 768-hidden, 12-heads , 110M parameters ['<ul>', '<li>']\n",
            "A TensorFlow checkpoint (bert_model.ckpt) containing the pre-trained weights (which is actually 3 files). ['<ul>', '<li>']\n",
            "A vocab file (vocab.txt) to map WordPiece to word id. ['<ul>', '<li>']\n",
            "A config file (bert_config.json) which specifies the hyperparameters of the model. ['<ul>', '<li>']\n",
            "Important: All results on the paper were fine-tuned on a single Cloud TPU, which has 64GB of RAM. It is currently not possible to re-produce most of the ['<p>', '<code>']\n",
            "BERT-Large results on the paper using a GPU with 12GB - 16GB of RAM, because the maximum batch size that can fit in memory is too small. We are working on adding code to this repository which allows for much larger effective batch size on the GPU. See the section on  for more details. ['<p>', '<code>']\n",
            "This code was tested with TensorFlow 1.11.0. It was tested with Python2 and Python3 (but more thoroughly with Python2, since this is what's used internally in Google). ['<p>']\n",
            "--use_tpu=True \\   --tpu_name=$TPU_NAME ['<pre>']\n",
            "Please see the ['<p>']\n",
            "for how to use Cloud TPUs. Alternatively, you can use the Google Colab notebook \"\". ['<p>']\n",
            "On Cloud TPUs, the pretrained model and the output directory will need to be on Google Cloud Storage. For example, if you have a bucket named some_bucket, you might use the following flags instead: ['<p>']\n",
            "Before running this example you must download the ['<p>', '<code>']\n",
            "by running ['<p>', '<code>']\n",
            "and unpack it to some directory $GLUE_DIR. Next, download the BERT-Base checkpoint and unzip it to some directory $BERT_BASE_DIR. ['<p>', '<code>']\n",
            "This example code fine-tunes BERT-Base on the Microsoft Research Paraphrase Corpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in a few minutes on most GPUs. ['<p>']\n",
            "export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12 ['<div highlight highlight-source-shell\">', '<span pl-k\">', '<span pl-smi\">']\n",
            "export GLUE_DIR=/path/to/glue ['<div highlight highlight-source-shell\">', '<span pl-k\">', '<span pl-smi\">']\n",
            "python run_classifier.py \\   --task_name=MRPC \\   --do_train=true \\   --do_eval=true \\   --data_dir=$GLUE_DIR/MRPC \\   --vocab_file=$BERT_BASE_DIR/vocab.txt \\   --bert_config_file=$BERT_BASE_DIR/bert_config.json \\   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\   --max_seq_length=128 \\   --train_batch_size=32 \\   --learning_rate=2e-5 \\   --num_train_epochs=3.0 \\   --output_dir=/tmp/mrpc_output/ ['<div highlight highlight-source-shell\">', '<span pl-k\">', '<span pl-smi\">']\n",
            "***** Eval results *****   eval_accuracy = 0.845588   eval_loss = 0.505248   global_step = 343   loss = 0.505248 ['<pre>']\n",
            "This means that the Dev set accuracy was 84.55%. Small sets like MRPC have a high variance in the Dev set accuracy, even when starting from the same pre-training checkpoint. If you re-run multiple times (making sure to point to different output_dir), you should see results between 84% and 88%. ['<p>']\n",
            "A few other pre-trained models are implemented off-the-shelf in ['<p>', '<code>']\n",
            "run_classifier.py, so it should be straightforward to follow those examples to use BERT for any single-sentence or sentence-pair classification task. ['<p>', '<code>']\n",
            "Once you have trained your classifier you can use it in inference mode by using the --do_predict=true command. You need to have a file named test.tsv in the input folder. Output will be created in file called test_results.tsv in the output folder. Each line will contain output for each sample, columns are the class probabilities. ['<p>']\n",
            "export TRAINED_CLASSIFIER=/path/to/fine/tuned/classifier ['<div highlight highlight-source-shell\">', '<span pl-k\">', '<span pl-smi\">']\n",
            "python run_classifier.py \\   --task_name=MRPC \\   --do_predict=true \\   --data_dir=$GLUE_DIR/MRPC \\   --vocab_file=$BERT_BASE_DIR/vocab.txt \\   --bert_config_file=$BERT_BASE_DIR/bert_config.json \\   --init_checkpoint=$TRAINED_CLASSIFIER \\   --max_seq_length=128 \\   --output_dir=/tmp/mrpc_output/ ['<div highlight highlight-source-shell\">', '<span pl-k\">', '<span pl-smi\">']\n",
            "The Stanford Question Answering Dataset (SQuAD) is a popular question answering benchmark dataset. BERT (at the time of the release) obtains state-of-the-art results on SQuAD with almost no task-specific network architecture modifications or data augmentation. However, it does require semi-complex data pre-processing and post-processing to deal with (a) the variable-length nature of SQuAD context paragraphs, and (b) the character-level answer annotations which are used for SQuAD training. This processing is implemented and documented in run_squad.py. ['<p>']\n",
            "To run on SQuAD, you will first need to download the dataset. The ['<p>']\n",
            "does not seem to link to the v1.1 datasets any longer, but the necessary files can be found here: ['<p>']\n",
            "The state-of-the-art SQuAD results from the paper currently cannot be reproduced on a 12GB-16GB GPU due to memory constraints (in fact, even batch size 1 does not seem to fit on a 12GB GPU using BERT-Large). However, a reasonably strong ['<p>', '<code>']\n",
            "BERT-Base model can be trained on the GPU with these hyperparameters: ['<p>', '<code>']\n",
            "python run_squad.py \\   --vocab_file=$BERT_BASE_DIR/vocab.txt \\   --bert_config_file=$BERT_BASE_DIR/bert_config.json \\   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\   --do_train=True \\   --train_file=$SQUAD_DIR/train-v1.1.json \\   --do_predict=True \\   --predict_file=$SQUAD_DIR/dev-v1.1.json \\   --train_batch_size=12 \\   --learning_rate=3e-5 \\   --num_train_epochs=2.0 \\   --max_seq_length=384 \\   --doc_stride=128 \\   --output_dir=/tmp/squad_base/ ['<div highlight highlight-source-shell\">']\n",
            "If you have access to a Cloud TPU, you can train with BERT-Large. Here is a set of hyperparameters (slightly different than the paper) which consistently obtain around 90.5%-91.0% F1 single-system trained only on SQuAD: ['<p>']\n",
            "python run_squad.py \\   --vocab_file=$BERT_LARGE_DIR/vocab.txt \\   --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\   --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\   --do_train=True \\   --train_file=$SQUAD_DIR/train-v1.1.json \\   --do_predict=True \\   --predict_file=$SQUAD_DIR/dev-v1.1.json \\   --train_batch_size=24 \\   --learning_rate=3e-5 \\   --num_train_epochs=2.0 \\   --max_seq_length=384 \\   --doc_stride=128 \\   --output_dir=gs://some_bucket/squad_large/ \\   --use_tpu=True \\   --tpu_name=$TPU_NAME ['<div highlight highlight-source-shell\">']\n",
            "If you fine-tune for one epoch on ['<p>']\n",
            "before this the results will be even better, but you will need to convert TriviaQA into the SQuAD json format. ['<p>']\n",
            "python run_squad.py \\   --vocab_file=$BERT_LARGE_DIR/vocab.txt \\   --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\   --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\   --do_train=True \\   --train_file=$SQUAD_DIR/train-v2.0.json \\   --do_predict=True \\   --predict_file=$SQUAD_DIR/dev-v2.0.json \\   --train_batch_size=24 \\   --learning_rate=3e-5 \\   --num_train_epochs=2.0 \\   --max_seq_length=384 \\   --doc_stride=128 \\   --output_dir=gs://some_bucket/squad_large/ \\   --use_tpu=True \\   --tpu_name=$TPU_NAME \\   --version_2_with_negative=True ['<div highlight highlight-source-shell\">']\n",
            "We assume you have copied everything from the output directory to a local directory called ./squad/. The initial dev set predictions will be at ./squad/predictions.json and the differences between the score of no answer (\"\") and the best non-null answer for each question will be in the file ./squad/null_odds.json ['<p>']\n",
            "Assume the script outputs \"best_f1_thresh\" THRESH. (Typical values are between -1.0 and -5.0). You can now re-run the model to generate predictions with the derived threshold or alternatively you can extract the appropriate answers from ./squad/nbest_predictions.json. ['<p>']\n",
            "python run_squad.py \\   --vocab_file=$BERT_LARGE_DIR/vocab.txt \\   --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\   --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\   --do_train=False \\   --train_file=$SQUAD_DIR/train-v2.0.json \\   --do_predict=True \\   --predict_file=$SQUAD_DIR/dev-v2.0.json \\   --train_batch_size=24 \\   --learning_rate=3e-5 \\   --num_train_epochs=2.0 \\   --max_seq_length=384 \\   --doc_stride=128 \\   --output_dir=gs://some_bucket/squad_large/ \\   --use_tpu=True \\   --tpu_name=$TPU_NAME \\   --version_2_with_negative=True \\   --null_score_diff_threshold=$THRESH ['<div highlight highlight-source-shell\">']\n",
            "All experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB of device RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likely to encounter out-of-memory issues if you use the same hyperparameters described in the paper. ['<p>']\n",
            "max_seq_length: The released models were trained with sequence lengths up to 512, but you can fine-tune with a shorter max sequence length to save substantial memory. This is controlled by the max_seq_length flag in our example code. ['<ul>', '<li>', '<p>']\n",
            "train_batch_size: The memory usage is also directly proportional to the batch size. ['<ul>', '<li>', '<p>']\n",
            "Model type, BERT-Base vs. BERT-Large: The BERT-Large model requires significantly more memory than BERT-Base. ['<ul>', '<li>', '<p>']\n",
            "Optimizer: The default optimizer for BERT is Adam, which requires a lot of extra memory to store the m and v vectors. Switching to a more memory efficient optimizer can reduce memory usage, but can also affect the results. We have not experimented with other optimizers for fine-tuning. ['<ul>', '<li>', '<p>']\n",
            "Using the default training scripts (run_classifier.py and run_squad.py), we benchmarked the maximum batch size on single Titan X GPU (12GB RAM) with TensorFlow 1.11.0: ['<p>']\n",
            "Seq Length ['<p>', '<code>']\n",
            "Max Batch Size ['<p>', '<code>']\n",
            "64 ['<p>', '<code>']\n",
            "320 ['<p>', '<code>']\n",
            "14 ['<p>', '<code>']\n",
            "384 ['<p>', '<code>']\n",
            "512 ['<p>', '<code>']\n",
            "BERT-Large ['<p>', '<code>']\n",
            "Unfortunately, these max batch sizes for BERT-Large are so small that they will actually harm the model accuracy, regardless of the learning rate used. We are working on adding code to this repository which will allow much larger effective batch sizes to be used on the GPU. The code will be based on one (or both) of the following techniques: ['<p>']\n",
            "Gradient accumulation: The samples in a minibatch are typically independent with respect to gradient computation (excluding batch normalization, which is not used here). This means that the gradients of multiple smaller minibatches can be accumulated before performing the weight update, and this will be exactly equivalent to a single larger update. ['<ul>', '<li>', '<p>']\n",
            ": The major use of GPU/TPU memory during DNN training is caching the intermediate activations in the forward pass that are necessary for efficient computation in the backward pass. \"Gradient checkpointing\" trades memory for compute time by re-computing the activations in an intelligent way. ['<ul>', '<li>', '<p>']\n",
            "In certain cases, rather than fine-tuning the entire pre-trained model end-to-end, it can be beneficial to obtained pre-trained contextual embeddings, which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues. ['<p>']\n",
            "# Sentence A and Sentence B are separated by the ||| delimiter for sentence ['<div highlight highlight-source-shell\">', '<span pl-c\">', '<span pl-c1\">', '<span pl-smi\">']\n",
            "# pair tasks like question answering and entailment. ['<div highlight highlight-source-shell\">', '<span pl-c\">', '<span pl-c1\">', '<span pl-smi\">']\n",
            "# For single sentence inputs, put one sentence per line and DON'T use the ['<div highlight highlight-source-shell\">', '<span pl-c\">', '<span pl-c1\">', '<span pl-smi\">']\n",
            "# delimiter. ['<div highlight highlight-source-shell\">', '<span pl-c\">', '<span pl-c1\">', '<span pl-smi\">']\n",
            "echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' &gt; /tmp/input.txt ['<div highlight highlight-source-shell\">', '<span pl-c\">', '<span pl-c1\">', '<span pl-smi\">']\n",
            "python extract_features.py \\   --input_file=/tmp/input.txt \\   --output_file=/tmp/output.jsonl \\   --vocab_file=$BERT_BASE_DIR/vocab.txt \\   --bert_config_file=$BERT_BASE_DIR/bert_config.json \\   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\   --layers=-1,-2,-3,-4 \\   --max_seq_length=128 \\   --batch_size=8 ['<div highlight highlight-source-shell\">', '<span pl-c\">', '<span pl-c1\">', '<span pl-smi\">']\n",
            "This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by layers (-1 is the final hidden layer of the Transformer, etc.) ['<p>']\n",
            "If you need to maintain alignment between the original and tokenized words (for projecting training labels), see the  section below. ['<p>']\n",
            "Note: You may see a message like Could not find trained model in model_dir: /tmp/tmpuB5g5c, running initialization to predict. This message is expected, it just means that we are using the init_from_checkpoint() API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint, this script will complain. ['<p>']\n",
            "For sentence-level tasks (or sentence-pair) tasks, tokenization is very simple. Just follow the example code in run_classifier.py and extract_features.py. The basic procedure for sentence-level tasks is: ['<p>']\n",
            "Instantiate an instance of tokenizer = tokenization.FullTokenizer ['<ol>', '<li>', '<p>']\n",
            "Tokenize the raw text with tokens = tokenizer.tokenize(raw_text). ['<ol>', '<li>', '<p>']\n",
            "Truncate to the maximum sequence length. (You can use up to 512, but you probably want to use shorter if possible for memory and speed reasons.) ['<ol>', '<li>', '<p>']\n",
            "Add the [CLS] and [SEP] tokens in the right place. ['<ol>', '<li>', '<p>']\n",
            "Word-level and span-level tasks (e.g., SQuAD and NER) are more complex, since you need to maintain alignment between your input text and output text so that you can project your training labels. SQuAD is a particularly complex example because the input labels are character-based, and SQuAD paragraphs are often longer than our maximum sequence length. See the code in run_squad.py to show how we handle this. ['<p>']\n",
            "Before we describe the general recipe for handling word-level tasks, it's important to understand what exactly our tokenizer is doing. It has three main steps: ['<p>']\n",
            "Text normalization: Convert all whitespace characters to spaces, and (for the Uncased model) lowercase the input and strip out accent markers. E.g., John Johanson's, → john johanson's,. ['<ol>', '<li>', '<p>', '<code>']\n",
            "Punctuation splitting: Split all punctuation characters on both sides (i.e., add whitespace around all punctuation characters). Punctuation characters are defined as (a) Anything with a P* Unicode class, (b) any non-letter/number/space ASCII character (e.g., characters like $ which are technically not punctuation). E.g., john johanson's, → john johanson ' s , ['<ol>', '<li>', '<p>', '<code>']\n",
            "WordPiece tokenization: Apply whitespace tokenization to the output of the above procedure, and apply ['<ol>', '<li>', '<p>', '<code>']\n",
            "tokenization to each token separately. (Our implementation is directly based on the one from tensor2tensor, which is linked). E.g., john johanson ' s , → john johan ##son ' s , ['<ol>', '<li>', '<p>', '<code>']\n",
            "The advantage of this scheme is that it is \"compatible\" with most existing English tokenizers. For example, imagine that you have a part-of-speech tagging task which looks like this: ['<p>']\n",
            "Input:  John Johanson 's   house Labels: NNP  NNP      POS NN ['<pre>']\n",
            "If you have a pre-tokenized representation with word-level annotations, you can simply tokenize each input word independently, and deterministically maintain an original-to-tokenized alignment: ['<p>']\n",
            "### Input ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "orig_tokens = [\"John\", \"Johanson\", \"'s\",  \"house\"] ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "labels      = [\"NNP\",  \"NNP\",      \"POS\", \"NN\"] ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "### Output ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "bert_tokens = [] ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# Token map will be an int -&gt; int mapping between the `orig_tokens` index and ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# the `bert_tokens` index. ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "orig_to_tok_map = [] ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "tokenizer = tokenization.FullTokenizer(     vocab_file=vocab_file, do_lower_case=True) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "bert_tokens.append(\"[CLS]\") ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "for orig_token in orig_tokens:   orig_to_tok_map.append(len(bert_tokens))   bert_tokens.extend(tokenizer.tokenize(orig_token)) ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "bert_tokens.append(\"[SEP]\") ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# bert_tokens == [\"[CLS]\", \"john\", \"johan\", \"##son\", \"'\", \"s\", \"house\", \"[SEP]\"] ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "# orig_to_tok_map == [1, 2, 4, 6] ['<div highlight highlight-source-python\">', '<span pl-s1\">', '<span pl-c\">', '<span pl-k\">']\n",
            "There are common English tokenization schemes which will cause a slight mismatch between how BERT was pre-trained. For example, if your input tokenization splits off contractions like do n't, this will cause a mismatch. If it is possible to do so, you should pre-process your data to convert these back to raw-looking text, but if it's not possible, this mismatch is likely not a big deal. ['<p>']\n",
            "We are releasing code to do \"masked LM\" and \"next sentence prediction\" on an arbitrary text corpus. Note that this is not the exact code that was used for the paper (the original code was written in C++, and had some additional complexity), but this code does generate pre-training data as described in the paper. ['<p>']\n",
            "Here's how to run the data generation. The input is a plain text file, with one sentence per line. (It is important that these be actual sentences for the \"next sentence prediction\" task). Documents are delimited by empty lines. The output is a set of tf.train.Examples serialized into TFRecord file format. ['<p>']\n",
            "You can perform sentence segmentation with an off-the-shelf NLP toolkit such as ['<p>', '<code>']\n",
            ". The create_pretraining_data.py script will concatenate segments until they reach the maximum sequence length to minimize computational waste from padding (see the script for more details). However, you may want to intentionally add a slight amount of noise to your input data (e.g., randomly truncate 2% of input segments) to make it more robust to non-sentential input during fine-tuning. ['<p>', '<code>']\n",
            "This script stores all of the examples for the entire input file in memory, so for large data files you should shard the input file and call the script multiple times. (You can pass in a file glob to run_pretraining.py, e.g., ['<p>', '<code>']\n",
            "tf_examples.tf_record*.) ['<p>', '<code>']\n",
            "The max_predictions_per_seq is the maximum number of masked LM predictions per sequence. You should set this to around max_seq_length * masked_lm_prob (the script doesn't do that automatically because the exact value needs to be passed to both scripts). ['<p>']\n",
            "python create_pretraining_data.py \\   --input_file=./sample_text.txt \\   --output_file=/tmp/tf_examples.tfrecord \\   --vocab_file=$BERT_BASE_DIR/vocab.txt \\   --do_lower_case=True \\   --max_seq_length=128 \\   --max_predictions_per_seq=20 \\   --masked_lm_prob=0.15 \\   --random_seed=12345 \\   --dupe_factor=5 ['<div highlight highlight-source-shell\">']\n",
            "Here's how to run the pre-training. Do not include init_checkpoint if you are pre-training from scratch. The model configuration (including vocab size) is specified in bert_config_file. This demo code only pre-trains for a small number of steps (20), but in practice you will probably want to set ['<p>', '<code>']\n",
            "num_train_steps to 10000 steps or more. The max_seq_length and ['<p>', '<code>']\n",
            "max_predictions_per_seq parameters passed to run_pretraining.py must be the same as create_pretraining_data.py. ['<p>', '<code>']\n",
            "python run_pretraining.py \\   --input_file=/tmp/tf_examples.tfrecord \\   --output_dir=/tmp/pretraining_output \\   --do_train=True \\   --do_eval=True \\   --bert_config_file=$BERT_BASE_DIR/bert_config.json \\   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\   --train_batch_size=32 \\   --max_seq_length=128 \\   --max_predictions_per_seq=20 \\   --num_train_steps=20 \\   --num_warmup_steps=10 \\   --learning_rate=2e-5 ['<div highlight highlight-source-shell\">']\n",
            "***** Eval results *****   global_step = 20   loss = 0.0979674   masked_lm_accuracy = 0.985479   masked_lm_loss = 0.0979328   next_sentence_accuracy = 1.0   next_sentence_loss = 3.45724e-05 ['<pre>']\n",
            "Note that since our sample_text.txt file is very small, this example training will overfit that data in only a few steps and produce unrealistically high accuracy numbers. ['<p>']\n",
            "If using your own vocabulary, make sure to change vocab_size in ['<ul>', '<li>', '<code>']\n",
            "bert_config.json. If you use a larger vocabulary without changing this, you will likely get NaNs when training on GPU or TPU due to unchecked out-of-bounds access. ['<ul>', '<li>', '<code>']\n",
            "If your task has a large domain-specific corpus available (e.g., \"movie reviews\" or \"scientific papers\"), it will likely be beneficial to run additional steps of pre-training on your corpus, starting from the BERT checkpoint. ['<ul>', '<li>', '<code>']\n",
            "The learning rate we used in the paper was 1e-4. However, if you are doing additional steps of pre-training starting from an existing BERT checkpoint, you should use a smaller learning rate (e.g., 2e-5). ['<ul>', '<li>', '<code>']\n",
            "Current BERT models are English-only, but we do plan to release a multilingual model which has been pre-trained on a lot of languages in the near future (hopefully by the end of November 2018). ['<ul>', '<li>', '<code>']\n",
            "Longer sequences are disproportionately expensive because attention is quadratic to the sequence length. In other words, a batch of 64 sequences of length 512 is much more expensive than a batch of 256 sequences of length 128. The fully-connected/convolutional cost is the same, but the attention cost is far greater for the 512-length sequences. Therefore, one good recipe is to pre-train for, say, 90,000 steps with a sequence length of 128 and then for 10,000 additional steps with a sequence length of 512. The very long sequences are mostly needed to learn positional embeddings, which can be learned fairly quickly. Note that this does require generating the data twice with different values of max_seq_length. ['<ul>', '<li>', '<code>']\n",
            "If you are pre-training from scratch, be prepared that pre-training is computationally expensive, especially on GPUs. If you are pre-training from scratch, our recommended recipe is to pre-train a BERT-Base on a single ['<ul>', '<li>', '<code>']\n",
            ", which takes about 2 weeks at a cost of about $500 USD (based on the pricing in October 2018). You will have to scale down the batch size when only training on a single Cloud TPU, compared to what was used in the paper. It is recommended to use the largest batch size that fits into TPU memory. ['<ul>', '<li>', '<code>']\n",
            "We will not be able to release the pre-processed datasets used in the paper. For Wikipedia, the recommended pre-processing is to download ['<p>']\n",
            ", extract the text with ['<p>']\n",
            ", and then apply any necessary cleanup to convert it into plain text. ['<p>']\n",
            "Unfortunately the researchers who collected the ['<p>']\n",
            "no longer have it available for public download. The ['<p>']\n",
            "is a somewhat smaller (200M word) collection of older books that are public domain. ['<p>']\n",
            "is another very large collection of text, but you will likely have to do substantial pre-processing and cleanup to extract a usable corpus for pre-training BERT. ['<p>']\n",
            "This repository does not include code for learning a new WordPiece vocabulary. The reason is that the code used in the paper was implemented in C++ with dependencies on Google's internal libraries. For English, it is almost always better to just start with our vocabulary and pre-trained models. For learning vocabularies of other languages, there are a number of open source options available. However, keep in mind that these are not compatible with our ['<p>', '<code>']\n",
            "tokenization.py library: ['<p>', '<code>']\n",
            "If you want to use BERT with , you can get started with the notebook \"\". ['<p>', '<strong>']\n",
            "At the time of this writing (October 31st, 2018), Colab users can access a Cloud TPU completely for free. Note: One per user, availability limited, requires a Google Cloud Platform account with storage (although storage may be purchased with free credit for signing up with GCP), and this capability may not longer be available in the future. Click on the BERT Colab that was just linked for more information. ['<p>', '<strong>']\n",
            "There is no official PyTorch implementation. However, NLP researchers from HuggingFace made a ['<p>']\n",
            "which is compatible with our pre-trained checkpoints and is able to reproduce our results. We were not involved in the creation or maintenance of the PyTorch implementation so please direct any questions towards the authors of that repository. ['<p>']\n",
            "There is no official Chainer implementation. However, Sosuke Kobayashi made a ['<p>']\n",
            "which is compatible with our pre-trained checkpoints and is able to reproduce our results. We were not involved in the creation or maintenance of the Chainer implementation so please direct any questions towards the authors of that repository. ['<p>']\n",
            "Yes, we plan to release a multi-lingual BERT model in the near future. We cannot make promises about exactly which languages will be included, but it will likely be a single model which includes most of the languages which have a significantly-sized Wikipedia. ['<p>']\n",
            "So far we have not attempted to train anything larger than BERT-Large. It is possible that we will release larger models if we are able to obtain significant improvements. ['<p>']\n",
            "@article{devlin2018bert,   title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},   author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},   journal={arXiv preprint arXiv:1810.04805},   year={2018} } ['<pre>']\n",
            "For personal communication related to BERT, please contact Jacob Devlin (jacobdevlin@google.com), Ming-Wei Chang (mingweichang@google.com), or Kenton Lee (kentonl@google.com). ['<p>']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}