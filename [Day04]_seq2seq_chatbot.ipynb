{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[Day04] seq2seq_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minnji88/NLP-study/blob/main/%5BDay04%5D_seq2seq_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdYpmUCnX9yt"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s7uNic-sKFN"
      },
      "source": [
        "# 인재양성 실무교육프로그램: Generative Bot based on Seq2Seq model\n",
        "\n",
        "Primary TA: 이영준\n",
        "\n",
        "TA's E-mail: passing2961@gmail.com\n",
        "\n",
        "\n",
        "본 실습에서는 sequence-to-sequence (seq2seq) 모델을 이용하여 일상생활 대화가 가능한 챗봇을 개발한다. 또한, seq2seq 모델을 학습시키기 위해 12,000 쌍으로 이루어진 일상생활 대화 데이터를 사용할 것이다.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "[seq2seq]: https://arxiv.org/abs/1409.3215\n",
        "[Neural Conversational Model]: https://arxiv.org/pdf/1506.05869.pdf\n",
        "\n",
        "Conversational Agent 라고도 불리는 챗봇은 기계가 자연어를 이용해 인간과 소통하기 위한 시스템이다. 대표적으로 잘 알려진 챗봇으로는 Google 의 Google Assistant 과 Apple 의 Siri 가 있다. 이렇듯 기존에 상용화된 챗봇들은 대부분 사용자의 요청하는 일을 처리하기 위함이 가장 큰 목적이다. 예를 들면, 사용자가 \"오늘 날씨를 알려줘?\" 라고 질문하였을 때, 챗봇은 오늘에 해당하는 날씨 정보를 가져와서 사용자에게 \"오늘은 맑아\" 라고 응답한다. 이렇게 상용화된 챗봇들은 주로 응답을 생성할 때, 미리 정의한 규칙이나 템플릿 기반으로 응답을 생성한다. 그렇다 보니, 실제로 사용자 입장에서는 챗봇이 생성하는 응답이 인간이 생성하는 응답처럼 느껴지지 않는다. 즉, 생성되는 응답이 비슷한 구조를 지니고 있게 된다. 그러므로, 챗봇에 대한 사용자의 참여 (engagement)가 떨어지게 되고 사용자에게 불안감 (frustration)을 형성시킬 수 있다. 이러한 이유로, 최근에는 end-to-end 방식의 딥러닝 모델을 이용하여 응답을 생성하는 연구들이 많이 이루어지고 있다. 대표적인 연구로는 Google 의 [Neural Conversational Model] 이 있다. 해당 모델은 인코더-디코더 구조인 [seq2seq 모델]이다. 따라서, 본 실습에서는 PyTorch 버전의 어텐션 기반의 seq2seq 모델을 개발할 것이다.\n",
        "\n",
        "\n",
        "\n",
        "**Types of Chatbot Systems**\n",
        "\n",
        "| Type  | Purpose | Characteristics |\n",
        "| :------------ | :----------- | :------------------- |\n",
        "| Task-oriented dialog systems | To complete specific tasks | Use hard-coded templates and rules |\n",
        "| Non-task or open-domain chatbots | To imitate human conversation | Use the end-to-end deep learning model |\n",
        "\n",
        "![chatbot](https://github.com/passing2961/KEMC/blob/master/chatbot.png?raw=true)\n",
        "\n",
        "\n",
        "**Example**: \n",
        "<pre>\n",
        "<code>\n",
        "문장을 입력하세요: 안녕\n",
        "Bot: 안녕하시어요.\n",
        "문장을 입력하세요: 사랑해\n",
        "Bot: 하늘만큼 땅만큼 사랑하아요.\n",
        "문장을 입력하세요: 배고파\n",
        "Bot: 뭐 좀 챙기어들시어요.\n",
        "</code>\n",
        "</pre>\n",
        "\n",
        "## Key Points\n",
        "\n",
        "- pynori 형태소 분석기를 이용한 한국어 대화 데이터 토크나이징\n",
        "- Luong attention mechanism 기반 seq2seq 모델 구현\n",
        "- seq2seq 모델의 학습 및 평가\n",
        "\n",
        "## Acknowledgement\n",
        "\n",
        "[chatbot 튜토리얼]: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html#\n",
        "[한국어 대화 데이터]: https://github.com/songys/Chatbot_data\n",
        "\n",
        "- 본 실습 코드는 PyTorch 에서 제공하는 [chatbot 튜토리얼]을 참고하였습니다.\n",
        "- seq2seq 모델의 학습을 위해 사용한 [한국어 대화 데이터]를 사용하였습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUzVAkY_sKFP"
      },
      "source": [
        "## Step 0: Connect to Google drive\n",
        "\n",
        "- 학습 도중 checkpoint 를 저장해서 나중에 불러와서 사용하고 싶은 경우에 필요하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "A2k5srlmsKFP",
        "outputId": "e1a5e5a9-ac2c-4410-a410-d1c860c124b5"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL1C8SdmsKFT"
      },
      "source": [
        "## Step 1: Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "UwGXnXUB92BB",
        "outputId": "02b5b2f4-ace6-4da8-b4c3-9b7e953bacaf"
      },
      "source": [
        "# 라이브러리 호출\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "import csv\n",
        "import pickle as pc\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "# reproducibility 를 위함\n",
        "torch.manual_seed(470)\n",
        "torch.cuda.manual_seed(470)\n",
        "\n",
        "# torch 버전 확인\n",
        "print(\"Pytorch Version: \", torch.__version__)\n",
        "\n",
        "# GPU 사용 가능한지 여부 확인\n",
        "if torch.cuda.is_available():\n",
        "    \n",
        "    # PyTorch 에게 GPU 사용할거라고 알려주기\n",
        "    device = torch.device(\"cuda\")\n",
        "    \n",
        "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
        "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"No GPU available, using the CPU instead.\")\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch Version:  1.4.0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtoL6_QfsKFY"
      },
      "source": [
        "## Step 2: Configure the experiments\n",
        "\n",
        "- 모델의 실험(학습 및 평가)을 필요한 파라미터 및 인자 설정\n",
        "    - Hyperparameter: hidden unit size, vocabulary size, max length, dropout rate 등\n",
        "    - Argument: file directory 등"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFnYo08gsKFZ"
      },
      "source": [
        "# 데이터, 모델 위치\n",
        "data_dir = '/content/gdrive/My Drive/Colab Notebooks/data/ChatbotData.csv'\n",
        "\n",
        "dirpath = '/content/gdrive/My Drive/Colab Notebooks/model/'\n",
        "if not os.path.exists(dirpath):\n",
        "    os.makedirs(dirpath)\n",
        "    \n",
        "WORD_DICT_DIR = '/content/gdrive/My Drive/Colab Notebooks/data/word2idx'\n",
        "THRESHOLD = 40000\n",
        "MAX_LEN = 25\n",
        "\n",
        "attn_model = 'dot'\n",
        "enc_hidden_size = 200\n",
        "dec_hidden_size = 400\n",
        "encoder_n_layers = 1\n",
        "decoder_n_layers = 1\n",
        "dropout = 0.1\n",
        "batch_size = 32\n",
        "max_epochs = 10\n",
        "\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.001\n",
        "max_patience = 5\n",
        "best_loss = 100000.0\n",
        "\n",
        "LOG_INTERVAL = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEwn1vkRsKFc"
      },
      "source": [
        "## Step 3: Data preparation\n",
        "\n",
        "모델의 학습을 위해 데이터를 준비하는 단계로써, 다음의 과정을 거친다.\n",
        "    \n",
        "   - Load data\n",
        "   - Tokenization\n",
        "   - Build vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVhCDjx6X9zP"
      },
      "source": [
        "### Step 3-1: Load data\n",
        "\n",
        "- **\"ChatbotData.csv\"** 파일 load\n",
        "- **`[utterance, response]`** pair 의 형태로 재구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "c-MNHEDSsKFd",
        "outputId": "8b00c3b7-cefa-4db9-b889-f87ec4600d65"
      },
      "source": [
        "# pair data load\n",
        "pair_data = list()\n",
        "\n",
        "f = open(data_dir, 'r', encoding='utf-8')\n",
        "reader = csv.reader(f)\n",
        "for idx, line in enumerate(reader):\n",
        "    if idx == 0:\n",
        "        continue\n",
        "        \n",
        "    pair_data.append([line[0], line[1]])\n",
        "f.close()\n",
        "\n",
        "# pair data 확인\n",
        "print(pair_data[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['12시 땡!', '하루가 또 가네요.'], ['1지망 학교 떨어졌어', '위로해 드립니다.'], ['3박4일 놀러가고 싶다', '여행은 언제나 좋죠.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-mz5L-bsKFf"
      },
      "source": [
        "### Step 3-2: Tokenization\n",
        "\n",
        "기존의 `NLTK` 나 `SpaCy` 를 이용하여 **whitespace** 단위로 토큰화를 진행하게 되면, 한국어의 경우에는 성능이 저하됩니다. 이러한 성능 저하를 막기 위해, 한국어의 경우에는 형태소 분석기를 이용하여 토큰화를 진행합니다. 본 실습에서는 `Pynori` 형태소 분석기를 사용합니다.\n",
        "\n",
        "- `KoreanAnalyzer`: argument 에 따른 한국어 형태소 분석기 초기화\n",
        "- `filtering`: 형태소 분석기를 통해 나온 결과에서 **termAtt** & **posTagAtt** 추출\n",
        "\n",
        "[github]: https://github.com/gritmind/python-nori\n",
        "[블로그]: https://gritmind.github.io/2019/05/nori-deep-dive.html\n",
        "\n",
        "Note: `Pynori` 관련 정보는 [github] 과 [블로그]에 자세히 작성되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "XLALIi0QtvXe",
        "outputId": "90ca3aea-53ab-48e8-c116-76d24c9512bf"
      },
      "source": [
        "# pynori 라이브러리 설치\n",
        "!pip install pynori"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pynori in /usr/local/lib/python3.6/dist-packages (0.1.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from pynori) (0.29.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNPpXSkNsKFg"
      },
      "source": [
        "from pynori.korean_analyzer import KoreanAnalyzer\n",
        "\n",
        "nori = KoreanAnalyzer(decompound_mode='DISCARD',\n",
        "                      discard_punctuation=False,\n",
        "                      output_unknown_unigrams=False,\n",
        "                      pos_filter=False,\n",
        "                      stop_tags=['JKS', 'JKB', 'VV', 'EF'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKJBCtJUsKFi"
      },
      "source": [
        "# 형태소 분석기를 통해 나온 결과 filtering\n",
        "def filtering(result):\n",
        "    text = result['termAtt']\n",
        "    morp = result['posTagAtt']\n",
        "    assert len(text) == len(morp)\n",
        "    \n",
        "    temp_list = list()\n",
        "    for i in range(len(text)):\n",
        "        if text[i] == ' ':\n",
        "            continue\n",
        "\n",
        "        temp_str = text[i] + '/' + morp[i]\n",
        "        temp_list.append(temp_str)\n",
        "    return temp_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "BM-jIp9dsKFk",
        "outputId": "319708da-8599-4341-d484-509edcd7c81b"
      },
      "source": [
        "# 형태소 분석된 [질문, 응답] 데이터셋 구축\n",
        "total_data = list()\n",
        "for each in pair_data:\n",
        "    utter_result = nori.do_analysis(each[0])\n",
        "    resp_result = nori.do_analysis(each[1])\n",
        "\n",
        "    utter = filtering(utter_result)\n",
        "    resp = filtering(resp_result)\n",
        "\n",
        "    total_data.append([utter, resp])\n",
        "    \n",
        "# 데이터 사이즈 및 실제 결과 확인\n",
        "print(\"Total size of data is\", len(total_data))\n",
        "print(\"\\nExample:\")\n",
        "print(total_data[:1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total size of data is 11823\n",
            "\n",
            "Example:\n",
            "[[['12/SN', '시/NNBC', '땡/MAG', '!/SF'], ['하루/NNG', '가/JKS', '또/MAG', '가/VV', '네요/EF', './SF']]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdnV9BVfsKFn"
      },
      "source": [
        "### Step 3-3: Data split & shuffling \n",
        "\n",
        "[링크]: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "학습 및 평가를 위해 원본 데이터에서 **90%** 는 학습 데이터로 사용하고, **10%** 는 평가 데이터로 사용합니다. 이 때, `sklearn` 라이브러리에 `train_test_split` 함수를 사용하고, 함수의 argument 정보들은 해당 [링크]로 가시면 확인할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "3zs1uXSSsKFo",
        "outputId": "cc1477ac-5c47-4cdf-dd69-1fd872109b43"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(total_data, test_size=0.1, random_state=42, shuffle=True)\n",
        "\n",
        "# 학습 및 평가 데이터 크기 확인\n",
        "print(\"Train/Test size is {}/{}\".format(len(train), len(test)))\n",
        "print(\"\\nExample:\")\n",
        "print(train[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train/Test size is 10640/1183\n",
            "\n",
            "Example:\n",
            "[[['나/NP', '한테/JKB', '질리/VV', '면/EC', '어쩌/VV', '지/EC', '걱정/NNG', '되/XSV', '어/EC'], ['당신/NP', '의/JKG', '겉/NNG', '모습/NNG', '이/JKC', '아니/VCN', 'ᆫ/ETM', '진정/XR', '하/XSA', 'ᆫ/ETM', '내면/NNG', '의/JKG', '모습/NNG', '을/JKO', '보이/VV', '어/EC', '주/VX', '시/EP', '어요/EF', './SF']], [['새롭/VA', 'ᆫ/ETM', '일/NNG', '벌리/VV', 'ㅓ도/EC', '되/VV', 'ᆯ까/EC'], ['도전/NNG', '하/XSV', '아/EC', '보/VX', '아도/EC', '좋/VA', '을/ETM', '거/NNB', '같/VA', '아요/EF', './SF']]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfGKoqU4sKF2"
      },
      "source": [
        "### Step 3-4: Create Word Dictionary\n",
        "\n",
        "실제 자연어로 이루어진 단어들을 기계가 이해할 수 있는 index 값으로 맵핑해주는 dictionary를 구축합니다. 추가로, 응답을 생성하는 단계를 위해 index 가 단어로 맵핑되는 dictionary 도 구축합니다.\n",
        "\n",
        "- `build_dict`: 학습 데이터의 토큰들을 빈도수 단위로 내림차순 정렬하여, 위에서부터 **threshold** 기준으로 dictionary 갯수/사이즈를 지정\n",
        "    - special tokens\n",
        "        - **pad**: GPU를 이용하여 모델을 학습시키기 위해서는 batch 내에 있는 모든 문장들이 동일한 길이를 가져야한다. 이를 위해, maximum 길이를 지정하고 남은 부분은 padding token 을 채워줍니다. 주의할 점은, 학습시 loss 를 구할 때 padding 에 해당되는 부분은 반영시키지 않아야합니다.\n",
        "        - **unk**: word dictionary 에 없는 단어(token)가 등장하면 unknown token 을 채워줍니다.\n",
        "        - **sos**: 디코더에서 문장의 시작을 알리는 토큰입니다.\n",
        "        - **eos**: 디코더에서 문장의 끝을 알리는 토큰입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP8YkNkJsKF3"
      },
      "source": [
        "def build_dict(data, threshold=40000):\n",
        "    \n",
        "    if not os.path.exists(WORD_DICT_DIR):\n",
        "        \"\"\"\n",
        "        Build word dictionary\n",
        "        \"\"\"\n",
        "        \n",
        "        vocab = list()\n",
        "        for doc in data:\n",
        "            for word in doc[0]:\n",
        "                vocab.append(word)\n",
        "            for word in doc[1]:\n",
        "                vocab.append(word)\n",
        "        \n",
        "        counter = collections.Counter(vocab).most_common(threshold)\n",
        "        \n",
        "        word2idx = dict()\n",
        "        word2idx['<pad>'] = 0\n",
        "        word2idx['<unk>'] = 1\n",
        "        word2idx['<sos>'] = 2\n",
        "        word2idx['<eos>'] = 3\n",
        "        \n",
        "        for word, _ in counter:\n",
        "            word2idx[word] = len(word2idx)\n",
        "        \n",
        "        with open(WORD_DICT_DIR, 'wb') as f:\n",
        "            pc.dump(word2idx, f)\n",
        "    else:\n",
        "        \"\"\"\n",
        "        Load word dictionary which was built before\n",
        "        \"\"\"\n",
        "        with open(WORD_DICT_DIR, 'rb') as f:\n",
        "            word2idx = pc.load(f)\n",
        "    \n",
        "    print(\"Load word dictionary and vocab\")\n",
        "    return word2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "pLhwSg0WsKF6",
        "outputId": "5b4aeabb-ee1a-4db1-c487-ba41d9213d5b"
      },
      "source": [
        "# build_word2idx\n",
        "word2idx = build_dict(train, THRESHOLD)\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "print(\"Size of word2idx is {}\".format(len(word2idx)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load word dictionary and vocab\n",
            "Size of word2idx is 5652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rmp9XJCsKGK"
      },
      "source": [
        "## Step 4: Prepare data for Model\n",
        "\n",
        "모델의 학습을 위해 학습 데이터를 word dictionary 를 이용하여 index 로 변환해줍니다. 그리고 **mini-batch stochastic gradient descent** 를 위해 학습 데이터를 mini-batch 단위로 준비해줍니다.\n",
        "\n",
        "- `batch_iter`: 학습 과정에서 iteration 돌 때마다, 배치 단위의 데이터를 불러오는 과정을 위한 함수\n",
        "- `batch_dataset`: mini-batch 단위 학습 데이터 만들기 위한 함수\n",
        "    - 모델 학습의 efficiency 를 위해 **MAX_LEN** 만큼 데이터 자르기\n",
        "    - 처음 보는 단어는 **unk** 토큰으로 채우기\n",
        "    - utterance 문장의 뒤에 **eos** 토큰 더하기\n",
        "    - response 문장의 앞/뒤에 **sos/eos** 토큰 더하기\n",
        "    - **MAX_LEN** 까지 남은 부분은 **pad** 토큰 채우기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q90Evxu7sKGK"
      },
      "source": [
        "def batch_iter(data, batch_size):\n",
        "    #num_batches_per_epoch = (len(data) - 1) // batch_size + 1\n",
        "    num_batches_per_epoch = int(len(data) / batch_size)\n",
        "    data = np.array(data)\n",
        "    \n",
        "    for batch_idx in range(num_batches_per_epoch):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min((batch_idx + 1) * batch_size, len(data))\n",
        "        enc_data = list()\n",
        "        dec_data = list()\n",
        "        \n",
        "        for each in data[start_idx:end_idx]:\n",
        "            enc_data.append(each[0])\n",
        "            dec_data.append(each[1])\n",
        "\n",
        "        yield enc_data, dec_data\n",
        "        \n",
        "def batch_dataset(batch_x, batch_y, word2idx):\n",
        "    # batch input & target\n",
        "    batch_x = list(map(lambda x: x[:MAX_LEN], batch_x))\n",
        "    batch_y = list(map(lambda x: x[:MAX_LEN], batch_y))\n",
        "\n",
        "    batch_x = list(map(lambda x: [word2idx.get(each, word2idx['<unk>']) for each in x], batch_x))\n",
        "    batch_y = list(map(lambda x: [word2idx.get(each, word2idx['<unk>']) for each in x], batch_y))\n",
        "                        \n",
        "    batch_enc_input = list(map(lambda x: list(x) + [word2idx['<eos>']], batch_x))            \n",
        "    batch_dec_target = list(map(lambda x: [word2idx['<sos>']] + list(x) + [word2idx['<eos>']], batch_y))\n",
        "            \n",
        "    batch_enc_input = list(map(lambda x: list(x) + (MAX_LEN+1 - len(x)) * [word2idx['<pad>']], batch_enc_input))         \n",
        "    batch_dec_target = list(map(lambda x: list(x) + (MAX_LEN+2 - len(x)) * [word2idx['<pad>']], batch_dec_target))\n",
        "    \n",
        "    batch_enc_input = np.array(batch_enc_input)\n",
        "    batch_dec_target = np.array(batch_dec_target)\n",
        "    \n",
        "    return batch_enc_input, batch_dec_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xJRIVVssKGM"
      },
      "source": [
        "## Step 5: Construct the seq2seq model with attention mechanism\n",
        "\n",
        "[Google Neural Machine Translation]: https://arxiv.org/abs/1609.08144\n",
        "\n",
        "인코더-디코더 구조의 가장 대표적인 seq2seq 모델은 두 개의 recurrent neural network 로 구성된 구조로써, 입력 문장에 대해 상응하는 출력 문장을 생성하게 이루어져있습니다. 또한 seq2seq 모델의 경우에는 가변적으로 변하는 문장의 길이를 잘 처리할 수 있다는 장점이 있습니다. 그렇기 때문에, 오늘날 기계 번역 및 대화 생성 등 여러 분야에서 사용되고 있습니다. 아래 그림은 [Google Neural Machine Translation] 모델입니다.\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/gmt.png?raw=True\" width=\"80%\" height=\"60%\" title=\"gmt\" alt=\"gmt\"></img></center>\n",
        "\n",
        "Seq2Seq 모델은 크게 두가지 부분으로 구성되어있습니다. \n",
        "- Encoder: **Sentence representation**\n",
        "    - 입력 문장을 하나의 고정된 차원의 context vector 로 변환\n",
        "    - 각 time step 에서 output 은 context dependent vector representation\n",
        "- Decoder: **Language modeling**\n",
        "    - context vector 가 주어졌을 때, 입력 문장에 적합한 출력 문장을 생성\n",
        "    - Conditional language modeling 이므로, causal structure 유지\n",
        "    \n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/seq2seq.png?raw=True\" width=\"80%\" height=\"60%\" title=\"seq2seq\" alt=\"seq2seq\"></img></center>\n",
        "\n",
        "*이미지 출처: https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#definition*\n",
        "\n",
        "[Bahdanau et al.]: https://arxiv.org/abs/1409.0473\n",
        "\n",
        "그러나, seq2seq 모델의 경우에는 문제점이 존재합니다. 인코더에서 **입력 문장의 문맥적 정보를 하나의 고정된 차원의 벡터에 다 담으려고 (collapsing)** 하다보니, 입력 문장의 길이가 길어지게 되면 **정보 손실 (information loss)** 이 발생합니다. 결국, 해당 문제는 **long-term dependency** 가 보장이 안된다는 것인데, 이를 위해 attention mechanism 이 **[Bahdanau et al.]** 에 의해 처음으로 제안되었습니다. \n",
        "\n",
        "Attention mechanism 은 long-term dependency 도 잘 보장하면서 동시에 디코더에서 다음 단어를 생성할 때 **입력 문장에서 제일 중요하게 반영해야하는 단어**를 반영할 수 있게 해줍니다. 즉, 기존의 seq2seq 에서 하나의 context vector 만 사용하는 것과는 달리, attention mechanism 을 적용하게 되면 디코더의 각 time step 마다 **dynamic 하게 context vector 를 사용할 수 있다는 장점**이 있습니다.\n",
        "\n",
        "[Luong]: https://arxiv.org/abs/1508.04025\n",
        "\n",
        "앞서 말하였지만, **Bahdanau** attention model 을 시작으로 많은 어텐션 모델들이 등장하였습니다. 그 중에, **Bahdanau** 어텐션 모델을 효율적이고 좀 더 simple 하게 만든 **[Luong]** 어텐션 모델을 본 실습에서는 다루겠습니다. (*Transfomer 모델 나오기 전까지 가장 많이 쓰였던 어텐션 모델입니다.*) 그 전에, luong 어텐션 메커니즘이 어떻게 동작이 이루어지는 아래의 그림을 보고 확인해보겠습니다.\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/seq2seq_attn.jpeg?raw=true\" width=\"80%\" height=\"70%\" title=\"seq2seq_attn\" alt=\"seq2seq_attn\"></img></center>\n",
        "\n",
        "*이미지 출처: https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-3-attention-92352bbdcb07*\n",
        "\n",
        "먼저, 디코더에서 현재 time step 의 RNN hidden state 값이랑 인코더에 있는 모든 RNN hidden state 들이랑 dot product (matrix multiplication) 연산을 하여, 디코더의 현재 time step 값의 단어가 인코더의 어느 단어와 제일 관련이 있는지를 구합니다. 이렇게 구해진 벡터를 **attention score** 라고 부릅니다. 여기서 dot product 는 보통 **score function** 이라고 말합니다. 이렇게 score function 을 통해 구한 attention score 벡터 값을 softmax 함수를 통과시켜 attention weight 을 구합니다.\n",
        "(*softmax 함수는 입력이 주어졌을 때, 출력값이 확률이 되는 형태가 되도록 해줍니다.*)\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/softmax.png?raw=true\" width=\"50%\" height=\"40%\" title=\"softmax\" alt=\"softmax\"></img></center>\n",
        "\n",
        "*이미지 출처: https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d*\n",
        "\n",
        "그런 다음, attention weight 과 인코더의 모든 RNN hidden state 들의 값을 weighted sum 연산을 통해 하나의 **context vector** 를 구합니다. 그리고 현재 time step 에서의 단어(토큰)을 예측할 때, 디코더의 현재 time step 에서의 hidden state 값(output)과 구한 context vector 를 결합(concatenation)하여 softmax 함수를 통과시킵니다.\n",
        "\n",
        "대표적인 어텐션 모델인 **Bahdanau** 와 **Luong** 의 계산 과정을 수식을 통해 자세하게 확인해보겠습니다.\n",
        "\n",
        "**Bahdanau attention mechanism**\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/bah_attn.png?raw=true\" width=\"30%\" height=\"20%\" title=\"bah_attn\" alt=\"bah_attn\"></img></center>\n",
        "\n",
        "Bahdanau attention mechanism 계산 과정\n",
        "\n",
        "- score function(additive style)\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/bah_score.png?raw=true\" width=\"30%\" height=\"20%\" title=\"bah_score\" alt=\"bah_score\"></img></center>\n",
        "\n",
        "- attention weights\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/bah_attn_weight.png?raw=true\" width=\"30%\" height=\"20%\" title=\"bah_attn_weight\" alt=\"bah_attn_weight\"></img></center>\n",
        "\n",
        "- context vector\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/bah_context.png?raw=true\" width=\"20%\" height=\"10%\" title=\"bah_context\" alt=\"bah_context\"></img></center>\n",
        "\n",
        "- prediction: a deep-output and a maxout layer\n",
        "\n",
        "\n",
        "\n",
        "**Luong attention mechanism**\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/luong_attn.png?raw=true\" width=\"100%\" height=\"100%\" title=\"luong_attn\" alt=\"luong_attn\"></img></center>\n",
        "\n",
        "Luong attention mechanism (Global) 계산 과정\n",
        "\n",
        "- score function\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/luong_score.png?raw=true\" width=\"70%\" height=\"60%\" title=\"luong_score\" alt=\"luong_score\"></img></center>\n",
        "\n",
        "- attention weights\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/luong_attn_weight.png?raw=true\" width=\"40%\" height=\"30%\" title=\"luong_attn_weight\" alt=\"luong_attn_weight\"></img></center>\n",
        "\n",
        "- context vector\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/luong_context.png?raw=true\" width=\"20%\" height=\"10%\" title=\"luong_context\" alt=\"luong_context\"></img></center>\n",
        "\n",
        "- predictions\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/luong_concat.png?raw=true\" width=\"30%\" height=\"20%\" title=\"luong_concat\" alt=\"luong_concat\"></img></center>\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/luong_softmax.png?raw=true\" width=\"35%\" height=\"25%\" title=\"luong_softmax\" alt=\"luong_softmax\"></img></center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9miahyT192bp"
      },
      "source": [
        "# Encoder Model\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        \n",
        "        # Initialize LSTM with bidirectional\n",
        "        # the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        # because our input size is a word embedding with number of features == hidden_size\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, bidirectional=True, dropout=dropout, batch_first=True)\n",
        "            \n",
        "        ##########################################################################\n",
        "        #### TODO: Initialize GRU with bidirectional                          ####\n",
        "        ##########################################################################\n",
        "\n",
        "        # self.gru = \n",
        "\n",
        "        ##########################################################################\n",
        "\n",
        "    def forward(self, enc_input):\n",
        "        # Convert word indexed to embeddings (mapping discrete tokens to continuous space)\n",
        "        # embedded shape == (batch_size, enc_max_len, embed_size)\n",
        "        embedded = self.embedding(enc_input)\n",
        "       \n",
        "        # Forward pass through RNN module\n",
        "        # if bidirectional, outputs shape == (batch_size, enc_max_len, hidden_size*2)\n",
        "        # if not bidirectional, outputs shape == (batch_size, enc_max_len, hidden_size)\n",
        "        # hidden shape == (num_directions * num_layers, batch_size, hidden_size)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        ##########################################################################\n",
        "        #### TODO: Forward pass through GRU module                            ####\n",
        "        ####       Return value should be changed.                            ####\n",
        "        ##########################################################################\n",
        "\n",
        "        # outputs, hidden = \n",
        "\n",
        "        ##########################################################################\n",
        "\n",
        "        # Sum bidirectional outputs\n",
        "        # shape == (batch_size, enc_max_len, hidden_size)\n",
        "        # outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "        \n",
        "        # output of shape (seq_len, batch, num_directions * hidden_size)\n",
        "        # h of shape (num_layers * num_directions, batch, hidden_size)\n",
        "        # c of shape (num_layers * num_directions, batch, hidden_size)\n",
        "        return outputs, (hidden, cell)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFFfPyraX90I"
      },
      "source": [
        "# Luong attention layer\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.method = method\n",
        "        \n",
        "        # method: indicator that determines the score function \n",
        "\n",
        "        # raise: 프로그래머가 지정한 예외가 발생할 수 있도록 강제함\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        \n",
        "        self.hidden_size = hidden_size   \n",
        "        ##########################################################################\n",
        "        #### TODO: Initialize network for 'general' & 'concat' score function ####\n",
        "        ####                                                                  ####\n",
        "        ##########################################################################     \n",
        "    \n",
        "    def dot_score(self, hidden, encoder_output):        \n",
        "        # attention dot score function (Luong)\n",
        "        # attention score shape == (batch_size, dec_max_len)\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "    \n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        ##########################################################################\n",
        "        #### TODO: Define the 'general' score function                          ####\n",
        "        ####                                                                  ####\n",
        "        ########################################################################## \n",
        "        return\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        ##########################################################################\n",
        "        #### TODO: Define the 'concat' score function                         ####\n",
        "        ####                                                                  ####\n",
        "        ########################################################################## \n",
        "        return\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'dot':\n",
        "            attn_weights = self.dot_score(hidden, encoder_outputs)\n",
        "        \n",
        "        ##########################################################################\n",
        "        #### TODO: Calculate the attention weights                            ####\n",
        "        ####       based on 'general' and 'concat' method                     ####\n",
        "        ########################################################################## \n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_weights, dim=1).unsqueeze(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O41_EgTtX90L"
      },
      "source": [
        "# Decoder Model\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, hidden_size, vocab_size, n_layers=1, dropout=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        \n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # Define layers\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        #self.embedding_dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, bidirectional=False, num_layers=1, batch_first=True)\n",
        "        self.concat = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "\n",
        "        ##########################################################################\n",
        "        #### TODO: Initialize GRU with unidirectional                         ####\n",
        "        ##########################################################################\n",
        "\n",
        "        # self.gru = \n",
        "\n",
        "        ##########################################################################\n",
        "\n",
        "        self.attn = Attention(self.attn_model, self.hidden_size)\n",
        "    \n",
        "    def forward(self, dec_input, hidden, cell, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        \n",
        "        # Get embedding of current input word\n",
        "        # embedded shape == (batch_size, 1, embed_size)\n",
        "        embedded = self.embedding(dec_input)\n",
        "    \n",
        "        # Forward through unidirectional LSTM\n",
        "        # output shape == (batch_size, 1, hidden_size)\n",
        "        # hidden shape == (num_directions * num_layers, batch_size, hidden_size)        \n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        \n",
        "        ##########################################################################\n",
        "        #### TODO: Forward through unidirectional LSTM                        ####\n",
        "        ####       Return values should be changed.                           ####\n",
        "        ##########################################################################\n",
        "\n",
        "        # output, hidden = \n",
        "\n",
        "        ##########################################################################\n",
        "\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        # attention weights shape == (batch_size, 1, dec_max_len)\n",
        "        attn_weights = self.attn(output, encoder_outputs)\n",
        "        \n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        # context vector shape == (batch_size, 1, hidden_size)\n",
        "        context = attn_weights.bmm(encoder_outputs)\n",
        "        \n",
        "        # Concatenate weighted context vector and GRU output using Luong\n",
        "        # output shape == (batch_size, hidden_size)\n",
        "        # context shape == (batch_size, hidden_size)\n",
        "        # concat_input shape == (batch_size, hidden_size * 2)\n",
        "        # concat_output shape == (batch_size, hidden_size)\n",
        "        output = output.squeeze(1)\n",
        "        context = context.squeeze(1)\n",
        "        \n",
        "        concat_input = torch.cat((output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        \n",
        "        # Predict next word using Luong\n",
        "        # output shape == (batch_size, vocab_size)\n",
        "        output = self.out(concat_output)\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        \n",
        "        # Return output and final hidden state\n",
        "        return output, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGdoo9pvsKGT"
      },
      "source": [
        "## Step 5: Define the optimizer and the loss function\n",
        "\n",
        "- optimizer: Adam 사용\n",
        "- loss function: negative log likelihood 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "CvT9E4hVC4RS",
        "outputId": "0f229cf7-8330-43ee-e779-e8f18f6ca5eb"
      },
      "source": [
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(vocab_size, enc_hidden_size, encoder_n_layers, dropout).cuda()\n",
        "decoder = AttnDecoderRNN(attn_model, dec_hidden_size, vocab_size, decoder_n_layers, dropout).cuda()\n",
        "\n",
        "print(\"Models build and ready to go!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Models build and ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoqcWpJ492hp"
      },
      "source": [
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "criterion= nn.NLLLoss(ignore_index=word2idx['<pad>'])\n",
        "#criterion= nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xejqIfmisKGZ"
      },
      "source": [
        "## Step 6: Training loop\n",
        "\n",
        "**The goal of AI**\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/basic1.png?raw=true\" width=\"80%\" height=\"70%\"  title=\"basic1\" alt=\"basic1\"></img></center>\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/basic2.png?raw=true\" width=\"80%\" height=\"70%\"  title=\"basic2\" alt=\"basic2\"></img></center>\n",
        "\n",
        "**Pipeline of training machine learning systems**\n",
        "\n",
        "- **Collect a data**: training/validation/test\n",
        "  - training: 실제 학습 용도\n",
        "  - validation: overfitting 방지 용도\n",
        "  - test: 실제 평가 용도\n",
        "\n",
        "- **Hypothesis set**: Design a network architecture\n",
        "\n",
        "- **Loss function**: Define a loss (objective) function\n",
        "\n",
        "- **optimize loss function**\n",
        "  - Automatic backpropagation: Compute the minibatch gradient\n",
        "  - Optimization: Update the parameters (e.g. Stochastic Gradient Descent)\n",
        "  - Earlystopping: An efficient way to prevent overfitting\n",
        "  - Adaptive learning rate\n",
        "\n",
        "위의 과정을 자세히 살펴보겠습니다.\n",
        "\n",
        "#### **Collect a data**\n",
        "\n",
        "먼저, 학습시키기 위해서는 데이터가 필요합니다. 이를 위해 데이터를 수집합니다. 이 경우에 직접 크롤링을 통해 데이터를 수집할 수도 있지만, 보통은 benchmark 처럼 기존에 표준처럼 사용되는 데이터를 사용합니다. 이렇게 수집한 이후에는 데이터를 train/validation/test 3개의 데이터셋으로 나눕니다. Training dataset 은 실제 모델의 학습을 위해 필요합니다. Valdiation dataset 은 검증 데이터셋으로써, 모델이 학습 도중에 너무 학습 데이터만 기억하게 되는 overfitting (memorization) 이 일어나는 것을 방지하기 위해 필요합니다. Test dataset 은 실제 학습된 모델의 성능을 평가하기 위해 필요합니다.\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/batch_data.png?raw=true\" width=\"80%\" height=\"70%\"  title=\"batch_data\" alt=\"batch_data\"></img></center>\n",
        "\n",
        "- 입력 인풋 형태: (batch_size, max_len)\n",
        "\n",
        "#### **Hypothesis set**\n",
        "\n",
        "데이터를 수집한 이후에는 주어진 task 에 대해서 어떤 모델 (neural network) 를 사용 혹은 고안할지 결정합니다. 예를 들어, 이미지가 고양이인지 강아지인지 분류하는 문제에서는 hypothesis set 은 VGGnet, GoogLeNet, ResNet 등이 포함될 수 있습니다. 즉, hypothesis set 은 무한 (infinite) 합니다.\n",
        "\n",
        "#### **Loss function**\n",
        "\n",
        "어떤 모델을 사용할지 결정하였으면, 해당 모델에서 loss function 을 설계합니다. 예를 들어, classification 문제의 경우에는 hinge loss, log loss 등이 있고, regression 문제의 경우에는 mean squared error (MSE), mean absolute error, robust loss 등이 있습니다. 그러나 대부분의 딥러닝 모델들은 **distribution-based loss function** 을 사용합니다. Distribution-based loss function 의 distribution 은 입력 x 가 주어졌을 때, 출력 y 가 특정 y' 값일 경우가 얼마나 가능한지 (혹은 그럴듯한지) 를 확인합니다. \n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/dist_loss.png?raw=true\" width=\"40%\" height=\"30%\"  title=\"dist_loss\" alt=\"dist_loss\"></img></center>\n",
        "    \n",
        "위의 수식이 distribution-based loss function 의 distribution 에 해당합니다. 그렇다면, 등식의 우변항이 어떤 종류의 문제이냐에 따라 distribution 이 달라집니다.\n",
        "\n",
        "- Binary classification: Bernoulli distribution\n",
        "- Multiclass classification: Categorical distribution\n",
        "- Linear regression: Gaussian distribution\n",
        "- Multimodal linear regression: Mixture of Gaussians\n",
        "\n",
        "이렇게 distribution 을 정하고 나면, 위의 conditional probability 수식이 training data 에 대해서 **maximally likely** 해야합니다. 아래의 수식에 해당합니다.\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/log_prob.png?raw=true\" width=\"50%\" height=\"40%\"  title=\"log_prob\" alt=\"log_prob\"></img></center>\n",
        "\n",
        "위의 수식이 **log-likelihood** 라고 부르고, 해당 수식을 최대화 되게끔 학습이 이루어집니다. 그런데 Loss function 은 최소화되어야하므로 아래의 수식처럼 log-likelihood 앞에 (-) 를 붙입니다. 이를 **negative log-likelihood** 라고 부르고, distribution-based loss function 이 됩니다.\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/nll.png?raw=true\" width=\"40%\" height=\"30%\"  title=\"nll\" alt=\"nll\"></img></center>\n",
        "\n",
        "#### **optimizer loss function**\n",
        "\n",
        "이렇게 loss function 을 결정한 이후에, 해당 loss 값을 최소화시키기 위해 어떻게 해야할까요? 고등수학으로 돌아가보면, 미분가능한 함수에서 최솟값은 미분값이 0인 지점이라는 것을 알 수 있습니다. 이 원리를 이용해서, gradient-based optimization 방법이 제안되었습니다.\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/gradient.png?raw=true\" width=\"60%\" height=\"50%\"  title=\"gradient\" alt=\"gradient\"></img></center>\n",
        "\n",
        "Gradient-based optimization 알고리즘을 살펴보면 아래와 같습니다.\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/gradient_alg.png?raw=true\" width=\"60%\" height=\"50%\"  title=\"gradient_alg\" alt=\"gradient_alg\"></img></center>\n",
        "\n",
        "그러나 해당 방법은 **모든 데이터에 대해서 한번에 gradient 를 계산해야하므로 비용이 많이 든다**는 문제점이 있습니다. 이를 해결하기 위해, 전체 데이터 셋에서 랜덤하게 하나만 샘플링해서 gradient 를 계산하는 방법인 **stochastic gradient descent** 방법이 제안되었습니다.\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/stochastic_alg.png?raw=true\" width=\"60%\" height=\"50%\"  title=\"stochastic_alg\" alt=\"stochastic_alg\"></img></center>\n",
        "\n",
        "위의 방법은 gradient-based optimization 방법보다는 효율적으로 계산이 가능하지만, **굉장히 noisy 하다**는 문제점이 있습니다. 만약 데이터 하나가 전체 데이터셋을 나타내는 표본이 아닌 경우에는, noise 가 있는 데이터 하나를 샘플링하게 되는 것이고 그렇게 되면 optimization 자체는 noise 가 있을 수 밖에 없습니다. 따라서, 데이터 하나를 샘플링하는 것이 아니라 묶음 단위로 하는 방법인 **minibatch stochastic gradient descent** 방법이 제안되었습니다.\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/mini_stochastic.png?raw=true\" width=\"60%\" height=\"50%\"  title=\"mini_stochastic\" alt=\"mini_stochastic\"></img></center>\n",
        "\n",
        "이렇게 모델의 학습을 진행하면 학습 모델의 loss 값이 계속 낮아지도록 학습이 이루어집니다. 그런데, 모델의 training loss 값이 낮은게 정말 좋은 모델을 의미할까요?\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/overfitting.png?raw=true\" width=\"70%\" height=\"60%\"  title=\"overfitting\" alt=\"overfitting\"></img></center>\n",
        "\n",
        "위의 그림을 보시면, 오른쪽 모델의 training loss 가 제일 낮은 것을 확인할 수 있습니다. 그러나 해당 모델이 제일 좋다고는 말할 수 없습니다. 왜냐하면 모델이 학습 데이터에 대해서는 좋은 성능을 보일 수 있어도, 너무 학습 데이터에만 과하게 학습이 되어서 새로운 (처음보는) 데이터가 들어왔을 때는 성능이 좋지 않을 수 있습니다. 이런 문제를 **overfitting** 이라고 부릅니다. Overfitting 현상은 모델이 학습 데이터를 일종의 **\"memorize\"** 현상이라고 볼 수 있습니다. **보통 training loss 는 낮은데, test error 는 높아질 때 발생합니다.** 또한, 모델의 parameter 가 training data 수보다 많은 경우에 일반적으로 발생합니다.\n",
        "\n",
        "이를 해결하는 방법 (Regularization) 이 대표적으로 **weight decay, dropout, early stopping** 이 있습니다. \n",
        "\n",
        "- **weigth decay**: Penalize complex solution using **additional constraints**\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/weight_decay.png?raw=true\" width=\"70%\" height=\"60%\"  title=\"weight_decay\" alt=\"weight_decay\"></img></center>\n",
        "\n",
        "- **Dropout**: **Randomly turn off** activations with some probability p\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/dropout.png?raw=true\" width=\"70%\" height=\"60%\"  title=\"dropout\" alt=\"dropout\"></img></center>\n",
        "\n",
        "- **Early stopping**: Stop training once the validation loss starts to increase\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/earlystopping.png?raw=true\" width=\"70%\" height=\"60%\"  title=\"earlystopping\" alt=\"earlystopping\"></img></center>\n",
        "\n",
        "*이미지 출처: MIT 6.S191: Introduction to Deep Learning*\n",
        "\n",
        "이렇게 Regularization 의 대표적인 3가지 기법들을 알아보았습니다. 그렇다면, 실제로 minibatch stochastic gradient descent 가 어떻게 이루어질까요?\n",
        "\n",
        "1. random 하게 batch 단위의 데이터를 불러오기\n",
        "2. minibatch gradient 계산\n",
        "3. 모델의 parameter 들 업데이트\n",
        "4. validation loss 가 더이상 좋아지지 않을 때까지 1, 2, 3 과정 반복\n",
        "\n",
        "[Adam optimizer]: https://arxiv.org/abs/1412.6980\n",
        "\n",
        "stochastic gradient descent 방법은 learning rate 를 고정시키고 학습을 진행하는데, 고정시킨 learning rate 값이 모델의 학습을 불안정 시키게 할 수 있습니다. 이를 위해 adaptive 하게 learning rate 를 조절하는 optimization 방법이 제안되었습니다. 대표적으로 **[Adam optimizer]** 가 있습니다. 현재 제일 많이 사용되는 optimizer 입니다. 본 실습에서도 Adam optimizer 를 사용할 것입니다. 아래는 optimizer 관련 계보입니다.\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/optimizers.png?raw=true\" width=\"70%\" height=\"60%\"  title=\"optimizers\" alt=\"optimizers\"></img></center>\n",
        "\n",
        "\n",
        "\n",
        "#### **본 실습에서 학습 과정**\n",
        "\n",
        "- `batch_iter` 를 통해 encoder 입력 문장과 decoder 입력 문장을 배치사이즈 단위로 가져오기\n",
        "- Forward pass: \n",
        "  - Encoder 에 입력 문장 주기\n",
        "  - Decoder 에 step-by-step 으로 단어 토큰 예측/생성\n",
        "  - Teacher forcing 적용\u001f\n",
        "- Backward pass:\n",
        "  - minibatch gradient 계산 (backpropagation)\n",
        "  - 모델의 파라미터들 업데이트 (optimization)\n",
        "  \n",
        "#### **Teacher Forcing**\n",
        "\n",
        "**만약에 디코더에서 첫 토큰이 잘못 예측되면, 그 뒤에 생성되는 토큰들도 잘못된 방향으로 가지 않을까?** (*autoregressive 속성 때문*)\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/autoregressive.png?raw=true\" width=\"60%\" height=\"50%\"  title=\"autoregressive\" alt=\"autoregressive\"></img></center>\n",
        "\n",
        "이런 문제를 해결하기 위해 teacher forcing 이라는 기법이 등장하였습니다. Teacher forcing 의 정의는 \"**the technique where the target word is passed as the next input to the decoder**\" 입니다. 즉, 학습과정에서 실제 정답 문장이 입력으로 주어지는 것을 말합니다. \n",
        "\n",
        "- 장점\n",
        "  - teacher forcing 기법으로 학습을 진행하면 수렴 (convergence) 이 빨리 진행됩니다.\n",
        "- 단점\n",
        "  - training 과정이랑 inference 과정이 다르기 때문에 실제 성능이 낮을 수 있습니다. (*Exposure bias*)\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/teacher_forcing.png?raw=true\" width=\"40%\" height=\"30%\"  title=\"teacher_forcing\" alt=\"teacher_forcing\"></img></center>\n",
        "\n",
        "##### **참고: Gradient Clipping**\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/clipping.png?raw=true\" width=\"60%\" height=\"50%\"  title=\"clipping\" alt=\"clipping\"></img></center>\n",
        "\n",
        "*이미지 출처: Goodfellow et al. Deep Learning. 2016 https://www.deeplearningbook.org/contents/rnn.html*\n",
        "\n",
        "학습 도중에 gradient 가 너무 커지는 현상인 \"**exploding gradient**\" 문제를 해결하는 방법입니다. 쉽게 설명드리자면, gradient 크기가 일정 기준(threshold)이상 너무 커지면 gradient 크기를 재조정하는 것입니다.\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/gradient_clipping_alg.png?raw=true\" width=\"60%\" height=\"50%\"  title=\"clipping_alg\" alt=\"clipping_alg\"></img></center>\n",
        "\n",
        "*Note: gradient clipping 관련 논문: https://arxiv.org/abs/1211.5063*\n",
        "\n",
        "##### **참고: Perplexity**\n",
        "\n",
        "Perplexity 는 context 를 봤을 때, 그 다음 단어를 몇 개의 vocabulary subset 안에서 고를 수 있는지 알려주는 지표입니다.\n",
        "\n",
        "- ppl=1 => 입력을 보면 완벽하게 다음 단어를 예측할 수 있다\n",
        "- ppl=10 => 입력을 보면 다음 단어가 무엇일지 10개 내외에서 맞출 수 있다\n",
        "- ppl=|V| => 아무리 입력을 봐도 대체 다음에 뭐가 나올지 맞출 수 없는 상황이다\n",
        "\n",
        "<center><img src=\"https://github.com/passing2961/KEMC/blob/master/ppl.png?raw=true\" width=\"60%\" height=\"50%\"  title=\"ppl\" alt=\"ppl\"></img></center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeE-uBytX90n"
      },
      "source": [
        "class EarlyStopping(object):\n",
        "    \"\"\"\n",
        "    EarlyStopping\n",
        "    \"\"\"\n",
        "    def __init__(self, best_loss, max_patience):\n",
        "        super(EarlyStopping, self).__init__()\n",
        "        \n",
        "        self.best_loss = best_loss\n",
        "        self.patience = 0\n",
        "        self.max_patience = max_patience\n",
        "        self.best_epoch = 0\n",
        "        \n",
        "    def update(self, recent_loss, recent_epoch):\n",
        "        if self.best_loss > recent_loss:\n",
        "            self.best_loss = recent_loss\n",
        "            self.patience = 0\n",
        "            \n",
        "            self.best_epoch = recent_epoch\n",
        "            return \"update\"\n",
        "        else:\n",
        "            self.patience += 1\n",
        "            if self.patience == self.max_patience:\n",
        "                return \"terminate\"\n",
        "            \n",
        "            return \"patience\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VVKl-8oX90r"
      },
      "source": [
        "def evaluate(encoder, decoder, val_batches, device, word2idx):\n",
        "\n",
        "    \n",
        "    total_loss = 0.\n",
        "    \n",
        "    for batch_idx, (batch_x, batch_y) in enumerate(val_batches):\n",
        "        if batch_idx == 1:\n",
        "            break\n",
        "            \n",
        "        batch_enc_input, batch_dec_target = batch_dataset(batch_x, batch_y, word2idx)\n",
        "        \n",
        "        batch_enc_input = torch.tensor(batch_enc_input, dtype = torch.long, device='cuda')\n",
        "        batch_dec_target = torch.tensor(batch_dec_target, dtype = torch.long, device='cuda')\n",
        "        \n",
        "        loss = 0.\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Forward pass through encoder\n",
        "            encoder_outputs, (encoder_hidden, encoder_cell) = encoder(batch_enc_input)\n",
        "            #encoder_output, (eh, ec) = encoder(enc_input)\n",
        "\n",
        "            eh = torch.cat((encoder_hidden[0], encoder_hidden[1]), dim=1).unsqueeze(0)\n",
        "            ec = torch.cat((encoder_cell[0], encoder_cell[1]), dim=1).unsqueeze(0)\n",
        "            \n",
        "            # Create initial decoder input (start with <sos> tokens for each sentence)\n",
        "            dec_input = torch.tensor([word2idx['<sos>']] * batch_size, dtype=torch.long, device=device)\n",
        "            dec_input = dec_input.unsqueeze(1)\n",
        "\n",
        "            # Set initial decoder hidden state to the encoder's final hidden state\n",
        "            decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "            \n",
        "            for t in range(1, batch_dec_target.size(1)):\n",
        "                decoder_output, eh, ec = decoder(dec_input, eh, ec, encoder_outputs)\n",
        "\n",
        "                # Calculate and accumulate loss\n",
        "                loss += criterion(decoder_output, batch_dec_target[:, t])\n",
        "\n",
        "                # Teacher forcing: next input is current target\n",
        "                dec_input = batch_dec_target[:, t].unsqueeze(1)\n",
        "                \n",
        "            batch_loss = loss.item() / int(batch_dec_target.size(1))\n",
        "            total_loss += batch_loss\n",
        "        \n",
        "    return total_loss / (batch_idx + 1)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "4U0-B3iUsKGZ",
        "outputId": "8bf51bf8-8efb-4359-c03f-b9289a481c22"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Define Earlystopping class\n",
        "earlystopping = EarlyStopping(best_loss, max_patience)\n",
        "\n",
        "num_batches_per_epoch = int(len(train) / batch_size)\n",
        "print(\"[num_batches_per_epoch] {}\".format(num_batches_per_epoch))\n",
        "\n",
        "    \n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "    total_loss = 0.\n",
        "    total_ppl = 0.\n",
        "    print_loss = 0.\n",
        "    \n",
        "    # Load batches for iteration\n",
        "    train_batches = batch_iter(train, batch_size)\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    # Training loop\n",
        "    for batch_idx, (batch_x, batch_y) in enumerate(train_batches):\n",
        "\n",
        "        batch_enc_input, batch_dec_target = batch_dataset(batch_x, batch_y, word2idx)\n",
        "        \n",
        "        # Initialize variables\n",
        "        loss = 0.\n",
        "        \n",
        "        # zero gradients\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        batch_enc_input = torch.tensor(batch_enc_input, dtype = torch.long, device='cuda')\n",
        "        batch_dec_target = torch.tensor(batch_dec_target, dtype = torch.long, device='cuda')\n",
        "\n",
        "        # Forward pass through encoder\n",
        "        encoder_outputs, (encoder_hidden, encoder_cell) = encoder(batch_enc_input)\n",
        "                 \n",
        "        eh = torch.cat((encoder_hidden[0], encoder_hidden[1]), dim=1).unsqueeze(0)\n",
        "        ec = torch.cat((encoder_cell[0], encoder_cell[1]), dim=1).unsqueeze(0)\n",
        "        \n",
        "        # Create initial decoder input (start with <sos> tokens for each sentence)\n",
        "        dec_input = torch.tensor([word2idx['<sos>']] * batch_size, dtype=torch.long, device=device)\n",
        "        dec_input = dec_input.unsqueeze(1)\n",
        "\n",
        "        # Set initial decoder hidden state to the encoder's final hidden state\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "        # Determine if we are using teacher forcing this iteration\n",
        "        # random.random() -> 0~1 사이의 난수 생성\n",
        "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "        \n",
        "        if use_teacher_forcing:\n",
        "            for t in range(1, batch_dec_target.size(1)):\n",
        "                \n",
        "                decoder_output, eh, ec = decoder(dec_input, eh, ec, encoder_outputs)\n",
        "                \n",
        "                # Calculate and accumulate loss\n",
        "                loss += criterion(decoder_output, batch_dec_target[:, t])\n",
        "\n",
        "                # Teacher forcing: next input is current target\n",
        "                dec_input = batch_dec_target[:, t].unsqueeze(1)                \n",
        "        else:\n",
        "            for t in range(1, batch_dec_target.size(1)):\n",
        "                decoder_output, eh, ec = decoder(dec_input, eh, ec, encoder_outputs)\n",
        "            \n",
        "                # No teacher forcing: next input is decoder's own current output\n",
        "                _, topi = decoder_output.topk(1)\n",
        "            \n",
        "                dec_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "                dec_input = decoder_input.to(device)\n",
        "            \n",
        "                # Calculate and accumulate loss\n",
        "                loss += criterion(decoder_ouput, batch_dec_target[:, t])\n",
        "            \n",
        "            \n",
        "        batch_loss = (loss.item() / int(batch_dec_target.size(1)))\n",
        "        batch_ppl = np.exp(batch_loss)\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        total_ppl += batch_ppl\n",
        "        print_loss += loss.item()\n",
        "        \n",
        "        # Perform backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient clipping: gradients are modified in place\n",
        "        #_ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "        #_ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "        \n",
        "        # Adjust/Update model weights\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "        \n",
        "        if (batch_idx + 1) % LOG_INTERVAL == 0:\n",
        "            print(\"[epoch {} | step {}/{}] loss: {:.4f} (Avg. {:4f}) PPL: {:.4f} (Avg. {:.4f})\".format(epoch+1,\n",
        "                                                                                                       batch_idx+1,\n",
        "                                                                                                       num_batches_per_epoch,\n",
        "                                                                                                       batch_loss, total_loss/(batch_idx + 1),\n",
        "                                                                                                       batch_ppl, total_ppl/(batch_idx + 1)))\n",
        "\n",
        "    # How to use EarlyStopping\n",
        "    val_batches = batch_iter(test, batch_size)\n",
        "    \n",
        "\n",
        "    encoder.cuda().eval()\n",
        "    decoder.cuda().eval()\n",
        "    val_loss = evaluate(encoder, decoder, val_batches, device, word2idx)\n",
        "    print(\"[epoch {}] loss: {:.4f}\".format(epoch+1, val_loss))    \n",
        "    \n",
        "    # EarlyStopping for preventing overfitting problem\n",
        "    exitcode = earlystopping.update(val_loss, epoch)\n",
        "    if exitcode == 'update':\n",
        "        # 학습된 모델 저장\n",
        "        torch.save(encoder, dirpath + 'best_encoder_' + str(epoch + 1) + '.pt')\n",
        "        torch.save(decoder, dirpath + 'best_decoder_' + str(epoch + 1) + '.pt')\n",
        "        print(\"[epoch {}] patience: {}\\tmax_patience: {}\\tbest_loss: {}\\tModel best performance!\".format(epoch+1, \n",
        "                                                                                                         earlystopping.patience,\n",
        "                                                                                                         earlystopping.max_patience,\n",
        "                                                                                                         earlystopping.best_loss))\n",
        "    \n",
        "    \n",
        "    torch.save(encoder, dirpath + 'encoder_' + str(epoch + 1) + '.pt')\n",
        "    torch.save(decoder, dirpath + 'decoder_' + str(epoch + 1) + '.pt')\n",
        "    #total_loss_per_epoch = (total_loss / (batch_idx + 1))\n",
        "            "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[num_batches_per_epoch] 332\n",
            "[epoch 1 | step 100/332] loss: 0.1151 (Avg. 0.121841) PPL: 1.1220 (Avg. 1.1298)\n",
            "[epoch 1 | step 200/332] loss: 0.1194 (Avg. 0.116804) PPL: 1.1268 (Avg. 1.1241)\n",
            "[epoch 1 | step 300/332] loss: 0.0925 (Avg. 0.111069) PPL: 1.0969 (Avg. 1.1177)\n",
            "[epoch 1] loss: 0.7992\n",
            "[epoch 1] patience: 0\tmax_patience: 5\tbest_loss: 0.7992199085376881\tModel best performance!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type AttnDecoderRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch 2 | step 100/332] loss: 0.0789 (Avg. 0.086322) PPL: 1.0821 (Avg. 1.0903)\n",
            "[epoch 2 | step 200/332] loss: 0.1079 (Avg. 0.087765) PPL: 1.1139 (Avg. 1.0919)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e8bcef821203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n# Define Earlystopping class\\nearlystopping = EarlyStopping(best_loss, max_patience)\\n\\nnum_batches_per_epoch = int(len(train) / batch_size)\\nprint(\"[num_batches_per_epoch] {}\".format(num_batches_per_epoch))\\n\\n    \\nfor epoch in range(max_epochs):\\n\\n    total_loss = 0.\\n    total_ppl = 0.\\n    print_loss = 0.\\n    \\n    # Load batches for iteration\\n    train_batches = batch_iter(train, batch_size)\\n    \\n    encoder.train()\\n    decoder.train()\\n\\n    # Training loop\\n    for batch_idx, (batch_x, batch_y) in enumerate(train_batches):\\n\\n        batch_enc_input, batch_dec_target = batch_dataset(batch_x, batch_y, word2idx)\\n        \\n        # Initialize variables\\n        loss = 0.\\n        \\n        # zero gradients\\n        encoder_optimizer.zero_grad()\\n        decoder_optimizer.zero_grad()\\n\\n        batch_enc_input = torch.tensor(batch_enc_input, dtype = torch.long, device=\\'cuda\\')\\n        batch_dec_target = torch.tensor(batch_dec_target, dtype = torch.long, device=\\'cuda\\')\\n\\n        # Forward pass through encoder\\n        encoder_outputs, (encoder_hidden, encoder_cell) = encoder(batch_enc_input)\\n                 \\n        eh = torch.cat((encoder_hidden[0], encoder_hidden[1]), dim=1).unsqueeze(0)\\n        ec = torch.cat((encoder_cell[0], encoder_cell[1]), dim=1).unsqueeze(0)\\n        \\n ...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-144a4e5e3d75>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dec_input, hidden, cell, encoder_outputs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Predict next word using Luong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# output shape == (batch_size, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj6kZUHJG0Ri"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}