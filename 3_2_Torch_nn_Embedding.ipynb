{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-2_Torch_nn_Embedding.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minnji88/NLP-study/blob/main/3_2_Torch_nn_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQVjqpojlilk"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8pnJM9UlajD",
        "outputId": "a68f28cb-4024-4203-a4e2-8feed6044eda"
      },
      "source": [
        "train_data = 'you need to know how to code'\r\n",
        "word_set = set(train_data.split()) # 중복을 제거한 단어들의 집합인 단어 집합 생성.\r\n",
        "vocab = {word: i+2 for i, word in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수 맵핑.\r\n",
        "vocab['<unk>'] = 0 # unknown 토큰\r\n",
        "vocab['<pad>'] = 1 # 문자열의 길이를 맞춰주는 padding 토큰 \r\n",
        "print(vocab)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'to': 2, 'you': 3, 'know': 4, 'code': 5, 'how': 6, 'need': 7, '<unk>': 0, '<pad>': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8FIidh_leDO"
      },
      "source": [
        "# 단어 집합의 크기만큼의 행을 가지는 임베딩 테이블 생성.\r\n",
        "embedding_table = torch.FloatTensor([\r\n",
        "                               [ 0.0,  0.0,  0.0],\r\n",
        "                               [ 0.0,  0.0,  0.0],\r\n",
        "                               [ 0.2,  0.9,  0.3],\r\n",
        "                               [ 0.1,  0.5,  0.7],\r\n",
        "                               [ 0.2,  0.1,  0.8],\r\n",
        "                               [ 0.4,  0.1,  0.1],\r\n",
        "                               [ 0.1,  0.8,  0.9],\r\n",
        "                               [ 0.6,  0.1,  0.1]])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dACv8RVslgNQ",
        "outputId": "4ce8853b-f170-4ebe-9835-acbdf5a2e90c"
      },
      "source": [
        "# 임의의 샘플 문장\r\n",
        "sample = 'you need to run'.split()\r\n",
        "idxes=[]\r\n",
        "# 각 단어를 정수로 변환\r\n",
        "for word in sample:\r\n",
        "  try:\r\n",
        "    idxes.append(vocab[word])\r\n",
        "  except KeyError: # 단어 집합에 없는 단어일 경우 <unk>로 대체된다.\r\n",
        "    idxes.append(vocab['<unk>'])\r\n",
        "idxes = torch.LongTensor(idxes)\r\n",
        "print(idxes)\r\n",
        "\r\n",
        "# 룩업 테이블\r\n",
        "lookup_result = embedding_table[idxes, :] # 각 정수를 인덱스로 임베딩 테이블에서 값을 가져온다.\r\n",
        "print(lookup_result)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3, 7, 2, 0])\n",
            "tensor([[0.1000, 0.5000, 0.7000],\n",
            "        [0.6000, 0.1000, 0.1000],\n",
            "        [0.2000, 0.9000, 0.3000],\n",
            "        [0.0000, 0.0000, 0.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y23Ksk71lnbz"
      },
      "source": [
        "train_data = 'you need to know how to code'\r\n",
        "word_set = set(train_data.split()) # 중복을 제거한 단어들의 집합인 단어 집합 생성.\r\n",
        "vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수 맵핑.\r\n",
        "vocab['<unk>'] = 0\r\n",
        "vocab['<pad>'] = 1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMqH-RRqlr-h"
      },
      "source": [
        "embedding_layer = nn.Embedding(num_embeddings = len(vocab), \r\n",
        "                               embedding_dim = 3,\r\n",
        "                               padding_idx = 1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CQ0C2ptltmA",
        "outputId": "b3d1f415-da31-49a9-9b6b-7bb98090fb83"
      },
      "source": [
        "print(embedding_layer.weight)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 1.5445, -2.0409,  1.2758],\n",
            "        [ 0.0000,  0.0000,  0.0000],\n",
            "        [ 0.8820,  0.4425,  1.7675],\n",
            "        [-0.6918,  0.1902,  0.7226],\n",
            "        [-1.7587, -0.0400,  0.2853],\n",
            "        [ 0.6229, -0.7249, -0.3257],\n",
            "        [-0.4559,  0.1322, -0.2601],\n",
            "        [ 1.4274, -0.0087,  1.1540]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}